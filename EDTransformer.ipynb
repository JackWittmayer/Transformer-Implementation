{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO2uI0DxDPziJBrPXtY9kdM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JackWittmayer/Transformer-Implementation/blob/main/EDTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37L0J9nog5yz",
        "outputId": "4d3c2fa2-beb3-4edc-e0b4-2a73964184d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VNf0Rr3ogy-4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import os\n",
        "import pickle\n",
        "from unicodedata import normalize\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import log_softmax, pad\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(25)\n",
        "random.seed(25)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "2Hs0kEZYlwc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c94ef30-70c3-466a-9a35-9fbccecfd670"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_X = torch.tensor([[3, 2, 0, 1], [1, 2, 3, 0]], dtype=torch.int32).to(device)\n",
        "SAMPLE_Z = torch.tensor([4, 1, 7, 6], dtype=torch.int32).to(device)"
      ],
      "metadata": {
        "id": "bMXiZFPpEtZa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printIfVerbose(verbose, tag, value):\n",
        "    if verbose:\n",
        "        print(tag, value)"
      ],
      "metadata": {
        "id": "6gsV7Wgv2DKh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        super().__init__()\n",
        "        self.table = nn.Embedding(vocab_size, embedding_size).to(device)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        embeddings = self.table(sequence)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "Pk7xtwam89Hh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_embedding():\n",
        "    torch.manual_seed(25)\n",
        "    vocab_size = 4\n",
        "    embedding = Embedding(vocab_size, 4)\n",
        "    print(\"weight:\", embedding.table.weight)\n",
        "    print(\"SAMPLE_X: \", SAMPLE_X)\n",
        "    output = embedding(SAMPLE_X)\n",
        "    print(\"output:\", output)\n",
        "    for j in range(len(output)):\n",
        "        #print(\"sample:\", sample)\n",
        "        for i in range(vocab_size):\n",
        "            assert output[j, i, :].eq(embedding.table.weight[SAMPLE_X[j, i]]).all()\n",
        "test_embedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StJF4qlIBYVj",
        "outputId": "0d596e11-d1b7-4ab5-8c79-98378d45c252"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight: Parameter containing:\n",
            "tensor([[ 0.0877, -0.6113,  0.3441, -1.2916],\n",
            "        [-0.5874,  0.8060,  1.3200,  0.4826],\n",
            "        [ 1.6671, -0.2342,  0.1074,  1.7852],\n",
            "        [ 0.7874, -0.2466,  0.2384, -0.6746]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "SAMPLE_X:  tensor([[3, 2, 0, 1],\n",
            "        [1, 2, 3, 0]], device='cuda:0', dtype=torch.int32)\n",
            "output: tensor([[[ 0.7874, -0.2466,  0.2384, -0.6746],\n",
            "         [ 1.6671, -0.2342,  0.1074,  1.7852],\n",
            "         [ 0.0877, -0.6113,  0.3441, -1.2916],\n",
            "         [-0.5874,  0.8060,  1.3200,  0.4826]],\n",
            "\n",
            "        [[-0.5874,  0.8060,  1.3200,  0.4826],\n",
            "         [ 1.6671, -0.2342,  0.1074,  1.7852],\n",
            "         [ 0.7874, -0.2466,  0.2384, -0.6746],\n",
            "         [ 0.0877, -0.6113,  0.3441, -1.2916]]], device='cuda:0',\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Unembedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Linear(embedding_size, vocab_size).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.weight(x)"
      ],
      "metadata": {
        "id": "EpoJIpc_CaX6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_unembedding():\n",
        "    torch.manual_seed(25)\n",
        "    vocab_size = 10\n",
        "    embedding_size = 4\n",
        "    sequence_length = 4\n",
        "    batch_size = 2\n",
        "    input = torch.rand(batch_size, sequence_length, embedding_size).to(device)\n",
        "    unembedding = Unembedding(vocab_size, embedding_size)\n",
        "\n",
        "    print(\"weight:\", unembedding.weight)\n",
        "    print(\"input: \", input)\n",
        "    output = unembedding(input)\n",
        "    print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, sequence_length, vocab_size)\n",
        "test_unembedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLZ0-F4xNOMf",
        "outputId": "f83116d1-523c-4981-f99c-f16d0142524a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight: Linear(in_features=4, out_features=10, bias=True)\n",
            "input:  tensor([[[0.7518, 0.1929, 0.0629, 0.9118],\n",
            "         [0.3828, 0.2990, 0.5933, 0.2911],\n",
            "         [0.2416, 0.5582, 0.0481, 0.3497],\n",
            "         [0.3520, 0.9528, 0.0284, 0.8488]],\n",
            "\n",
            "        [[0.3947, 0.5181, 0.9726, 0.8813],\n",
            "         [0.0056, 0.3056, 0.9384, 0.7949],\n",
            "         [0.4399, 0.1766, 0.8739, 0.1425],\n",
            "         [0.4682, 0.6254, 0.3040, 0.7923]]], device='cuda:0')\n",
            "output: tensor([[[-4.9334e-01,  3.9030e-01, -3.4348e-03,  2.0479e-01, -1.7155e-01,\n",
            "           3.0325e-01,  9.5298e-01, -9.2740e-01,  3.5209e-01, -2.1405e-02],\n",
            "         [-6.2972e-02, -6.9980e-02, -5.4304e-02,  1.2675e-01, -5.5075e-01,\n",
            "           1.8844e-01,  8.6408e-01, -5.4956e-01,  4.7789e-01,  7.8078e-02],\n",
            "         [-2.9110e-01,  3.9284e-02,  7.2926e-02,  2.0875e-01, -3.4683e-01,\n",
            "           1.1962e-01,  7.2445e-01, -5.6804e-01,  4.2547e-01,  1.0732e-02],\n",
            "         [-3.4147e-01,  6.4171e-02, -5.8167e-02,  1.8160e-01, -2.1651e-01,\n",
            "          -1.4458e-01,  1.0255e+00, -9.2857e-01,  5.6889e-01, -7.0657e-02]],\n",
            "\n",
            "        [[ 3.2716e-02, -1.6396e-01, -2.2999e-01,  1.5606e-01, -5.7223e-01,\n",
            "          -1.2983e-01,  1.2599e+00, -8.8230e-01,  6.9367e-01,  1.1626e-01],\n",
            "         [ 7.3857e-03, -2.2392e-01, -6.1284e-02,  3.6410e-01, -6.3714e-01,\n",
            "          -1.2969e-01,  1.0509e+00, -6.4156e-01,  6.5060e-01,  2.7214e-01],\n",
            "         [ 8.3425e-02, -1.5596e-01, -1.1466e-01,  6.0901e-02, -6.8152e-01,\n",
            "           2.4368e-01,  8.9560e-01, -4.8380e-01,  4.9643e-01,  1.0562e-01],\n",
            "         [-2.5832e-01,  7.1240e-02, -8.8499e-02,  1.6288e-01, -3.2179e-01,\n",
            "          -1.3083e-04,  1.0437e+00, -8.7687e-01,  5.3716e-01, -1.0231e-02]]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, embedding_size, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.table = nn.Embedding(max_sequence_length, embedding_size).to(device)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        positions = torch.zeros(sequence.shape, dtype=torch.int32)\n",
        "        positions[:, ::] = torch.arange(0, sequence.shape[-1])\n",
        "        #print(\"positions\", positions)\n",
        "        positional_embeddings = self.table(positions.to(device))\n",
        "        return positional_embeddings"
      ],
      "metadata": {
        "id": "5xpYM9Zf_KLw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_positional_embedding():\n",
        "    embedding_size = 8\n",
        "    max_sequence_length = 10\n",
        "    batch_size = 2\n",
        "    positional_embedding = PositionalEmbedding(embedding_size, max_sequence_length)\n",
        "    output = positional_embedding(SAMPLE_X)\n",
        "    print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, SAMPLE_X.shape[-1], embedding_size)\n",
        "test_positional_embedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l1Z_ZH8Pb_P",
        "outputId": "f45acaf3-b2f3-49c8-88fe-521d6785311a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output: tensor([[[ 0.3412, -0.2888, -0.4162, -1.2433,  0.3364, -2.1897, -0.2086,\n",
            "           0.0196],\n",
            "         [ 0.2461, -0.0812, -0.4464, -1.2595,  0.5963, -1.3647, -0.7684,\n",
            "           0.3472],\n",
            "         [-0.0142,  0.1426,  2.0701, -0.1623, -0.4448,  0.8318, -0.2930,\n",
            "          -0.2068],\n",
            "         [-0.8096,  1.2487,  0.5594, -0.3657,  0.5478, -1.4327, -1.4111,\n",
            "          -0.4237]],\n",
            "\n",
            "        [[ 0.3412, -0.2888, -0.4162, -1.2433,  0.3364, -2.1897, -0.2086,\n",
            "           0.0196],\n",
            "         [ 0.2461, -0.0812, -0.4464, -1.2595,  0.5963, -1.3647, -0.7684,\n",
            "           0.3472],\n",
            "         [-0.0142,  0.1426,  2.0701, -0.1623, -0.4448,  0.8318, -0.2930,\n",
            "          -0.2068],\n",
            "         [-0.8096,  1.2487,  0.5594, -0.3657,  0.5478, -1.4327, -1.4111,\n",
            "          -0.4237]]], device='cuda:0', grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(queries, keys, values, mask, dropout, verbose):\n",
        "    printIfVerbose(verbose, \"queries:\", queries)\n",
        "    printIfVerbose(verbose, \"keys:\", keys)\n",
        "    printIfVerbose(verbose, \"values:\", values)\n",
        "    keys_transposed = torch.transpose(keys, -2, -1)\n",
        "    printIfVerbose(verbose, \"keys_transposed:\", keys_transposed)\n",
        "    scores = torch.matmul(queries, keys_transposed)\n",
        "    #assert scores.shape == (keys.shape[0], keys.shape[-1], queries.shape[-1])\n",
        "    printIfVerbose(verbose, \"scores:\", scores)\n",
        "    printIfVerbose(verbose, \"scores:\", scores.shape)\n",
        "    printIfVerbose(verbose, \"masks:\", mask.shape)\n",
        "    scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    printIfVerbose(verbose, \"masked scores:\", scores)\n",
        "    d_attn = keys.shape[-1]\n",
        "    scaled_scores = scores / math.sqrt(d_attn)\n",
        "    printIfVerbose(verbose, \"scaled_scores:\", scaled_scores)\n",
        "    softmax_scores = torch.softmax(scaled_scores, -1)\n",
        "    softmax_scores = dropout(softmax_scores)\n",
        "    printIfVerbose(verbose, \"softmax_scores:\", softmax_scores)\n",
        "    printIfVerbose(verbose, \"softmax_socres shape:\", softmax_scores.shape)\n",
        "    printIfVerbose(verbose, \"values:\", values)\n",
        "    v_out = torch.matmul(softmax_scores, values)\n",
        "    return v_out"
      ],
      "metadata": {
        "id": "DL3t3E_ThGF2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_attention():\n",
        "    d_attn = 4\n",
        "    length_x = 4\n",
        "    length_z = 3\n",
        "    batch_size = 2\n",
        "    d_out = 2\n",
        "\n",
        "    queries = torch.rand(batch_size, length_x, d_attn)\n",
        "    keys = torch.rand(batch_size, length_z, d_attn)\n",
        "    values = torch.rand(batch_size, length_z, d_out)\n",
        "    mask = torch.tril(torch.ones(length_x, length_z) == 1)\n",
        "    padding_mask = torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0]], dtype=torch.int32)\n",
        "\n",
        "    v_out = attention(queries, keys, values, mask, nn.Dropout(0.1), True)\n",
        "    #print(\"output:\", v_out)\n",
        "    assert v_out.shape == (batch_size, length_x, d_out)\n",
        "test_attention()"
      ],
      "metadata": {
        "id": "KhGXLYp6i61z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f10ce257-ce28-48ed-ac4a-d635b67db0e2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "queries: tensor([[[0.4961, 0.6278, 0.3572, 0.5220],\n",
            "         [0.1997, 0.5286, 0.4723, 0.0238],\n",
            "         [0.1838, 0.2010, 0.1765, 0.8587],\n",
            "         [0.7776, 0.1199, 0.8638, 0.1066]],\n",
            "\n",
            "        [[0.1084, 0.8448, 0.7043, 0.9275],\n",
            "         [0.3953, 0.2704, 0.6228, 0.6078],\n",
            "         [0.7686, 0.3296, 0.4959, 0.0065],\n",
            "         [0.9125, 0.8358, 0.6698, 0.4129]]])\n",
            "keys: tensor([[[0.0129, 0.5052, 0.5967, 0.3134],\n",
            "         [0.1648, 0.4834, 0.2368, 0.7654],\n",
            "         [0.9255, 0.3393, 0.5612, 0.0953]],\n",
            "\n",
            "        [[0.5582, 0.5739, 0.5244, 0.6292],\n",
            "         [0.7426, 0.3134, 0.7793, 0.9385],\n",
            "         [0.1588, 0.3427, 0.3863, 0.2306]]])\n",
            "values: tensor([[[0.1533, 0.0876],\n",
            "         [0.9218, 0.8859],\n",
            "         [0.6448, 0.5202]],\n",
            "\n",
            "        [[0.3174, 0.8487],\n",
            "         [0.8658, 0.5804],\n",
            "         [0.1021, 0.1329]]])\n",
            "keys_transposed: tensor([[[0.0129, 0.1648, 0.9255],\n",
            "         [0.5052, 0.4834, 0.3393],\n",
            "         [0.5967, 0.2368, 0.5612],\n",
            "         [0.3134, 0.7654, 0.0953]],\n",
            "\n",
            "        [[0.5582, 0.7426, 0.1588],\n",
            "         [0.5739, 0.3134, 0.3427],\n",
            "         [0.5244, 0.7793, 0.3863],\n",
            "         [0.6292, 0.9385, 0.2306]]])\n",
            "scores: tensor([[[0.7003, 0.8693, 0.9223],\n",
            "         [0.5590, 0.4186, 0.6315],\n",
            "         [0.4784, 0.8265, 0.4192],\n",
            "         [0.6194, 0.4722, 1.2552]],\n",
            "\n",
            "        [[1.4984, 1.7646, 0.7927],\n",
            "         [1.0850, 1.4341, 0.5363],\n",
            "         [0.8825, 1.0667, 0.4282],\n",
            "         [1.6002, 1.8490, 0.7854]]])\n",
            "scores: torch.Size([2, 4, 3])\n",
            "masks: torch.Size([4, 3])\n",
            "masked scores: tensor([[[ 7.0027e-01, -1.0000e+09, -1.0000e+09],\n",
            "         [ 5.5897e-01,  4.1855e-01, -1.0000e+09],\n",
            "         [ 4.7836e-01,  8.2648e-01,  4.1922e-01],\n",
            "         [ 6.1941e-01,  4.7223e-01,  1.2552e+00]],\n",
            "\n",
            "        [[ 1.4984e+00, -1.0000e+09, -1.0000e+09],\n",
            "         [ 1.0850e+00,  1.4341e+00, -1.0000e+09],\n",
            "         [ 8.8246e-01,  1.0667e+00,  4.2816e-01],\n",
            "         [ 1.6002e+00,  1.8490e+00,  7.8537e-01]]])\n",
            "scaled_scores: tensor([[[ 3.5014e-01, -5.0000e+08, -5.0000e+08],\n",
            "         [ 2.7948e-01,  2.0928e-01, -5.0000e+08],\n",
            "         [ 2.3918e-01,  4.1324e-01,  2.0961e-01],\n",
            "         [ 3.0970e-01,  2.3612e-01,  6.2761e-01]],\n",
            "\n",
            "        [[ 7.4918e-01, -5.0000e+08, -5.0000e+08],\n",
            "         [ 5.4248e-01,  7.1705e-01, -5.0000e+08],\n",
            "         [ 4.4123e-01,  5.3335e-01,  2.1408e-01],\n",
            "         [ 8.0008e-01,  9.2452e-01,  3.9269e-01]]])\n",
            "softmax_scores: tensor([[[1.1111, 0.0000, 0.0000],\n",
            "         [0.5751, 0.5361, 0.0000],\n",
            "         [0.3515, 0.4183, 0.3413],\n",
            "         [0.3364, 0.3125, 0.0000]],\n",
            "\n",
            "        [[1.1111, 0.0000, 0.0000],\n",
            "         [0.5072, 0.6039, 0.0000],\n",
            "         [0.0000, 0.4211, 0.3060],\n",
            "         [0.0000, 0.4497, 0.2642]]])\n",
            "softmax_socres shape: torch.Size([2, 4, 3])\n",
            "values: tensor([[[0.1533, 0.0876],\n",
            "         [0.9218, 0.8859],\n",
            "         [0.6448, 0.5202]],\n",
            "\n",
            "        [[0.3174, 0.8487],\n",
            "         [0.8658, 0.5804],\n",
            "         [0.1021, 0.1329]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "class MaskStrategy(Enum):\n",
        "    UNMASKED = 1\n",
        "    MASKED = 2"
      ],
      "metadata": {
        "id": "wUJ-CbaCB9g-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, num_heads, d_attn, d_x, d_z, d_out, d_mid, maskStrategy, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.num_heads = num_heads\n",
        "        self.d_attn = d_attn\n",
        "        self.d_x = d_x\n",
        "        self.d_z = d_z\n",
        "        self.d_out = d_out\n",
        "        self.d_mid = d_mid\n",
        "        self.maskStrategy = maskStrategy\n",
        "        self.weight_query = nn.Linear(d_x, d_attn).to(device)\n",
        "        self.weight_key = nn.Linear(d_z, d_attn).to(device)\n",
        "        self.weight_value = nn.Linear(d_z, d_mid).to(device)\n",
        "        self.weight_out = nn.Linear(d_mid, d_out).to(device)\n",
        "        self.dropout = nn.Dropout(p_dropout)\n",
        "\n",
        "    def forward(self, z, x, padding_mask):\n",
        "        length_z = z.shape[-2]\n",
        "        length_x = x.shape[-2]\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        queries = self.weight_query(x).view(batch_size, length_x, self.num_heads, -1).transpose(1, 2)\n",
        "        keys = self.weight_key(z).view(batch_size, length_z, self.num_heads, -1).transpose(1, 2)\n",
        "        values = self.weight_value(z).view(batch_size, length_z, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        assert queries.shape == (batch_size, self.num_heads, length_x, self.d_attn / self.num_heads)\n",
        "        assert keys.shape == (batch_size, self.num_heads, length_z, self.d_attn / self.num_heads)\n",
        "        assert values.shape == (batch_size, self.num_heads, length_z, self.d_mid / self.num_heads)\n",
        "\n",
        "        if self.maskStrategy == MaskStrategy['UNMASKED']:\n",
        "            mask = padding_mask.unsqueeze(-2)\n",
        "        elif self.maskStrategy == MaskStrategy['MASKED']:\n",
        "            padding_mask = padding_mask.unsqueeze(-2)\n",
        "            mask = torch.tril(torch.ones(length_x, length_z) == 1).to(device)\n",
        "            printIfVerbose(self.verbose, \"padding mask:\", padding_mask.shape)\n",
        "            printIfVerbose(self.verbose, \"mask tril\", mask)\n",
        "            mask = mask & padding_mask\n",
        "            printIfVerbose(self.verbose, \"merged mask:\", mask)\n",
        "        mask = mask.unsqueeze(1)\n",
        "        printIfVerbose(self.verbose, \"mask\", mask)\n",
        "        printIfVerbose(self.verbose, \"mask\", mask.shape)\n",
        "        v_out = attention(queries, keys, values, mask, self.dropout, self.verbose)\n",
        "        printIfVerbose(self.verbose, \"v_out shape\", v_out.shape)\n",
        "        assert v_out.shape == (batch_size, self.num_heads, length_x, self.d_mid / self.num_heads)\n",
        "        printIfVerbose(self.verbose, \"v_out:\", v_out)\n",
        "        printIfVerbose(self.verbose, \"v_out shape before:\", v_out.shape)\n",
        "        v_out = v_out.transpose(1, 2).reshape(batch_size, length_x, -1)\n",
        "        printIfVerbose(self.verbose, \"v_out shape:\", v_out.shape)\n",
        "        printIfVerbose(self.verbose, \"v_out reshaped:\", v_out)\n",
        "        output = self.weight_out(v_out)\n",
        "        printIfVerbose(self.verbose, \"output shape\", output.shape)\n",
        "        assert output.shape == (batch_size, length_x, self.d_out)\n",
        "        return output\n",
        "\n",
        "    def disable_subsequent_mask(self):\n",
        "        self.maskStrategy = MaskStrategy['UNMASKED']\n",
        "\n",
        "    def enable_subsequent_mask(self):\n",
        "        self.maskStrategy = MaskStrategy['MASKED']\n"
      ],
      "metadata": {
        "id": "CX2A1i9Z4lVO"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_headed_attention_encoder_fixed():\n",
        "    num_heads = 1\n",
        "    d_attn = 4\n",
        "    d_x = 4\n",
        "    d_z = 4\n",
        "    d_out = 1\n",
        "    d_mid = 3\n",
        "    length_z = 3\n",
        "    batch_size = 1\n",
        "    padding_mask = torch.tensor([[1, 1, 0]], dtype=torch.int32).to(device)\n",
        "\n",
        "    multi_headed_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], 0.0, True).to(device)\n",
        "    z = torch.tensor([[[1, 0, 1, 0], [0, 2, 0, 2], [1, 1, 1, 1]]], dtype=torch.float32).to(device)\n",
        "    #print(\"z:\", z\n",
        "    output = multi_headed_attention(z, z, padding_mask)\n",
        "    #print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, length_z, d_out)\n",
        "test_multi_headed_attention_encoder_fixed()"
      ],
      "metadata": {
        "id": "kJuRy-3dTMY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e7b26d0-77e4-432b-d970-c23a4cb5ca93"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "queries torch.Size([1, 3, 4])\n",
            "torch.Size([1, 1, 3, 4])\n",
            "torch.Size([1, 1, 3, 4])\n",
            "torch.Size([1, 1, 3, 3])\n",
            "mask tensor([[[[1, 1, 0]]]], device='cuda:0', dtype=torch.int32)\n",
            "mask torch.Size([1, 1, 1, 3])\n",
            "queries: tensor([[[[ 0.1323,  0.5921, -0.6788, -0.2635],\n",
            "          [ 1.6504,  0.6980, -1.5903,  1.1315],\n",
            "          [ 0.9161,  0.6927, -1.4619,  0.5480]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "keys: tensor([[[[ 0.5118,  0.5339, -0.4241, -0.7160],\n",
            "          [ 1.7491,  0.4337, -0.8936, -0.6943],\n",
            "          [ 1.2685,  0.5772, -0.8389, -0.9327]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "values: tensor([[[[-0.4436,  0.3099, -0.4846],\n",
            "          [-0.3242,  0.6657,  0.2512],\n",
            "          [-0.6543,  0.4318, -0.4064]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "keys_transposed: tensor([[[[ 0.5118,  1.7491,  1.2685],\n",
            "          [ 0.5339,  0.4337,  0.5772],\n",
            "          [-0.4241, -0.8936, -0.8389],\n",
            "          [-0.7160, -0.6943, -0.9327]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "scores: tensor([[[[0.8604, 1.2778, 1.3249],\n",
            "          [1.0817, 3.8250, 2.7751],\n",
            "          [1.0663, 2.8287, 2.2772]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "scores: torch.Size([1, 1, 3, 3])\n",
            "masks: torch.Size([1, 1, 1, 3])\n",
            "masked scores: tensor([[[[ 8.6039e-01,  1.2778e+00, -1.0000e+09],\n",
            "          [ 1.0817e+00,  3.8250e+00, -1.0000e+09],\n",
            "          [ 1.0663e+00,  2.8287e+00, -1.0000e+09]]]], device='cuda:0',\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "scaled_scores: tensor([[[[ 4.3019e-01,  6.3890e-01, -5.0000e+08],\n",
            "          [ 5.4084e-01,  1.9125e+00, -5.0000e+08],\n",
            "          [ 5.3317e-01,  1.4144e+00, -5.0000e+08]]]], device='cuda:0',\n",
            "       grad_fn=<DivBackward0>)\n",
            "softmax_scores: tensor([[[[0.4480, 0.5520, 0.0000],\n",
            "          [0.2024, 0.7976, 0.0000],\n",
            "          [0.2929, 0.7071, 0.0000]]]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "softmax_socres shape: torch.Size([1, 1, 3, 3])\n",
            "values: tensor([[[[-0.4436,  0.3099, -0.4846],\n",
            "          [-0.3242,  0.6657,  0.2512],\n",
            "          [-0.6543,  0.4318, -0.4064]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "v_out shape torch.Size([1, 1, 3, 3])\n",
            "v_out: tensor([[[[-0.3777,  0.5063, -0.0785],\n",
            "          [-0.3484,  0.5937,  0.1023],\n",
            "          [-0.3592,  0.5615,  0.0356]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "v_out shape before: torch.Size([1, 1, 3, 3])\n",
            "v_out shape: torch.Size([1, 3, 3])\n",
            "v_out reshaped: tensor([[[-0.3777,  0.5063, -0.0785],\n",
            "         [-0.3484,  0.5937,  0.1023],\n",
            "         [-0.3592,  0.5615,  0.0356]]], device='cuda:0',\n",
            "       grad_fn=<ViewBackward0>)\n",
            "output shape torch.Size([1, 3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_headed_attention_encoder():\n",
        "    num_heads = 4\n",
        "    d_attn = 4\n",
        "    d_x = 4\n",
        "    d_z = 4\n",
        "    d_out = 1\n",
        "    d_mid = 4\n",
        "    length_z = 3\n",
        "    batch_size = 3\n",
        "    padding_mask = torch.tensor([[1, 1, 0], [1, 1, 0], [1, 1, 1]], dtype=torch.int32).to(device)\n",
        "\n",
        "    multi_headed_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], 0.0, True).to(device)\n",
        "    z = torch.tensor([[[1, 0, 1, 0], [0, 2, 0, 2], [1, 1, 1, 1]],\n",
        "                      [[1, 0, 1, 0], [0, 2, 0, 2], [1, 1, 1, 1]],\n",
        "                      [[1, 0, 1, 0], [0, 2, 0, 2], [1, 1, 1, 1]]], dtype=torch.float32).to(device)\n",
        "    #print(\"z:\", z\n",
        "    output = multi_headed_attention(z, z, padding_mask)\n",
        "    #print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, length_z, d_out)\n",
        "test_multi_headed_attention_encoder()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9CYHYJbMObt",
        "outputId": "13d05b80-ed07-4dc6-d88a-93cf3d85e8c5"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "queries torch.Size([3, 3, 4])\n",
            "torch.Size([3, 4, 3, 1])\n",
            "torch.Size([3, 4, 3, 1])\n",
            "torch.Size([3, 4, 3, 1])\n",
            "mask tensor([[[[1, 1, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 1]]]], device='cuda:0', dtype=torch.int32)\n",
            "mask torch.Size([3, 1, 1, 3])\n",
            "queries: tensor([[[[-0.3482],\n",
            "          [-1.1754],\n",
            "          [-0.7185]],\n",
            "\n",
            "         [[ 0.3963],\n",
            "          [ 0.4933],\n",
            "          [ 0.4069]],\n",
            "\n",
            "         [[ 0.4984],\n",
            "          [-0.5069],\n",
            "          [ 0.2341]],\n",
            "\n",
            "         [[ 0.5366],\n",
            "          [ 0.7522],\n",
            "          [ 0.7844]]],\n",
            "\n",
            "\n",
            "        [[[-0.3482],\n",
            "          [-1.1754],\n",
            "          [-0.7185]],\n",
            "\n",
            "         [[ 0.3963],\n",
            "          [ 0.4933],\n",
            "          [ 0.4069]],\n",
            "\n",
            "         [[ 0.4984],\n",
            "          [-0.5069],\n",
            "          [ 0.2341]],\n",
            "\n",
            "         [[ 0.5366],\n",
            "          [ 0.7522],\n",
            "          [ 0.7844]]],\n",
            "\n",
            "\n",
            "        [[[-0.3482],\n",
            "          [-1.1754],\n",
            "          [-0.7185]],\n",
            "\n",
            "         [[ 0.3963],\n",
            "          [ 0.4933],\n",
            "          [ 0.4069]],\n",
            "\n",
            "         [[ 0.4984],\n",
            "          [-0.5069],\n",
            "          [ 0.2341]],\n",
            "\n",
            "         [[ 0.5366],\n",
            "          [ 0.7522],\n",
            "          [ 0.7844]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "keys: tensor([[[[ 0.0478],\n",
            "          [-0.2717],\n",
            "          [-0.1440]],\n",
            "\n",
            "         [[-0.6246],\n",
            "          [ 1.2480],\n",
            "          [-0.0473]],\n",
            "\n",
            "         [[ 0.2522],\n",
            "          [-0.5438],\n",
            "          [-0.1306]],\n",
            "\n",
            "         [[-0.8426],\n",
            "          [ 0.6068],\n",
            "          [-0.4385]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0478],\n",
            "          [-0.2717],\n",
            "          [-0.1440]],\n",
            "\n",
            "         [[-0.6246],\n",
            "          [ 1.2480],\n",
            "          [-0.0473]],\n",
            "\n",
            "         [[ 0.2522],\n",
            "          [-0.5438],\n",
            "          [-0.1306]],\n",
            "\n",
            "         [[-0.8426],\n",
            "          [ 0.6068],\n",
            "          [-0.4385]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0478],\n",
            "          [-0.2717],\n",
            "          [-0.1440]],\n",
            "\n",
            "         [[-0.6246],\n",
            "          [ 1.2480],\n",
            "          [-0.0473]],\n",
            "\n",
            "         [[ 0.2522],\n",
            "          [-0.5438],\n",
            "          [-0.1306]],\n",
            "\n",
            "         [[-0.8426],\n",
            "          [ 0.6068],\n",
            "          [-0.4385]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "values: tensor([[[[ 0.2765],\n",
            "          [-0.0787],\n",
            "          [ 0.3918]],\n",
            "\n",
            "         [[ 0.7885],\n",
            "          [-0.5177],\n",
            "          [ 0.3691]],\n",
            "\n",
            "         [[-0.9013],\n",
            "          [ 0.4129],\n",
            "          [-0.5904]],\n",
            "\n",
            "         [[-0.5238],\n",
            "          [-1.3185],\n",
            "          [-1.2697]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2765],\n",
            "          [-0.0787],\n",
            "          [ 0.3918]],\n",
            "\n",
            "         [[ 0.7885],\n",
            "          [-0.5177],\n",
            "          [ 0.3691]],\n",
            "\n",
            "         [[-0.9013],\n",
            "          [ 0.4129],\n",
            "          [-0.5904]],\n",
            "\n",
            "         [[-0.5238],\n",
            "          [-1.3185],\n",
            "          [-1.2697]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2765],\n",
            "          [-0.0787],\n",
            "          [ 0.3918]],\n",
            "\n",
            "         [[ 0.7885],\n",
            "          [-0.5177],\n",
            "          [ 0.3691]],\n",
            "\n",
            "         [[-0.9013],\n",
            "          [ 0.4129],\n",
            "          [-0.5904]],\n",
            "\n",
            "         [[-0.5238],\n",
            "          [-1.3185],\n",
            "          [-1.2697]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "keys_transposed: tensor([[[[ 0.0478, -0.2717, -0.1440]],\n",
            "\n",
            "         [[-0.6246,  1.2480, -0.0473]],\n",
            "\n",
            "         [[ 0.2522, -0.5438, -0.1306]],\n",
            "\n",
            "         [[-0.8426,  0.6068, -0.4385]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0478, -0.2717, -0.1440]],\n",
            "\n",
            "         [[-0.6246,  1.2480, -0.0473]],\n",
            "\n",
            "         [[ 0.2522, -0.5438, -0.1306]],\n",
            "\n",
            "         [[-0.8426,  0.6068, -0.4385]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0478, -0.2717, -0.1440]],\n",
            "\n",
            "         [[-0.6246,  1.2480, -0.0473]],\n",
            "\n",
            "         [[ 0.2522, -0.5438, -0.1306]],\n",
            "\n",
            "         [[-0.8426,  0.6068, -0.4385]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "scores: tensor([[[[-0.0166,  0.0946,  0.0501],\n",
            "          [-0.0562,  0.3193,  0.1693],\n",
            "          [-0.0343,  0.1952,  0.1035]],\n",
            "\n",
            "         [[-0.2475,  0.4946, -0.0188],\n",
            "          [-0.3081,  0.6157, -0.0233],\n",
            "          [-0.2541,  0.5078, -0.0193]],\n",
            "\n",
            "         [[ 0.1257, -0.2710, -0.0651],\n",
            "          [-0.1278,  0.2757,  0.0662],\n",
            "          [ 0.0590, -0.1273, -0.0306]],\n",
            "\n",
            "         [[-0.4521,  0.3256, -0.2353],\n",
            "          [-0.6338,  0.4565, -0.3298],\n",
            "          [-0.6610,  0.4760, -0.3440]]],\n",
            "\n",
            "\n",
            "        [[[-0.0166,  0.0946,  0.0501],\n",
            "          [-0.0562,  0.3193,  0.1693],\n",
            "          [-0.0343,  0.1952,  0.1035]],\n",
            "\n",
            "         [[-0.2475,  0.4946, -0.0188],\n",
            "          [-0.3081,  0.6157, -0.0233],\n",
            "          [-0.2541,  0.5078, -0.0193]],\n",
            "\n",
            "         [[ 0.1257, -0.2710, -0.0651],\n",
            "          [-0.1278,  0.2757,  0.0662],\n",
            "          [ 0.0590, -0.1273, -0.0306]],\n",
            "\n",
            "         [[-0.4521,  0.3256, -0.2353],\n",
            "          [-0.6338,  0.4565, -0.3298],\n",
            "          [-0.6610,  0.4760, -0.3440]]],\n",
            "\n",
            "\n",
            "        [[[-0.0166,  0.0946,  0.0501],\n",
            "          [-0.0562,  0.3193,  0.1693],\n",
            "          [-0.0343,  0.1952,  0.1035]],\n",
            "\n",
            "         [[-0.2475,  0.4946, -0.0188],\n",
            "          [-0.3081,  0.6157, -0.0233],\n",
            "          [-0.2541,  0.5078, -0.0193]],\n",
            "\n",
            "         [[ 0.1257, -0.2710, -0.0651],\n",
            "          [-0.1278,  0.2757,  0.0662],\n",
            "          [ 0.0590, -0.1273, -0.0306]],\n",
            "\n",
            "         [[-0.4521,  0.3256, -0.2353],\n",
            "          [-0.6338,  0.4565, -0.3298],\n",
            "          [-0.6610,  0.4760, -0.3440]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "scores: torch.Size([3, 4, 3, 3])\n",
            "masks: torch.Size([3, 1, 1, 3])\n",
            "masked scores: tensor([[[[-1.6638e-02,  9.4593e-02, -1.0000e+09],\n",
            "          [-5.6166e-02,  3.1933e-01, -1.0000e+09],\n",
            "          [-3.4332e-02,  1.9519e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-2.4754e-01,  4.9461e-01, -1.0000e+09],\n",
            "          [-3.0812e-01,  6.1567e-01, -1.0000e+09],\n",
            "          [-2.5413e-01,  5.0777e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.2568e-01, -2.7103e-01, -1.0000e+09],\n",
            "          [-1.2782e-01,  2.7567e-01, -1.0000e+09],\n",
            "          [ 5.9047e-02, -1.2734e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-4.5211e-01,  3.2560e-01, -1.0000e+09],\n",
            "          [-6.3380e-01,  4.5646e-01, -1.0000e+09],\n",
            "          [-6.6097e-01,  4.7602e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.6638e-02,  9.4593e-02, -1.0000e+09],\n",
            "          [-5.6166e-02,  3.1933e-01, -1.0000e+09],\n",
            "          [-3.4332e-02,  1.9519e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-2.4754e-01,  4.9461e-01, -1.0000e+09],\n",
            "          [-3.0812e-01,  6.1567e-01, -1.0000e+09],\n",
            "          [-2.5413e-01,  5.0777e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.2568e-01, -2.7103e-01, -1.0000e+09],\n",
            "          [-1.2782e-01,  2.7567e-01, -1.0000e+09],\n",
            "          [ 5.9047e-02, -1.2734e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-4.5211e-01,  3.2560e-01, -1.0000e+09],\n",
            "          [-6.3380e-01,  4.5646e-01, -1.0000e+09],\n",
            "          [-6.6097e-01,  4.7602e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.6638e-02,  9.4593e-02,  5.0139e-02],\n",
            "          [-5.6166e-02,  3.1933e-01,  1.6926e-01],\n",
            "          [-3.4332e-02,  1.9519e-01,  1.0346e-01]],\n",
            "\n",
            "         [[-2.4754e-01,  4.9461e-01, -1.8754e-02],\n",
            "          [-3.0812e-01,  6.1567e-01, -2.3344e-02],\n",
            "          [-2.5413e-01,  5.0777e-01, -1.9253e-02]],\n",
            "\n",
            "         [[ 1.2568e-01, -2.7103e-01, -6.5065e-02],\n",
            "          [-1.2782e-01,  2.7567e-01,  6.6177e-02],\n",
            "          [ 5.9047e-02, -1.2734e-01, -3.0570e-02]],\n",
            "\n",
            "         [[-4.5211e-01,  3.2560e-01, -2.3529e-01],\n",
            "          [-6.3380e-01,  4.5646e-01, -3.2984e-01],\n",
            "          [-6.6097e-01,  4.7602e-01, -3.4398e-01]]]], device='cuda:0',\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "scaled_scores: tensor([[[[-1.6638e-02,  9.4593e-02, -1.0000e+09],\n",
            "          [-5.6166e-02,  3.1933e-01, -1.0000e+09],\n",
            "          [-3.4332e-02,  1.9519e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-2.4754e-01,  4.9461e-01, -1.0000e+09],\n",
            "          [-3.0812e-01,  6.1567e-01, -1.0000e+09],\n",
            "          [-2.5413e-01,  5.0777e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.2568e-01, -2.7103e-01, -1.0000e+09],\n",
            "          [-1.2782e-01,  2.7567e-01, -1.0000e+09],\n",
            "          [ 5.9047e-02, -1.2734e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-4.5211e-01,  3.2560e-01, -1.0000e+09],\n",
            "          [-6.3380e-01,  4.5646e-01, -1.0000e+09],\n",
            "          [-6.6097e-01,  4.7602e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.6638e-02,  9.4593e-02, -1.0000e+09],\n",
            "          [-5.6166e-02,  3.1933e-01, -1.0000e+09],\n",
            "          [-3.4332e-02,  1.9519e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-2.4754e-01,  4.9461e-01, -1.0000e+09],\n",
            "          [-3.0812e-01,  6.1567e-01, -1.0000e+09],\n",
            "          [-2.5413e-01,  5.0777e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.2568e-01, -2.7103e-01, -1.0000e+09],\n",
            "          [-1.2782e-01,  2.7567e-01, -1.0000e+09],\n",
            "          [ 5.9047e-02, -1.2734e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-4.5211e-01,  3.2560e-01, -1.0000e+09],\n",
            "          [-6.3380e-01,  4.5646e-01, -1.0000e+09],\n",
            "          [-6.6097e-01,  4.7602e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.6638e-02,  9.4593e-02,  5.0139e-02],\n",
            "          [-5.6166e-02,  3.1933e-01,  1.6926e-01],\n",
            "          [-3.4332e-02,  1.9519e-01,  1.0346e-01]],\n",
            "\n",
            "         [[-2.4754e-01,  4.9461e-01, -1.8754e-02],\n",
            "          [-3.0812e-01,  6.1567e-01, -2.3344e-02],\n",
            "          [-2.5413e-01,  5.0777e-01, -1.9253e-02]],\n",
            "\n",
            "         [[ 1.2568e-01, -2.7103e-01, -6.5065e-02],\n",
            "          [-1.2782e-01,  2.7567e-01,  6.6177e-02],\n",
            "          [ 5.9047e-02, -1.2734e-01, -3.0570e-02]],\n",
            "\n",
            "         [[-4.5211e-01,  3.2560e-01, -2.3529e-01],\n",
            "          [-6.3380e-01,  4.5646e-01, -3.2984e-01],\n",
            "          [-6.6097e-01,  4.7602e-01, -3.4398e-01]]]], device='cuda:0',\n",
            "       grad_fn=<DivBackward0>)\n",
            "softmax_scores: tensor([[[[0.4722, 0.5278, 0.0000],\n",
            "          [0.4072, 0.5928, 0.0000],\n",
            "          [0.4429, 0.5571, 0.0000]],\n",
            "\n",
            "         [[0.3225, 0.6775, 0.0000],\n",
            "          [0.2842, 0.7158, 0.0000],\n",
            "          [0.3182, 0.6818, 0.0000]],\n",
            "\n",
            "         [[0.5979, 0.4021, 0.0000],\n",
            "          [0.4005, 0.5995, 0.0000],\n",
            "          [0.5465, 0.4535, 0.0000]],\n",
            "\n",
            "         [[0.3148, 0.6852, 0.0000],\n",
            "          [0.2516, 0.7484, 0.0000],\n",
            "          [0.2429, 0.7571, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.4722, 0.5278, 0.0000],\n",
            "          [0.4072, 0.5928, 0.0000],\n",
            "          [0.4429, 0.5571, 0.0000]],\n",
            "\n",
            "         [[0.3225, 0.6775, 0.0000],\n",
            "          [0.2842, 0.7158, 0.0000],\n",
            "          [0.3182, 0.6818, 0.0000]],\n",
            "\n",
            "         [[0.5979, 0.4021, 0.0000],\n",
            "          [0.4005, 0.5995, 0.0000],\n",
            "          [0.5465, 0.4535, 0.0000]],\n",
            "\n",
            "         [[0.3148, 0.6852, 0.0000],\n",
            "          [0.2516, 0.7484, 0.0000],\n",
            "          [0.2429, 0.7571, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.3138, 0.3507, 0.3355],\n",
            "          [0.2696, 0.3925, 0.3378],\n",
            "          [0.2936, 0.3694, 0.3370]],\n",
            "\n",
            "         [[0.2295, 0.4820, 0.2885],\n",
            "          [0.2063, 0.5195, 0.2742],\n",
            "          [0.2269, 0.4861, 0.2870]],\n",
            "\n",
            "         [[0.4002, 0.2691, 0.3307],\n",
            "          [0.2695, 0.4034, 0.3271],\n",
            "          [0.3644, 0.3024, 0.3332]],\n",
            "\n",
            "         [[0.2263, 0.4926, 0.2811],\n",
            "          [0.1876, 0.5581, 0.2542],\n",
            "          [0.1821, 0.5678, 0.2501]]]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "softmax_socres shape: torch.Size([3, 4, 3, 3])\n",
            "values: tensor([[[[ 0.2765],\n",
            "          [-0.0787],\n",
            "          [ 0.3918]],\n",
            "\n",
            "         [[ 0.7885],\n",
            "          [-0.5177],\n",
            "          [ 0.3691]],\n",
            "\n",
            "         [[-0.9013],\n",
            "          [ 0.4129],\n",
            "          [-0.5904]],\n",
            "\n",
            "         [[-0.5238],\n",
            "          [-1.3185],\n",
            "          [-1.2697]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2765],\n",
            "          [-0.0787],\n",
            "          [ 0.3918]],\n",
            "\n",
            "         [[ 0.7885],\n",
            "          [-0.5177],\n",
            "          [ 0.3691]],\n",
            "\n",
            "         [[-0.9013],\n",
            "          [ 0.4129],\n",
            "          [-0.5904]],\n",
            "\n",
            "         [[-0.5238],\n",
            "          [-1.3185],\n",
            "          [-1.2697]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2765],\n",
            "          [-0.0787],\n",
            "          [ 0.3918]],\n",
            "\n",
            "         [[ 0.7885],\n",
            "          [-0.5177],\n",
            "          [ 0.3691]],\n",
            "\n",
            "         [[-0.9013],\n",
            "          [ 0.4129],\n",
            "          [-0.5904]],\n",
            "\n",
            "         [[-0.5238],\n",
            "          [-1.3185],\n",
            "          [-1.2697]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "v_out shape torch.Size([3, 4, 3, 1])\n",
            "v_out: tensor([[[[ 0.0890],\n",
            "          [ 0.0659],\n",
            "          [ 0.0786]],\n",
            "\n",
            "         [[-0.0964],\n",
            "          [-0.1465],\n",
            "          [-0.1020]],\n",
            "\n",
            "         [[-0.3729],\n",
            "          [-0.1134],\n",
            "          [-0.3053]],\n",
            "\n",
            "         [[-1.0683],\n",
            "          [-1.1185],\n",
            "          [-1.1255]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0890],\n",
            "          [ 0.0659],\n",
            "          [ 0.0786]],\n",
            "\n",
            "         [[-0.0964],\n",
            "          [-0.1465],\n",
            "          [-0.1020]],\n",
            "\n",
            "         [[-0.3729],\n",
            "          [-0.1134],\n",
            "          [-0.3053]],\n",
            "\n",
            "         [[-1.0683],\n",
            "          [-1.1185],\n",
            "          [-1.1255]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1906],\n",
            "          [ 0.1760],\n",
            "          [ 0.1841]],\n",
            "\n",
            "         [[ 0.0379],\n",
            "          [-0.0051],\n",
            "          [ 0.0332]],\n",
            "\n",
            "         [[-0.4448],\n",
            "          [-0.2694],\n",
            "          [-0.4003]],\n",
            "\n",
            "         [[-1.1249],\n",
            "          [-1.1570],\n",
            "          [-1.1615]]]], device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
            "v_out shape before: torch.Size([3, 4, 3, 1])\n",
            "v_out shape: torch.Size([3, 3, 4])\n",
            "v_out reshaped: tensor([[[ 0.0890, -0.0964, -0.3729, -1.0683],\n",
            "         [ 0.0659, -0.1465, -0.1134, -1.1185],\n",
            "         [ 0.0786, -0.1020, -0.3053, -1.1255]],\n",
            "\n",
            "        [[ 0.0890, -0.0964, -0.3729, -1.0683],\n",
            "         [ 0.0659, -0.1465, -0.1134, -1.1185],\n",
            "         [ 0.0786, -0.1020, -0.3053, -1.1255]],\n",
            "\n",
            "        [[ 0.1906,  0.0379, -0.4448, -1.1249],\n",
            "         [ 0.1760, -0.0051, -0.2694, -1.1570],\n",
            "         [ 0.1841,  0.0332, -0.4003, -1.1615]]], device='cuda:0',\n",
            "       grad_fn=<ReshapeAliasBackward0>)\n",
            "output shape torch.Size([3, 3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_headed_attention_encoder_decoder():\n",
        "    num_heads = 4\n",
        "    d_attn = 4\n",
        "    d_x = 4\n",
        "    d_z = 4\n",
        "    d_out = 4\n",
        "    d_mid = 4\n",
        "    length_x = 3\n",
        "    length_z = 3\n",
        "    batch_size = 4\n",
        "    padding_mask = torch.tensor([[1, 1, 0]], dtype=torch.int32).to(device)\n",
        "\n",
        "    multi_headed_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], 0.0, True).to(device)\n",
        "    x = torch.rand(batch_size, length_x, d_x).to(device)\n",
        "    z = torch.rand(batch_size, length_z, d_z).to(device)\n",
        "    output = multi_headed_attention(z, x, padding_mask)\n",
        "    print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, length_x, d_out)\n",
        "test_multi_headed_attention_encoder_decoder()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbYma85ffEHr",
        "outputId": "c733e2c4-f56b-409f-e029-2959f7bbcba3"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "queries torch.Size([4, 3, 4])\n",
            "torch.Size([4, 4, 3, 1])\n",
            "torch.Size([4, 4, 3, 1])\n",
            "torch.Size([4, 4, 3, 1])\n",
            "mask tensor([[[[1, 1, 0]]]], device='cuda:0', dtype=torch.int32)\n",
            "mask torch.Size([1, 1, 1, 3])\n",
            "queries: tensor([[[[-0.2298],\n",
            "          [-0.3926],\n",
            "          [-0.4845]],\n",
            "\n",
            "         [[-0.2591],\n",
            "          [-0.4324],\n",
            "          [ 0.0713]],\n",
            "\n",
            "         [[-0.1487],\n",
            "          [-0.0835],\n",
            "          [-0.1806]],\n",
            "\n",
            "         [[ 0.7135],\n",
            "          [ 0.8557],\n",
            "          [ 0.4950]]],\n",
            "\n",
            "\n",
            "        [[[-0.3888],\n",
            "          [-0.3381],\n",
            "          [-0.4572]],\n",
            "\n",
            "         [[-0.2430],\n",
            "          [-0.1997],\n",
            "          [-0.4964]],\n",
            "\n",
            "         [[-0.1004],\n",
            "          [-0.1711],\n",
            "          [-0.4900]],\n",
            "\n",
            "         [[ 1.0472],\n",
            "          [ 1.0148],\n",
            "          [ 0.7370]]],\n",
            "\n",
            "\n",
            "        [[[-0.2425],\n",
            "          [-0.4885],\n",
            "          [-0.2626]],\n",
            "\n",
            "         [[ 0.1059],\n",
            "          [-0.6467],\n",
            "          [-0.0156]],\n",
            "\n",
            "         [[-0.0546],\n",
            "          [-0.2998],\n",
            "          [-0.1688]],\n",
            "\n",
            "         [[ 0.4608],\n",
            "          [ 1.0953],\n",
            "          [ 0.8941]]],\n",
            "\n",
            "\n",
            "        [[[-0.6503],\n",
            "          [-0.5625],\n",
            "          [-0.4481]],\n",
            "\n",
            "         [[-0.2099],\n",
            "          [-0.0177],\n",
            "          [-0.3038]],\n",
            "\n",
            "         [[-0.3177],\n",
            "          [ 0.2686],\n",
            "          [-0.1285]],\n",
            "\n",
            "         [[ 0.6978],\n",
            "          [ 0.6362],\n",
            "          [ 1.1104]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "keys: tensor([[[[ 0.4506],\n",
            "          [ 0.4500],\n",
            "          [ 0.2348]],\n",
            "\n",
            "         [[ 0.0562],\n",
            "          [ 0.1279],\n",
            "          [-0.0156]],\n",
            "\n",
            "         [[ 0.4304],\n",
            "          [ 0.4279],\n",
            "          [ 0.3336]],\n",
            "\n",
            "         [[ 0.1536],\n",
            "          [ 0.1609],\n",
            "          [ 0.2983]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4718],\n",
            "          [ 0.5589],\n",
            "          [ 0.0988]],\n",
            "\n",
            "         [[ 0.0798],\n",
            "          [ 0.2457],\n",
            "          [-0.0504]],\n",
            "\n",
            "         [[ 0.5783],\n",
            "          [ 0.4936],\n",
            "          [ 0.1449]],\n",
            "\n",
            "         [[ 0.0684],\n",
            "          [ 0.3631],\n",
            "          [ 0.3616]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5788],\n",
            "          [ 0.2732],\n",
            "          [ 0.1826]],\n",
            "\n",
            "         [[ 0.3581],\n",
            "          [-0.3379],\n",
            "          [-0.1399]],\n",
            "\n",
            "         [[ 0.4959],\n",
            "          [ 0.4120],\n",
            "          [ 0.5550]],\n",
            "\n",
            "         [[ 0.3287],\n",
            "          [ 0.3160],\n",
            "          [ 0.3188]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4993],\n",
            "          [ 0.3656],\n",
            "          [ 0.7465]],\n",
            "\n",
            "         [[-0.1842],\n",
            "          [-0.0474],\n",
            "          [ 0.0466]],\n",
            "\n",
            "         [[ 0.6092],\n",
            "          [ 0.4571],\n",
            "          [ 0.8577]],\n",
            "\n",
            "         [[ 0.5535],\n",
            "          [ 0.0515],\n",
            "          [ 0.3549]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "values: tensor([[[[-0.1217],\n",
            "          [-0.1759],\n",
            "          [ 0.0253]],\n",
            "\n",
            "         [[ 0.3473],\n",
            "          [ 0.3339],\n",
            "          [ 0.4987]],\n",
            "\n",
            "         [[-0.0670],\n",
            "          [-0.1457],\n",
            "          [-0.2021]],\n",
            "\n",
            "         [[-0.2718],\n",
            "          [-0.3290],\n",
            "          [-0.3617]]],\n",
            "\n",
            "\n",
            "        [[[-0.0712],\n",
            "          [-0.1591],\n",
            "          [-0.0101]],\n",
            "\n",
            "         [[ 0.2086],\n",
            "          [ 0.5606],\n",
            "          [ 0.5872]],\n",
            "\n",
            "         [[-0.1090],\n",
            "          [-0.3236],\n",
            "          [-0.2122]],\n",
            "\n",
            "         [[-0.3836],\n",
            "          [-0.5281],\n",
            "          [-0.2712]]],\n",
            "\n",
            "\n",
            "        [[[-0.2719],\n",
            "          [ 0.3345],\n",
            "          [ 0.3393]],\n",
            "\n",
            "         [[ 0.4892],\n",
            "          [ 0.6161],\n",
            "          [ 0.4939]],\n",
            "\n",
            "         [[-0.4076],\n",
            "          [ 0.1179],\n",
            "          [-0.2496]],\n",
            "\n",
            "         [[-0.5816],\n",
            "          [-0.1797],\n",
            "          [-0.5580]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3974],\n",
            "          [-0.0328],\n",
            "          [ 0.2095]],\n",
            "\n",
            "         [[ 0.8808],\n",
            "          [ 0.2231],\n",
            "          [ 0.5881]],\n",
            "\n",
            "         [[-0.0895],\n",
            "          [ 0.0104],\n",
            "          [-0.1497]],\n",
            "\n",
            "         [[-0.4809],\n",
            "          [-0.2251],\n",
            "          [-0.6220]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "keys_transposed: tensor([[[[ 0.4506,  0.4500,  0.2348]],\n",
            "\n",
            "         [[ 0.0562,  0.1279, -0.0156]],\n",
            "\n",
            "         [[ 0.4304,  0.4279,  0.3336]],\n",
            "\n",
            "         [[ 0.1536,  0.1609,  0.2983]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4718,  0.5589,  0.0988]],\n",
            "\n",
            "         [[ 0.0798,  0.2457, -0.0504]],\n",
            "\n",
            "         [[ 0.5783,  0.4936,  0.1449]],\n",
            "\n",
            "         [[ 0.0684,  0.3631,  0.3616]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5788,  0.2732,  0.1826]],\n",
            "\n",
            "         [[ 0.3581, -0.3379, -0.1399]],\n",
            "\n",
            "         [[ 0.4959,  0.4120,  0.5550]],\n",
            "\n",
            "         [[ 0.3287,  0.3160,  0.3188]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4993,  0.3656,  0.7465]],\n",
            "\n",
            "         [[-0.1842, -0.0474,  0.0466]],\n",
            "\n",
            "         [[ 0.6092,  0.4571,  0.8577]],\n",
            "\n",
            "         [[ 0.5535,  0.0515,  0.3549]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "scores: tensor([[[[-0.1036, -0.1034, -0.0540],\n",
            "          [-0.1769, -0.1766, -0.0922],\n",
            "          [-0.2183, -0.2180, -0.1138]],\n",
            "\n",
            "         [[-0.0146, -0.0331,  0.0040],\n",
            "          [-0.0243, -0.0553,  0.0068],\n",
            "          [ 0.0040,  0.0091, -0.0011]],\n",
            "\n",
            "         [[-0.0640, -0.0636, -0.0496],\n",
            "          [-0.0359, -0.0357, -0.0279],\n",
            "          [-0.0777, -0.0773, -0.0603]],\n",
            "\n",
            "         [[ 0.1096,  0.1148,  0.2128],\n",
            "          [ 0.1314,  0.1377,  0.2553],\n",
            "          [ 0.0760,  0.0797,  0.1477]]],\n",
            "\n",
            "\n",
            "        [[[-0.1834, -0.2173, -0.0384],\n",
            "          [-0.1595, -0.1890, -0.0334],\n",
            "          [-0.2157, -0.2555, -0.0452]],\n",
            "\n",
            "         [[-0.0194, -0.0597,  0.0123],\n",
            "          [-0.0159, -0.0490,  0.0101],\n",
            "          [-0.0396, -0.1220,  0.0250]],\n",
            "\n",
            "         [[-0.0580, -0.0495, -0.0145],\n",
            "          [-0.0990, -0.0845, -0.0248],\n",
            "          [-0.2833, -0.2419, -0.0710]],\n",
            "\n",
            "         [[ 0.0716,  0.3802,  0.3786],\n",
            "          [ 0.0694,  0.3685,  0.3669],\n",
            "          [ 0.0504,  0.2676,  0.2665]]],\n",
            "\n",
            "\n",
            "        [[[-0.1404, -0.0663, -0.0443],\n",
            "          [-0.2827, -0.1334, -0.0892],\n",
            "          [-0.1520, -0.0717, -0.0480]],\n",
            "\n",
            "         [[ 0.0379, -0.0358, -0.0148],\n",
            "          [-0.2316,  0.2185,  0.0905],\n",
            "          [-0.0056,  0.0053,  0.0022]],\n",
            "\n",
            "         [[-0.0271, -0.0225, -0.0303],\n",
            "          [-0.1487, -0.1235, -0.1664],\n",
            "          [-0.0837, -0.0695, -0.0937]],\n",
            "\n",
            "         [[ 0.1514,  0.1456,  0.1469],\n",
            "          [ 0.3600,  0.3461,  0.3492],\n",
            "          [ 0.2939,  0.2825,  0.2851]]],\n",
            "\n",
            "\n",
            "        [[[-0.3247, -0.2377, -0.4855],\n",
            "          [-0.2808, -0.2056, -0.4199],\n",
            "          [-0.2237, -0.1638, -0.3346]],\n",
            "\n",
            "         [[ 0.0387,  0.0099, -0.0098],\n",
            "          [ 0.0033,  0.0008, -0.0008],\n",
            "          [ 0.0560,  0.0144, -0.0142]],\n",
            "\n",
            "         [[-0.1935, -0.1452, -0.2725],\n",
            "          [ 0.1636,  0.1228,  0.2304],\n",
            "          [-0.0783, -0.0587, -0.1103]],\n",
            "\n",
            "         [[ 0.3862,  0.0360,  0.2476],\n",
            "          [ 0.3521,  0.0328,  0.2258],\n",
            "          [ 0.6146,  0.0572,  0.3941]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "scores: torch.Size([4, 4, 3, 3])\n",
            "masks: torch.Size([1, 1, 1, 3])\n",
            "masked scores: tensor([[[[-1.0357e-01, -1.0343e-01, -1.0000e+09],\n",
            "          [-1.7689e-01, -1.7665e-01, -1.0000e+09],\n",
            "          [-2.1833e-01, -2.1803e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.4569e-02, -3.3124e-02, -1.0000e+09],\n",
            "          [-2.4317e-02, -5.5288e-02, -1.0000e+09],\n",
            "          [ 4.0089e-03,  9.1148e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-6.3996e-02, -6.3619e-02, -1.0000e+09],\n",
            "          [-3.5932e-02, -3.5720e-02, -1.0000e+09],\n",
            "          [-7.7748e-02, -7.7290e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.0959e-01,  1.1481e-01, -1.0000e+09],\n",
            "          [ 1.3144e-01,  1.3769e-01, -1.0000e+09],\n",
            "          [ 7.6035e-02,  7.9654e-02, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.8343e-01, -2.1732e-01, -1.0000e+09],\n",
            "          [-1.5952e-01, -1.8899e-01, -1.0000e+09],\n",
            "          [-2.1570e-01, -2.5554e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.9390e-02, -5.9694e-02, -1.0000e+09],\n",
            "          [-1.5932e-02, -4.9048e-02, -1.0000e+09],\n",
            "          [-3.9616e-02, -1.2196e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-5.8036e-02, -4.9542e-02, -1.0000e+09],\n",
            "          [-9.8954e-02, -8.4472e-02, -1.0000e+09],\n",
            "          [-2.8333e-01, -2.4186e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 7.1643e-02,  3.8021e-01, -1.0000e+09],\n",
            "          [ 6.9432e-02,  3.6847e-01, -1.0000e+09],\n",
            "          [ 5.0423e-02,  2.6759e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.4036e-01, -6.6250e-02, -1.0000e+09],\n",
            "          [-2.8272e-01, -1.3345e-01, -1.0000e+09],\n",
            "          [-1.5198e-01, -7.1736e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.7940e-02, -3.5802e-02, -1.0000e+09],\n",
            "          [-2.3159e-01,  2.1854e-01, -1.0000e+09],\n",
            "          [-5.6032e-03,  5.2875e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-2.7053e-02, -2.2478e-02, -1.0000e+09],\n",
            "          [-1.4867e-01, -1.2353e-01, -1.0000e+09],\n",
            "          [-8.3685e-02, -6.9533e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.5144e-01,  1.4560e-01, -1.0000e+09],\n",
            "          [ 3.5997e-01,  3.4607e-01, -1.0000e+09],\n",
            "          [ 2.9386e-01,  2.8252e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-3.2467e-01, -2.3773e-01, -1.0000e+09],\n",
            "          [-2.8084e-01, -2.0564e-01, -1.0000e+09],\n",
            "          [-2.2375e-01, -1.6383e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.8666e-02,  9.9399e-03, -1.0000e+09],\n",
            "          [ 3.2569e-03,  8.3724e-04, -1.0000e+09],\n",
            "          [ 5.5962e-02,  1.4386e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-1.9355e-01, -1.4520e-01, -1.0000e+09],\n",
            "          [ 1.6362e-01,  1.2275e-01, -1.0000e+09],\n",
            "          [-7.8310e-02, -5.8750e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.8619e-01,  3.5966e-02, -1.0000e+09],\n",
            "          [ 3.5214e-01,  3.2795e-02, -1.0000e+09],\n",
            "          [ 6.1457e-01,  5.7236e-02, -1.0000e+09]]]], device='cuda:0',\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "scaled_scores: tensor([[[[-1.0357e-01, -1.0343e-01, -1.0000e+09],\n",
            "          [-1.7689e-01, -1.7665e-01, -1.0000e+09],\n",
            "          [-2.1833e-01, -2.1803e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.4569e-02, -3.3124e-02, -1.0000e+09],\n",
            "          [-2.4317e-02, -5.5288e-02, -1.0000e+09],\n",
            "          [ 4.0089e-03,  9.1148e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-6.3996e-02, -6.3619e-02, -1.0000e+09],\n",
            "          [-3.5932e-02, -3.5720e-02, -1.0000e+09],\n",
            "          [-7.7748e-02, -7.7290e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.0959e-01,  1.1481e-01, -1.0000e+09],\n",
            "          [ 1.3144e-01,  1.3769e-01, -1.0000e+09],\n",
            "          [ 7.6035e-02,  7.9654e-02, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.8343e-01, -2.1732e-01, -1.0000e+09],\n",
            "          [-1.5952e-01, -1.8899e-01, -1.0000e+09],\n",
            "          [-2.1570e-01, -2.5554e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.9390e-02, -5.9694e-02, -1.0000e+09],\n",
            "          [-1.5932e-02, -4.9048e-02, -1.0000e+09],\n",
            "          [-3.9616e-02, -1.2196e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-5.8036e-02, -4.9542e-02, -1.0000e+09],\n",
            "          [-9.8954e-02, -8.4472e-02, -1.0000e+09],\n",
            "          [-2.8333e-01, -2.4186e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 7.1643e-02,  3.8021e-01, -1.0000e+09],\n",
            "          [ 6.9432e-02,  3.6847e-01, -1.0000e+09],\n",
            "          [ 5.0423e-02,  2.6759e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.4036e-01, -6.6250e-02, -1.0000e+09],\n",
            "          [-2.8272e-01, -1.3345e-01, -1.0000e+09],\n",
            "          [-1.5198e-01, -7.1736e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.7940e-02, -3.5802e-02, -1.0000e+09],\n",
            "          [-2.3159e-01,  2.1854e-01, -1.0000e+09],\n",
            "          [-5.6032e-03,  5.2875e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-2.7053e-02, -2.2478e-02, -1.0000e+09],\n",
            "          [-1.4867e-01, -1.2353e-01, -1.0000e+09],\n",
            "          [-8.3685e-02, -6.9533e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.5144e-01,  1.4560e-01, -1.0000e+09],\n",
            "          [ 3.5997e-01,  3.4607e-01, -1.0000e+09],\n",
            "          [ 2.9386e-01,  2.8252e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-3.2467e-01, -2.3773e-01, -1.0000e+09],\n",
            "          [-2.8084e-01, -2.0564e-01, -1.0000e+09],\n",
            "          [-2.2375e-01, -1.6383e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.8666e-02,  9.9399e-03, -1.0000e+09],\n",
            "          [ 3.2569e-03,  8.3724e-04, -1.0000e+09],\n",
            "          [ 5.5962e-02,  1.4386e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-1.9355e-01, -1.4520e-01, -1.0000e+09],\n",
            "          [ 1.6362e-01,  1.2275e-01, -1.0000e+09],\n",
            "          [-7.8310e-02, -5.8750e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.8619e-01,  3.5966e-02, -1.0000e+09],\n",
            "          [ 3.5214e-01,  3.2795e-02, -1.0000e+09],\n",
            "          [ 6.1457e-01,  5.7236e-02, -1.0000e+09]]]], device='cuda:0',\n",
            "       grad_fn=<DivBackward0>)\n",
            "softmax_scores: tensor([[[[0.5000, 0.5000, 0.0000],\n",
            "          [0.4999, 0.5001, 0.0000],\n",
            "          [0.4999, 0.5001, 0.0000]],\n",
            "\n",
            "         [[0.5046, 0.4954, 0.0000],\n",
            "          [0.5077, 0.4923, 0.0000],\n",
            "          [0.4987, 0.5013, 0.0000]],\n",
            "\n",
            "         [[0.4999, 0.5001, 0.0000],\n",
            "          [0.4999, 0.5001, 0.0000],\n",
            "          [0.4999, 0.5001, 0.0000]],\n",
            "\n",
            "         [[0.4987, 0.5013, 0.0000],\n",
            "          [0.4984, 0.5016, 0.0000],\n",
            "          [0.4991, 0.5009, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.5085, 0.4915, 0.0000],\n",
            "          [0.5074, 0.4926, 0.0000],\n",
            "          [0.5100, 0.4900, 0.0000]],\n",
            "\n",
            "         [[0.5101, 0.4899, 0.0000],\n",
            "          [0.5083, 0.4917, 0.0000],\n",
            "          [0.5206, 0.4794, 0.0000]],\n",
            "\n",
            "         [[0.4979, 0.5021, 0.0000],\n",
            "          [0.4964, 0.5036, 0.0000],\n",
            "          [0.4896, 0.5104, 0.0000]],\n",
            "\n",
            "         [[0.4235, 0.5765, 0.0000],\n",
            "          [0.4258, 0.5742, 0.0000],\n",
            "          [0.4459, 0.5541, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.4815, 0.5185, 0.0000],\n",
            "          [0.4628, 0.5372, 0.0000],\n",
            "          [0.4799, 0.5201, 0.0000]],\n",
            "\n",
            "         [[0.5184, 0.4816, 0.0000],\n",
            "          [0.3893, 0.6107, 0.0000],\n",
            "          [0.4973, 0.5027, 0.0000]],\n",
            "\n",
            "         [[0.4989, 0.5011, 0.0000],\n",
            "          [0.4937, 0.5063, 0.0000],\n",
            "          [0.4965, 0.5035, 0.0000]],\n",
            "\n",
            "         [[0.5015, 0.4985, 0.0000],\n",
            "          [0.5035, 0.4965, 0.0000],\n",
            "          [0.5028, 0.4972, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.4783, 0.5217, 0.0000],\n",
            "          [0.4812, 0.5188, 0.0000],\n",
            "          [0.4850, 0.5150, 0.0000]],\n",
            "\n",
            "         [[0.5072, 0.4928, 0.0000],\n",
            "          [0.5006, 0.4994, 0.0000],\n",
            "          [0.5104, 0.4896, 0.0000]],\n",
            "\n",
            "         [[0.4879, 0.5121, 0.0000],\n",
            "          [0.5102, 0.4898, 0.0000],\n",
            "          [0.4951, 0.5049, 0.0000]],\n",
            "\n",
            "         [[0.5867, 0.4133, 0.0000],\n",
            "          [0.5792, 0.4208, 0.0000],\n",
            "          [0.6358, 0.3642, 0.0000]]]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "softmax_socres shape: torch.Size([4, 4, 3, 3])\n",
            "values: tensor([[[[-0.1217],\n",
            "          [-0.1759],\n",
            "          [ 0.0253]],\n",
            "\n",
            "         [[ 0.3473],\n",
            "          [ 0.3339],\n",
            "          [ 0.4987]],\n",
            "\n",
            "         [[-0.0670],\n",
            "          [-0.1457],\n",
            "          [-0.2021]],\n",
            "\n",
            "         [[-0.2718],\n",
            "          [-0.3290],\n",
            "          [-0.3617]]],\n",
            "\n",
            "\n",
            "        [[[-0.0712],\n",
            "          [-0.1591],\n",
            "          [-0.0101]],\n",
            "\n",
            "         [[ 0.2086],\n",
            "          [ 0.5606],\n",
            "          [ 0.5872]],\n",
            "\n",
            "         [[-0.1090],\n",
            "          [-0.3236],\n",
            "          [-0.2122]],\n",
            "\n",
            "         [[-0.3836],\n",
            "          [-0.5281],\n",
            "          [-0.2712]]],\n",
            "\n",
            "\n",
            "        [[[-0.2719],\n",
            "          [ 0.3345],\n",
            "          [ 0.3393]],\n",
            "\n",
            "         [[ 0.4892],\n",
            "          [ 0.6161],\n",
            "          [ 0.4939]],\n",
            "\n",
            "         [[-0.4076],\n",
            "          [ 0.1179],\n",
            "          [-0.2496]],\n",
            "\n",
            "         [[-0.5816],\n",
            "          [-0.1797],\n",
            "          [-0.5580]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3974],\n",
            "          [-0.0328],\n",
            "          [ 0.2095]],\n",
            "\n",
            "         [[ 0.8808],\n",
            "          [ 0.2231],\n",
            "          [ 0.5881]],\n",
            "\n",
            "         [[-0.0895],\n",
            "          [ 0.0104],\n",
            "          [-0.1497]],\n",
            "\n",
            "         [[-0.4809],\n",
            "          [-0.2251],\n",
            "          [-0.6220]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "v_out shape torch.Size([4, 4, 3, 1])\n",
            "v_out: tensor([[[[-0.1488],\n",
            "          [-0.1488],\n",
            "          [-0.1488]],\n",
            "\n",
            "         [[ 0.3407],\n",
            "          [ 0.3407],\n",
            "          [ 0.3406]],\n",
            "\n",
            "         [[-0.1064],\n",
            "          [-0.1064],\n",
            "          [-0.1064]],\n",
            "\n",
            "         [[-0.3005],\n",
            "          [-0.3005],\n",
            "          [-0.3004]]],\n",
            "\n",
            "\n",
            "        [[[-0.1144],\n",
            "          [-0.1145],\n",
            "          [-0.1143]],\n",
            "\n",
            "         [[ 0.3811],\n",
            "          [ 0.3817],\n",
            "          [ 0.3774]],\n",
            "\n",
            "         [[-0.2168],\n",
            "          [-0.2171],\n",
            "          [-0.2185]],\n",
            "\n",
            "         [[-0.4670],\n",
            "          [-0.4666],\n",
            "          [-0.4637]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0425],\n",
            "          [ 0.0539],\n",
            "          [ 0.0434]],\n",
            "\n",
            "         [[ 0.5503],\n",
            "          [ 0.5667],\n",
            "          [ 0.5530]],\n",
            "\n",
            "         [[-0.1443],\n",
            "          [-0.1416],\n",
            "          [-0.1430]],\n",
            "\n",
            "         [[-0.3812],\n",
            "          [-0.3820],\n",
            "          [-0.3818]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1730],\n",
            "          [ 0.1742],\n",
            "          [ 0.1759]],\n",
            "\n",
            "         [[ 0.5567],\n",
            "          [ 0.5523],\n",
            "          [ 0.5588]],\n",
            "\n",
            "         [[-0.0383],\n",
            "          [-0.0406],\n",
            "          [-0.0391]],\n",
            "\n",
            "         [[-0.3751],\n",
            "          [-0.3732],\n",
            "          [-0.3877]]]], device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
            "v_out shape before: torch.Size([4, 4, 3, 1])\n",
            "v_out shape: torch.Size([4, 3, 4])\n",
            "v_out reshaped: tensor([[[-0.1488,  0.3407, -0.1064, -0.3005],\n",
            "         [-0.1488,  0.3407, -0.1064, -0.3005],\n",
            "         [-0.1488,  0.3406, -0.1064, -0.3004]],\n",
            "\n",
            "        [[-0.1144,  0.3811, -0.2168, -0.4670],\n",
            "         [-0.1145,  0.3817, -0.2171, -0.4666],\n",
            "         [-0.1143,  0.3774, -0.2185, -0.4637]],\n",
            "\n",
            "        [[ 0.0425,  0.5503, -0.1443, -0.3812],\n",
            "         [ 0.0539,  0.5667, -0.1416, -0.3820],\n",
            "         [ 0.0434,  0.5530, -0.1430, -0.3818]],\n",
            "\n",
            "        [[ 0.1730,  0.5567, -0.0383, -0.3751],\n",
            "         [ 0.1742,  0.5523, -0.0406, -0.3732],\n",
            "         [ 0.1759,  0.5588, -0.0391, -0.3877]]], device='cuda:0',\n",
            "       grad_fn=<ReshapeAliasBackward0>)\n",
            "output shape torch.Size([4, 3, 4])\n",
            "output: tensor([[[ 0.1426, -0.1900,  0.0498, -0.4583],\n",
            "         [ 0.1426, -0.1900,  0.0498, -0.4583],\n",
            "         [ 0.1426, -0.1900,  0.0498, -0.4583]],\n",
            "\n",
            "        [[ 0.1520, -0.1843, -0.0111, -0.5433],\n",
            "         [ 0.1517, -0.1848, -0.0111, -0.5434],\n",
            "         [ 0.1516, -0.1851, -0.0081, -0.5438]],\n",
            "\n",
            "        [[ 0.1527, -0.1819, -0.0719, -0.5735],\n",
            "         [ 0.1528, -0.1817, -0.0800, -0.5776],\n",
            "         [ 0.1528, -0.1816, -0.0735, -0.5734]],\n",
            "\n",
            "        [[ 0.2126, -0.0883, -0.1099, -0.5799],\n",
            "         [ 0.2128, -0.0882, -0.1072, -0.5813],\n",
            "         [ 0.2158, -0.0836, -0.1158, -0.5827]]], device='cuda:0',\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_headed_attention_decoder_self():\n",
        "    num_heads = 8\n",
        "    d_attn = 8\n",
        "    d_x = 8\n",
        "    d_out = 8\n",
        "    d_mid = 8\n",
        "    length_x = 3\n",
        "    batch_size = 4\n",
        "    padding_mask = torch.tensor([[1, 1, 0], [1, 1, 0], [1, 0, 0], [1, 1, 1]], dtype=torch.int32).to(device)\n",
        "\n",
        "    multi_headed_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_x, d_out, d_mid, MaskStrategy['UNMASKED'], 0.0, True).to(device)\n",
        "    multi_headed_attention.enable_subsequent_mask()\n",
        "    x = torch.rand(batch_size, length_x, d_x).to(device)\n",
        "    output = multi_headed_attention(x, x, padding_mask)\n",
        "    print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, length_x, d_out)\n",
        "test_multi_headed_attention_decoder_self()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vn9s5ak_Yr1B",
        "outputId": "8ede2a0d-d5e4-43af-bb52-00b509b97883"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "queries torch.Size([4, 3, 8])\n",
            "torch.Size([4, 8, 3, 1])\n",
            "torch.Size([4, 8, 3, 1])\n",
            "torch.Size([4, 8, 3, 1])\n",
            "padding mask: torch.Size([4, 1, 3])\n",
            "mask tril tensor([[ True, False, False],\n",
            "        [ True,  True, False],\n",
            "        [ True,  True,  True]], device='cuda:0')\n",
            "merged mask: tensor([[[1, 0, 0],\n",
            "         [1, 1, 0],\n",
            "         [1, 1, 0]],\n",
            "\n",
            "        [[1, 0, 0],\n",
            "         [1, 1, 0],\n",
            "         [1, 1, 0]],\n",
            "\n",
            "        [[1, 0, 0],\n",
            "         [1, 0, 0],\n",
            "         [1, 0, 0]],\n",
            "\n",
            "        [[1, 0, 0],\n",
            "         [1, 1, 0],\n",
            "         [1, 1, 1]]], device='cuda:0', dtype=torch.int32)\n",
            "mask tensor([[[[1, 0, 0],\n",
            "          [1, 1, 0],\n",
            "          [1, 1, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 0, 0],\n",
            "          [1, 1, 0],\n",
            "          [1, 1, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 0, 0],\n",
            "          [1, 0, 0],\n",
            "          [1, 0, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 0, 0],\n",
            "          [1, 1, 0],\n",
            "          [1, 1, 1]]]], device='cuda:0', dtype=torch.int32)\n",
            "mask torch.Size([4, 1, 3, 3])\n",
            "queries: tensor([[[[ 2.8792e-01],\n",
            "          [ 3.0256e-01],\n",
            "          [-3.9677e-02]],\n",
            "\n",
            "         [[ 8.0188e-01],\n",
            "          [ 5.2465e-01],\n",
            "          [ 6.4416e-01]],\n",
            "\n",
            "         [[ 3.1568e-01],\n",
            "          [ 4.8959e-01],\n",
            "          [ 9.5684e-01]],\n",
            "\n",
            "         [[-2.8292e-01],\n",
            "          [ 1.2687e-01],\n",
            "          [-3.1234e-01]],\n",
            "\n",
            "         [[ 7.6956e-02],\n",
            "          [ 4.8025e-01],\n",
            "          [ 3.8537e-01]],\n",
            "\n",
            "         [[-3.6390e-01],\n",
            "          [-3.7672e-01],\n",
            "          [-8.2057e-01]],\n",
            "\n",
            "         [[ 3.0520e-01],\n",
            "          [ 2.9588e-01],\n",
            "          [ 7.2760e-02]],\n",
            "\n",
            "         [[ 4.4399e-01],\n",
            "          [ 5.4580e-01],\n",
            "          [ 3.1898e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.4692e-01],\n",
            "          [ 1.2710e-01],\n",
            "          [-1.2809e-01]],\n",
            "\n",
            "         [[ 4.1641e-01],\n",
            "          [ 6.8935e-01],\n",
            "          [ 2.7090e-01]],\n",
            "\n",
            "         [[ 4.6507e-01],\n",
            "          [ 7.3577e-01],\n",
            "          [ 5.3853e-01]],\n",
            "\n",
            "         [[-9.3184e-02],\n",
            "          [-2.1053e-01],\n",
            "          [ 3.3745e-01]],\n",
            "\n",
            "         [[ 4.3646e-01],\n",
            "          [ 3.8362e-01],\n",
            "          [ 7.9303e-01]],\n",
            "\n",
            "         [[-3.7769e-01],\n",
            "          [-5.6144e-01],\n",
            "          [-5.3031e-01]],\n",
            "\n",
            "         [[ 1.4465e-01],\n",
            "          [ 2.3583e-01],\n",
            "          [ 1.7842e-01]],\n",
            "\n",
            "         [[-1.7607e-01],\n",
            "          [ 1.8183e-01],\n",
            "          [ 8.4074e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.8981e-02],\n",
            "          [ 1.4711e-01],\n",
            "          [-1.3095e-01]],\n",
            "\n",
            "         [[ 6.1351e-01],\n",
            "          [ 6.8854e-01],\n",
            "          [ 4.0509e-01]],\n",
            "\n",
            "         [[ 3.6233e-01],\n",
            "          [ 8.8673e-02],\n",
            "          [ 5.2742e-01]],\n",
            "\n",
            "         [[-1.9211e-04],\n",
            "          [ 8.8756e-02],\n",
            "          [ 5.9930e-02]],\n",
            "\n",
            "         [[ 3.7186e-01],\n",
            "          [ 3.6667e-01],\n",
            "          [ 6.9956e-01]],\n",
            "\n",
            "         [[ 3.6163e-02],\n",
            "          [-2.1034e-01],\n",
            "          [-2.9985e-01]],\n",
            "\n",
            "         [[ 3.8461e-01],\n",
            "          [ 3.7383e-01],\n",
            "          [ 1.0265e-01]],\n",
            "\n",
            "         [[-1.0375e-02],\n",
            "          [ 2.1604e-01],\n",
            "          [-1.7259e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7378e-02],\n",
            "          [-9.5055e-02],\n",
            "          [ 1.8910e-01]],\n",
            "\n",
            "         [[ 8.1435e-01],\n",
            "          [ 5.6710e-01],\n",
            "          [ 6.4457e-01]],\n",
            "\n",
            "         [[ 4.4089e-01],\n",
            "          [ 7.2155e-01],\n",
            "          [ 2.5555e-01]],\n",
            "\n",
            "         [[-1.3263e-01],\n",
            "          [ 1.8870e-01],\n",
            "          [ 6.1234e-02]],\n",
            "\n",
            "         [[ 3.8301e-01],\n",
            "          [ 7.1023e-01],\n",
            "          [ 4.8682e-01]],\n",
            "\n",
            "         [[-8.2488e-02],\n",
            "          [-4.7256e-01],\n",
            "          [-2.7025e-01]],\n",
            "\n",
            "         [[ 3.1632e-01],\n",
            "          [ 3.4908e-01],\n",
            "          [ 2.4880e-01]],\n",
            "\n",
            "         [[-1.8838e-02],\n",
            "          [-2.0652e-02],\n",
            "          [ 6.3491e-02]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "keys: tensor([[[[ 0.1226],\n",
            "          [ 0.1854],\n",
            "          [-0.1382]],\n",
            "\n",
            "         [[-0.2115],\n",
            "          [-0.5868],\n",
            "          [-0.2015]],\n",
            "\n",
            "         [[ 0.7872],\n",
            "          [ 0.8159],\n",
            "          [ 0.8837]],\n",
            "\n",
            "         [[ 0.3559],\n",
            "          [ 0.1374],\n",
            "          [-0.0599]],\n",
            "\n",
            "         [[ 0.2820],\n",
            "          [ 0.5458],\n",
            "          [ 0.2374]],\n",
            "\n",
            "         [[-0.1841],\n",
            "          [-0.0684],\n",
            "          [-0.2472]],\n",
            "\n",
            "         [[-0.5582],\n",
            "          [-0.5298],\n",
            "          [-0.6967]],\n",
            "\n",
            "         [[-0.1017],\n",
            "          [-0.1159],\n",
            "          [-0.4072]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0802],\n",
            "          [ 0.0215],\n",
            "          [ 0.3208]],\n",
            "\n",
            "         [[-0.0065],\n",
            "          [-0.2861],\n",
            "          [-0.5867]],\n",
            "\n",
            "         [[ 0.4539],\n",
            "          [ 0.7375],\n",
            "          [ 0.7037]],\n",
            "\n",
            "         [[ 0.1229],\n",
            "          [ 0.0885],\n",
            "          [-0.0995]],\n",
            "\n",
            "         [[ 0.4746],\n",
            "          [ 0.2544],\n",
            "          [ 0.7756]],\n",
            "\n",
            "         [[ 0.2878],\n",
            "          [-0.0281],\n",
            "          [ 0.1955]],\n",
            "\n",
            "         [[ 0.1232],\n",
            "          [-0.5163],\n",
            "          [-0.1703]],\n",
            "\n",
            "         [[-0.2377],\n",
            "          [-0.3342],\n",
            "          [-0.2900]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1075],\n",
            "          [ 0.4581],\n",
            "          [ 0.1609]],\n",
            "\n",
            "         [[ 0.0512],\n",
            "          [-0.4968],\n",
            "          [-0.3522]],\n",
            "\n",
            "         [[ 0.6164],\n",
            "          [ 0.7586],\n",
            "          [ 0.4726]],\n",
            "\n",
            "         [[ 0.4581],\n",
            "          [ 0.3742],\n",
            "          [ 0.1426]],\n",
            "\n",
            "         [[ 0.4243],\n",
            "          [ 0.5207],\n",
            "          [ 0.3910]],\n",
            "\n",
            "         [[ 0.2767],\n",
            "          [-0.0271],\n",
            "          [ 0.1010]],\n",
            "\n",
            "         [[ 0.1638],\n",
            "          [-0.3639],\n",
            "          [-0.1030]],\n",
            "\n",
            "         [[-0.3561],\n",
            "          [-0.1732],\n",
            "          [-0.2984]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1009],\n",
            "          [ 0.2650],\n",
            "          [ 0.2809]],\n",
            "\n",
            "         [[-0.1294],\n",
            "          [-0.4353],\n",
            "          [-0.5913]],\n",
            "\n",
            "         [[ 0.6668],\n",
            "          [ 0.8515],\n",
            "          [ 0.4827]],\n",
            "\n",
            "         [[ 0.5006],\n",
            "          [ 0.0265],\n",
            "          [ 0.3031]],\n",
            "\n",
            "         [[ 0.2061],\n",
            "          [ 0.5461],\n",
            "          [ 0.4582]],\n",
            "\n",
            "         [[-0.0362],\n",
            "          [ 0.2156],\n",
            "          [ 0.0061]],\n",
            "\n",
            "         [[-0.2129],\n",
            "          [-0.2573],\n",
            "          [-0.3080]],\n",
            "\n",
            "         [[-0.4043],\n",
            "          [-0.5952],\n",
            "          [-0.0241]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "values: tensor([[[[-0.4421],\n",
            "          [-0.4828],\n",
            "          [-0.3752]],\n",
            "\n",
            "         [[ 0.6036],\n",
            "          [ 0.6242],\n",
            "          [ 0.8511]],\n",
            "\n",
            "         [[ 0.0412],\n",
            "          [ 0.0077],\n",
            "          [ 0.2528]],\n",
            "\n",
            "         [[ 0.7564],\n",
            "          [ 0.9165],\n",
            "          [ 1.0039]],\n",
            "\n",
            "         [[-0.1312],\n",
            "          [-0.2673],\n",
            "          [-0.2868]],\n",
            "\n",
            "         [[ 0.3612],\n",
            "          [ 0.2347],\n",
            "          [ 0.0306]],\n",
            "\n",
            "         [[-0.8842],\n",
            "          [-0.4901],\n",
            "          [-0.9032]],\n",
            "\n",
            "         [[-0.2133],\n",
            "          [-0.2283],\n",
            "          [-0.3304]]],\n",
            "\n",
            "\n",
            "        [[[-0.3210],\n",
            "          [-0.2037],\n",
            "          [-0.3132]],\n",
            "\n",
            "         [[ 0.0236],\n",
            "          [ 0.7791],\n",
            "          [ 0.3552]],\n",
            "\n",
            "         [[-0.2618],\n",
            "          [ 0.0232],\n",
            "          [-0.1934]],\n",
            "\n",
            "         [[ 0.6312],\n",
            "          [ 0.9531],\n",
            "          [ 0.8175]],\n",
            "\n",
            "         [[ 0.4339],\n",
            "          [-0.1309],\n",
            "          [ 0.1612]],\n",
            "\n",
            "         [[-0.1243],\n",
            "          [ 0.1641],\n",
            "          [-0.1817]],\n",
            "\n",
            "         [[-0.5422],\n",
            "          [-0.6978],\n",
            "          [-0.3557]],\n",
            "\n",
            "         [[-0.4952],\n",
            "          [-0.3138],\n",
            "          [-0.3588]]],\n",
            "\n",
            "\n",
            "        [[[-0.3404],\n",
            "          [-0.2503],\n",
            "          [-0.1635]],\n",
            "\n",
            "         [[-0.1434],\n",
            "          [ 0.5357],\n",
            "          [ 0.5229]],\n",
            "\n",
            "         [[-0.5624],\n",
            "          [-0.2688],\n",
            "          [-0.2555]],\n",
            "\n",
            "         [[ 0.8203],\n",
            "          [ 0.6791],\n",
            "          [ 0.6522]],\n",
            "\n",
            "         [[ 0.3699],\n",
            "          [ 0.1479],\n",
            "          [ 0.2772]],\n",
            "\n",
            "         [[ 0.3809],\n",
            "          [ 0.3575],\n",
            "          [ 0.0997]],\n",
            "\n",
            "         [[-0.5416],\n",
            "          [-0.6572],\n",
            "          [-0.3753]],\n",
            "\n",
            "         [[-0.3194],\n",
            "          [-0.0777],\n",
            "          [-0.1731]]],\n",
            "\n",
            "\n",
            "        [[[-0.1762],\n",
            "          [-0.0455],\n",
            "          [-0.2318]],\n",
            "\n",
            "         [[ 0.4854],\n",
            "          [ 0.5005],\n",
            "          [ 0.7248]],\n",
            "\n",
            "         [[-0.3959],\n",
            "          [-0.3573],\n",
            "          [-0.1602]],\n",
            "\n",
            "         [[ 0.8613],\n",
            "          [ 1.0814],\n",
            "          [ 0.7300]],\n",
            "\n",
            "         [[ 0.2179],\n",
            "          [ 0.1167],\n",
            "          [ 0.1899]],\n",
            "\n",
            "         [[ 0.5898],\n",
            "          [ 0.1046],\n",
            "          [ 0.2981]],\n",
            "\n",
            "         [[-0.6992],\n",
            "          [-0.5217],\n",
            "          [-0.4574]],\n",
            "\n",
            "         [[-0.0751],\n",
            "          [-0.3024],\n",
            "          [-0.1810]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "keys_transposed: tensor([[[[ 0.1226,  0.1854, -0.1382]],\n",
            "\n",
            "         [[-0.2115, -0.5868, -0.2015]],\n",
            "\n",
            "         [[ 0.7872,  0.8159,  0.8837]],\n",
            "\n",
            "         [[ 0.3559,  0.1374, -0.0599]],\n",
            "\n",
            "         [[ 0.2820,  0.5458,  0.2374]],\n",
            "\n",
            "         [[-0.1841, -0.0684, -0.2472]],\n",
            "\n",
            "         [[-0.5582, -0.5298, -0.6967]],\n",
            "\n",
            "         [[-0.1017, -0.1159, -0.4072]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0802,  0.0215,  0.3208]],\n",
            "\n",
            "         [[-0.0065, -0.2861, -0.5867]],\n",
            "\n",
            "         [[ 0.4539,  0.7375,  0.7037]],\n",
            "\n",
            "         [[ 0.1229,  0.0885, -0.0995]],\n",
            "\n",
            "         [[ 0.4746,  0.2544,  0.7756]],\n",
            "\n",
            "         [[ 0.2878, -0.0281,  0.1955]],\n",
            "\n",
            "         [[ 0.1232, -0.5163, -0.1703]],\n",
            "\n",
            "         [[-0.2377, -0.3342, -0.2900]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1075,  0.4581,  0.1609]],\n",
            "\n",
            "         [[ 0.0512, -0.4968, -0.3522]],\n",
            "\n",
            "         [[ 0.6164,  0.7586,  0.4726]],\n",
            "\n",
            "         [[ 0.4581,  0.3742,  0.1426]],\n",
            "\n",
            "         [[ 0.4243,  0.5207,  0.3910]],\n",
            "\n",
            "         [[ 0.2767, -0.0271,  0.1010]],\n",
            "\n",
            "         [[ 0.1638, -0.3639, -0.1030]],\n",
            "\n",
            "         [[-0.3561, -0.1732, -0.2984]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1009,  0.2650,  0.2809]],\n",
            "\n",
            "         [[-0.1294, -0.4353, -0.5913]],\n",
            "\n",
            "         [[ 0.6668,  0.8515,  0.4827]],\n",
            "\n",
            "         [[ 0.5006,  0.0265,  0.3031]],\n",
            "\n",
            "         [[ 0.2061,  0.5461,  0.4582]],\n",
            "\n",
            "         [[-0.0362,  0.2156,  0.0061]],\n",
            "\n",
            "         [[-0.2129, -0.2573, -0.3080]],\n",
            "\n",
            "         [[-0.4043, -0.5952, -0.0241]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "scores: tensor([[[[ 3.5311e-02,  5.3369e-02, -3.9789e-02],\n",
            "          [ 3.7107e-02,  5.6083e-02, -4.1812e-02],\n",
            "          [-4.8661e-03, -7.3547e-03,  5.4832e-03]],\n",
            "\n",
            "         [[-1.6956e-01, -4.7052e-01, -1.6156e-01],\n",
            "          [-1.1094e-01, -3.0785e-01, -1.0570e-01],\n",
            "          [-1.3621e-01, -3.7798e-01, -1.2978e-01]],\n",
            "\n",
            "         [[ 2.4849e-01,  2.5756e-01,  2.7898e-01],\n",
            "          [ 3.8538e-01,  3.9945e-01,  4.3266e-01],\n",
            "          [ 7.5319e-01,  7.8068e-01,  8.4560e-01]],\n",
            "\n",
            "         [[-1.0070e-01, -3.8872e-02,  1.6959e-02],\n",
            "          [ 4.5158e-02,  1.7432e-02, -7.6052e-03],\n",
            "          [-1.1117e-01, -4.2915e-02,  1.8723e-02]],\n",
            "\n",
            "         [[ 2.1702e-02,  4.2006e-02,  1.8273e-02],\n",
            "          [ 1.3544e-01,  2.6214e-01,  1.1403e-01],\n",
            "          [ 1.0868e-01,  2.1035e-01,  9.1503e-02]],\n",
            "\n",
            "         [[ 6.6980e-02,  2.4908e-02,  8.9973e-02],\n",
            "          [ 6.9339e-02,  2.5785e-02,  9.3143e-02],\n",
            "          [ 1.5103e-01,  5.6166e-02,  2.0288e-01]],\n",
            "\n",
            "         [[-1.7036e-01, -1.6168e-01, -2.1262e-01],\n",
            "          [-1.6516e-01, -1.5675e-01, -2.0613e-01],\n",
            "          [-4.0614e-02, -3.8546e-02, -5.0690e-02]],\n",
            "\n",
            "         [[-4.5149e-02, -5.1453e-02, -1.8079e-01],\n",
            "          [-5.5502e-02, -6.3251e-02, -2.2224e-01],\n",
            "          [-3.2437e-02, -3.6965e-02, -1.2988e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.1781e-02, -3.1585e-03, -4.7134e-02],\n",
            "          [ 1.0191e-02,  2.7324e-03,  4.0775e-02],\n",
            "          [-1.0271e-02, -2.7537e-03, -4.1093e-02]],\n",
            "\n",
            "         [[-2.7198e-03, -1.1912e-01, -2.4431e-01],\n",
            "          [-4.5027e-03, -1.9720e-01, -4.0445e-01],\n",
            "          [-1.7694e-03, -7.7495e-02, -1.5894e-01]],\n",
            "\n",
            "         [[ 2.1110e-01,  3.4300e-01,  3.2726e-01],\n",
            "          [ 3.3397e-01,  5.4264e-01,  5.1774e-01],\n",
            "          [ 2.4444e-01,  3.9717e-01,  3.7894e-01]],\n",
            "\n",
            "         [[-1.1456e-02, -8.2510e-03,  9.2689e-03],\n",
            "          [-2.5883e-02, -1.8641e-02,  2.0941e-02],\n",
            "          [ 4.1486e-02,  2.9879e-02, -3.3566e-02]],\n",
            "\n",
            "         [[ 2.0714e-01,  1.1106e-01,  3.3852e-01],\n",
            "          [ 1.8207e-01,  9.7613e-02,  2.9754e-01],\n",
            "          [ 3.7637e-01,  2.0178e-01,  6.1508e-01]],\n",
            "\n",
            "         [[-1.0870e-01,  1.0596e-02, -7.3845e-02],\n",
            "          [-1.6158e-01,  1.5751e-02, -1.0977e-01],\n",
            "          [-1.5262e-01,  1.4878e-02, -1.0368e-01]],\n",
            "\n",
            "         [[ 1.7817e-02, -7.4687e-02, -2.4628e-02],\n",
            "          [ 2.9047e-02, -1.2176e-01, -4.0151e-02],\n",
            "          [ 2.1976e-02, -9.2123e-02, -3.0377e-02]],\n",
            "\n",
            "         [[ 4.1845e-02,  5.8840e-02,  5.1065e-02],\n",
            "          [-4.3214e-02, -6.0765e-02, -5.2736e-02],\n",
            "          [-1.9982e-02, -2.8097e-02, -2.4384e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.1916e-03,  1.7856e-02,  6.2714e-03],\n",
            "          [ 1.5819e-02,  6.7389e-02,  2.3668e-02],\n",
            "          [-1.4081e-02, -5.9988e-02, -2.1068e-02]],\n",
            "\n",
            "         [[ 3.1428e-02, -3.0481e-01, -2.1606e-01],\n",
            "          [ 3.5271e-02, -3.4209e-01, -2.4248e-01],\n",
            "          [ 2.0751e-02, -2.0126e-01, -1.4266e-01]],\n",
            "\n",
            "         [[ 2.2333e-01,  2.7486e-01,  1.7123e-01],\n",
            "          [ 5.4657e-02,  6.7267e-02,  4.1905e-02],\n",
            "          [ 3.2510e-01,  4.0010e-01,  2.4925e-01]],\n",
            "\n",
            "         [[-8.8006e-05, -7.1878e-05, -2.7388e-05],\n",
            "          [ 4.0660e-02,  3.3209e-02,  1.2654e-02],\n",
            "          [ 2.7455e-02,  2.2423e-02,  8.5439e-03]],\n",
            "\n",
            "         [[ 1.5779e-01,  1.9361e-01,  1.4540e-01],\n",
            "          [ 1.5559e-01,  1.9091e-01,  1.4337e-01],\n",
            "          [ 2.9684e-01,  3.6423e-01,  2.7354e-01]],\n",
            "\n",
            "         [[ 1.0008e-02, -9.8108e-04,  3.6531e-03],\n",
            "          [-5.8209e-02,  5.7064e-03, -2.1249e-02],\n",
            "          [-8.2979e-02,  8.1347e-03, -3.0291e-02]],\n",
            "\n",
            "         [[ 6.3016e-02, -1.3996e-01, -3.9600e-02],\n",
            "          [ 6.1248e-02, -1.3604e-01, -3.8490e-02],\n",
            "          [ 1.6818e-02, -3.7354e-02, -1.0569e-02]],\n",
            "\n",
            "         [[ 3.6942e-03,  1.7972e-03,  3.0959e-03],\n",
            "          [-7.6925e-02, -3.7424e-02, -6.4466e-02],\n",
            "          [ 6.1453e-02,  2.9897e-02,  5.1500e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7722e-03,  9.9036e-03,  1.0499e-02],\n",
            "          [-9.5929e-03, -2.5185e-02, -2.6700e-02],\n",
            "          [ 1.9084e-02,  5.0102e-02,  5.3115e-02]],\n",
            "\n",
            "         [[-1.0541e-01, -3.5452e-01, -4.8154e-01],\n",
            "          [-7.3408e-02, -2.4688e-01, -3.3534e-01],\n",
            "          [-8.3435e-02, -2.8061e-01, -3.8114e-01]],\n",
            "\n",
            "         [[ 2.9399e-01,  3.7543e-01,  2.1282e-01],\n",
            "          [ 4.8114e-01,  6.1441e-01,  3.4830e-01],\n",
            "          [ 1.7040e-01,  2.1760e-01,  1.2335e-01]],\n",
            "\n",
            "         [[-6.6400e-02, -3.5135e-03, -4.0203e-02],\n",
            "          [ 9.4474e-02,  4.9990e-03,  5.7200e-02],\n",
            "          [ 3.0656e-02,  1.6222e-03,  1.8561e-02]],\n",
            "\n",
            "         [[ 7.8938e-02,  2.0916e-01,  1.7550e-01],\n",
            "          [ 1.4638e-01,  3.8785e-01,  3.2544e-01],\n",
            "          [ 1.0033e-01,  2.6585e-01,  2.2306e-01]],\n",
            "\n",
            "         [[ 2.9879e-03, -1.7785e-02, -5.0117e-04],\n",
            "          [ 1.7117e-02, -1.0189e-01, -2.8712e-03],\n",
            "          [ 9.7892e-03, -5.8269e-02, -1.6420e-03]],\n",
            "\n",
            "         [[-6.7342e-02, -8.1387e-02, -9.7423e-02],\n",
            "          [-7.4318e-02, -8.9817e-02, -1.0751e-01],\n",
            "          [-5.2969e-02, -6.4016e-02, -7.6629e-02]],\n",
            "\n",
            "         [[ 7.6162e-03,  1.1213e-02,  4.5375e-04],\n",
            "          [ 8.3498e-03,  1.2293e-02,  4.9746e-04],\n",
            "          [-2.5670e-02, -3.7792e-02, -1.5293e-03]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "scores: torch.Size([4, 8, 3, 3])\n",
            "masks: torch.Size([4, 1, 3, 3])\n",
            "masked scores: tensor([[[[ 3.5311e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.7107e-02,  5.6083e-02, -1.0000e+09],\n",
            "          [-4.8661e-03, -7.3547e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-1.6956e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.1094e-01, -3.0785e-01, -1.0000e+09],\n",
            "          [-1.3621e-01, -3.7798e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.4849e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.8538e-01,  3.9945e-01, -1.0000e+09],\n",
            "          [ 7.5319e-01,  7.8068e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.0070e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.5158e-02,  1.7432e-02, -1.0000e+09],\n",
            "          [-1.1117e-01, -4.2915e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.1702e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.3544e-01,  2.6214e-01, -1.0000e+09],\n",
            "          [ 1.0868e-01,  2.1035e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 6.6980e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 6.9339e-02,  2.5785e-02, -1.0000e+09],\n",
            "          [ 1.5103e-01,  5.6166e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-1.7036e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.6516e-01, -1.5675e-01, -1.0000e+09],\n",
            "          [-4.0614e-02, -3.8546e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-4.5149e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-5.5502e-02, -6.3251e-02, -1.0000e+09],\n",
            "          [-3.2437e-02, -3.6965e-02, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.1781e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.0191e-02,  2.7324e-03, -1.0000e+09],\n",
            "          [-1.0271e-02, -2.7537e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-2.7198e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [-4.5027e-03, -1.9720e-01, -1.0000e+09],\n",
            "          [-1.7694e-03, -7.7495e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.1110e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.3397e-01,  5.4264e-01, -1.0000e+09],\n",
            "          [ 2.4444e-01,  3.9717e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.1456e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-2.5883e-02, -1.8641e-02, -1.0000e+09],\n",
            "          [ 4.1486e-02,  2.9879e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.0714e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.8207e-01,  9.7613e-02, -1.0000e+09],\n",
            "          [ 3.7637e-01,  2.0178e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.0870e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.6158e-01,  1.5751e-02, -1.0000e+09],\n",
            "          [-1.5262e-01,  1.4878e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.7817e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.9047e-02, -1.2176e-01, -1.0000e+09],\n",
            "          [ 2.1976e-02, -9.2123e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 4.1845e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-4.3214e-02, -6.0765e-02, -1.0000e+09],\n",
            "          [-1.9982e-02, -2.8097e-02, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 4.1916e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.5819e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.4081e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.1428e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.5271e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.0751e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.2333e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 5.4657e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.2510e-01, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[-8.8006e-05, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.0660e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.7455e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.5779e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.5559e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.9684e-01, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.0008e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-5.8209e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-8.2979e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 6.3016e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 6.1248e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.6818e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.6942e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [-7.6925e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 6.1453e-02, -1.0000e+09, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7722e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [-9.5929e-03, -2.5185e-02, -1.0000e+09],\n",
            "          [ 1.9084e-02,  5.0102e-02,  5.3115e-02]],\n",
            "\n",
            "         [[-1.0541e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-7.3408e-02, -2.4688e-01, -1.0000e+09],\n",
            "          [-8.3435e-02, -2.8061e-01, -3.8114e-01]],\n",
            "\n",
            "         [[ 2.9399e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.8114e-01,  6.1441e-01, -1.0000e+09],\n",
            "          [ 1.7040e-01,  2.1760e-01,  1.2335e-01]],\n",
            "\n",
            "         [[-6.6400e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 9.4474e-02,  4.9990e-03, -1.0000e+09],\n",
            "          [ 3.0656e-02,  1.6222e-03,  1.8561e-02]],\n",
            "\n",
            "         [[ 7.8938e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.4638e-01,  3.8785e-01, -1.0000e+09],\n",
            "          [ 1.0033e-01,  2.6585e-01,  2.2306e-01]],\n",
            "\n",
            "         [[ 2.9879e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.7117e-02, -1.0189e-01, -1.0000e+09],\n",
            "          [ 9.7892e-03, -5.8269e-02, -1.6420e-03]],\n",
            "\n",
            "         [[-6.7342e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-7.4318e-02, -8.9817e-02, -1.0000e+09],\n",
            "          [-5.2969e-02, -6.4016e-02, -7.6629e-02]],\n",
            "\n",
            "         [[ 7.6162e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 8.3498e-03,  1.2293e-02, -1.0000e+09],\n",
            "          [-2.5670e-02, -3.7792e-02, -1.5293e-03]]]], device='cuda:0',\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "scaled_scores: tensor([[[[ 3.5311e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.7107e-02,  5.6083e-02, -1.0000e+09],\n",
            "          [-4.8661e-03, -7.3547e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-1.6956e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.1094e-01, -3.0785e-01, -1.0000e+09],\n",
            "          [-1.3621e-01, -3.7798e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.4849e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.8538e-01,  3.9945e-01, -1.0000e+09],\n",
            "          [ 7.5319e-01,  7.8068e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.0070e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.5158e-02,  1.7432e-02, -1.0000e+09],\n",
            "          [-1.1117e-01, -4.2915e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.1702e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.3544e-01,  2.6214e-01, -1.0000e+09],\n",
            "          [ 1.0868e-01,  2.1035e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 6.6980e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 6.9339e-02,  2.5785e-02, -1.0000e+09],\n",
            "          [ 1.5103e-01,  5.6166e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-1.7036e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.6516e-01, -1.5675e-01, -1.0000e+09],\n",
            "          [-4.0614e-02, -3.8546e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-4.5149e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-5.5502e-02, -6.3251e-02, -1.0000e+09],\n",
            "          [-3.2437e-02, -3.6965e-02, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.1781e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.0191e-02,  2.7324e-03, -1.0000e+09],\n",
            "          [-1.0271e-02, -2.7537e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-2.7198e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [-4.5027e-03, -1.9720e-01, -1.0000e+09],\n",
            "          [-1.7694e-03, -7.7495e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.1110e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.3397e-01,  5.4264e-01, -1.0000e+09],\n",
            "          [ 2.4444e-01,  3.9717e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.1456e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-2.5883e-02, -1.8641e-02, -1.0000e+09],\n",
            "          [ 4.1486e-02,  2.9879e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.0714e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.8207e-01,  9.7613e-02, -1.0000e+09],\n",
            "          [ 3.7637e-01,  2.0178e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.0870e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.6158e-01,  1.5751e-02, -1.0000e+09],\n",
            "          [-1.5262e-01,  1.4878e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.7817e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.9047e-02, -1.2176e-01, -1.0000e+09],\n",
            "          [ 2.1976e-02, -9.2123e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 4.1845e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-4.3214e-02, -6.0765e-02, -1.0000e+09],\n",
            "          [-1.9982e-02, -2.8097e-02, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 4.1916e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.5819e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.4081e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.1428e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.5271e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.0751e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.2333e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 5.4657e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.2510e-01, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[-8.8006e-05, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.0660e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.7455e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.5779e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.5559e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.9684e-01, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.0008e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-5.8209e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-8.2979e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 6.3016e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 6.1248e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.6818e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.6942e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [-7.6925e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 6.1453e-02, -1.0000e+09, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7722e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [-9.5929e-03, -2.5185e-02, -1.0000e+09],\n",
            "          [ 1.9084e-02,  5.0102e-02,  5.3115e-02]],\n",
            "\n",
            "         [[-1.0541e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-7.3408e-02, -2.4688e-01, -1.0000e+09],\n",
            "          [-8.3435e-02, -2.8061e-01, -3.8114e-01]],\n",
            "\n",
            "         [[ 2.9399e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.8114e-01,  6.1441e-01, -1.0000e+09],\n",
            "          [ 1.7040e-01,  2.1760e-01,  1.2335e-01]],\n",
            "\n",
            "         [[-6.6400e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 9.4474e-02,  4.9990e-03, -1.0000e+09],\n",
            "          [ 3.0656e-02,  1.6222e-03,  1.8561e-02]],\n",
            "\n",
            "         [[ 7.8938e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.4638e-01,  3.8785e-01, -1.0000e+09],\n",
            "          [ 1.0033e-01,  2.6585e-01,  2.2306e-01]],\n",
            "\n",
            "         [[ 2.9879e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.7117e-02, -1.0189e-01, -1.0000e+09],\n",
            "          [ 9.7892e-03, -5.8269e-02, -1.6420e-03]],\n",
            "\n",
            "         [[-6.7342e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-7.4318e-02, -8.9817e-02, -1.0000e+09],\n",
            "          [-5.2969e-02, -6.4016e-02, -7.6629e-02]],\n",
            "\n",
            "         [[ 7.6162e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 8.3498e-03,  1.2293e-02, -1.0000e+09],\n",
            "          [-2.5670e-02, -3.7792e-02, -1.5293e-03]]]], device='cuda:0',\n",
            "       grad_fn=<DivBackward0>)\n",
            "softmax_scores: tensor([[[[1.0000, 0.0000, 0.0000],\n",
            "          [0.4953, 0.5047, 0.0000],\n",
            "          [0.5006, 0.4994, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5491, 0.4509, 0.0000],\n",
            "          [0.5602, 0.4398, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4965, 0.5035, 0.0000],\n",
            "          [0.4931, 0.5069, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5069, 0.4931, 0.0000],\n",
            "          [0.4829, 0.5171, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4684, 0.5316, 0.0000],\n",
            "          [0.4746, 0.5254, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5109, 0.4891, 0.0000],\n",
            "          [0.5237, 0.4763, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4979, 0.5021, 0.0000],\n",
            "          [0.4995, 0.5005, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5019, 0.4981, 0.0000],\n",
            "          [0.5011, 0.4989, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000],\n",
            "          [0.5019, 0.4981, 0.0000],\n",
            "          [0.4981, 0.5019, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5480, 0.4520, 0.0000],\n",
            "          [0.5189, 0.4811, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4480, 0.5520, 0.0000],\n",
            "          [0.4619, 0.5381, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4982, 0.5018, 0.0000],\n",
            "          [0.5029, 0.4971, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5211, 0.4789, 0.0000],\n",
            "          [0.5435, 0.4565, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4558, 0.5442, 0.0000],\n",
            "          [0.4582, 0.5418, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5376, 0.4624, 0.0000],\n",
            "          [0.5285, 0.4715, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5044, 0.4956, 0.0000],\n",
            "          [0.5020, 0.4980, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000],\n",
            "          [0.5039, 0.4961, 0.0000],\n",
            "          [0.3261, 0.3364, 0.3374]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5433, 0.4567, 0.0000],\n",
            "          [0.3901, 0.3203, 0.2896]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4667, 0.5333, 0.0000],\n",
            "          [0.3331, 0.3492, 0.3178]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5224, 0.4776, 0.0000],\n",
            "          [0.3379, 0.3282, 0.3338]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4399, 0.5601, 0.0000],\n",
            "          [0.3021, 0.3564, 0.3415]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5297, 0.4703, 0.0000],\n",
            "          [0.3421, 0.3196, 0.3382]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5039, 0.4961, 0.0000],\n",
            "          [0.3372, 0.3335, 0.3293]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4990, 0.5010, 0.0000],\n",
            "          [0.3320, 0.3280, 0.3401]]]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "softmax_socres shape: torch.Size([4, 8, 3, 3])\n",
            "values: tensor([[[[-0.4421],\n",
            "          [-0.4828],\n",
            "          [-0.3752]],\n",
            "\n",
            "         [[ 0.6036],\n",
            "          [ 0.6242],\n",
            "          [ 0.8511]],\n",
            "\n",
            "         [[ 0.0412],\n",
            "          [ 0.0077],\n",
            "          [ 0.2528]],\n",
            "\n",
            "         [[ 0.7564],\n",
            "          [ 0.9165],\n",
            "          [ 1.0039]],\n",
            "\n",
            "         [[-0.1312],\n",
            "          [-0.2673],\n",
            "          [-0.2868]],\n",
            "\n",
            "         [[ 0.3612],\n",
            "          [ 0.2347],\n",
            "          [ 0.0306]],\n",
            "\n",
            "         [[-0.8842],\n",
            "          [-0.4901],\n",
            "          [-0.9032]],\n",
            "\n",
            "         [[-0.2133],\n",
            "          [-0.2283],\n",
            "          [-0.3304]]],\n",
            "\n",
            "\n",
            "        [[[-0.3210],\n",
            "          [-0.2037],\n",
            "          [-0.3132]],\n",
            "\n",
            "         [[ 0.0236],\n",
            "          [ 0.7791],\n",
            "          [ 0.3552]],\n",
            "\n",
            "         [[-0.2618],\n",
            "          [ 0.0232],\n",
            "          [-0.1934]],\n",
            "\n",
            "         [[ 0.6312],\n",
            "          [ 0.9531],\n",
            "          [ 0.8175]],\n",
            "\n",
            "         [[ 0.4339],\n",
            "          [-0.1309],\n",
            "          [ 0.1612]],\n",
            "\n",
            "         [[-0.1243],\n",
            "          [ 0.1641],\n",
            "          [-0.1817]],\n",
            "\n",
            "         [[-0.5422],\n",
            "          [-0.6978],\n",
            "          [-0.3557]],\n",
            "\n",
            "         [[-0.4952],\n",
            "          [-0.3138],\n",
            "          [-0.3588]]],\n",
            "\n",
            "\n",
            "        [[[-0.3404],\n",
            "          [-0.2503],\n",
            "          [-0.1635]],\n",
            "\n",
            "         [[-0.1434],\n",
            "          [ 0.5357],\n",
            "          [ 0.5229]],\n",
            "\n",
            "         [[-0.5624],\n",
            "          [-0.2688],\n",
            "          [-0.2555]],\n",
            "\n",
            "         [[ 0.8203],\n",
            "          [ 0.6791],\n",
            "          [ 0.6522]],\n",
            "\n",
            "         [[ 0.3699],\n",
            "          [ 0.1479],\n",
            "          [ 0.2772]],\n",
            "\n",
            "         [[ 0.3809],\n",
            "          [ 0.3575],\n",
            "          [ 0.0997]],\n",
            "\n",
            "         [[-0.5416],\n",
            "          [-0.6572],\n",
            "          [-0.3753]],\n",
            "\n",
            "         [[-0.3194],\n",
            "          [-0.0777],\n",
            "          [-0.1731]]],\n",
            "\n",
            "\n",
            "        [[[-0.1762],\n",
            "          [-0.0455],\n",
            "          [-0.2318]],\n",
            "\n",
            "         [[ 0.4854],\n",
            "          [ 0.5005],\n",
            "          [ 0.7248]],\n",
            "\n",
            "         [[-0.3959],\n",
            "          [-0.3573],\n",
            "          [-0.1602]],\n",
            "\n",
            "         [[ 0.8613],\n",
            "          [ 1.0814],\n",
            "          [ 0.7300]],\n",
            "\n",
            "         [[ 0.2179],\n",
            "          [ 0.1167],\n",
            "          [ 0.1899]],\n",
            "\n",
            "         [[ 0.5898],\n",
            "          [ 0.1046],\n",
            "          [ 0.2981]],\n",
            "\n",
            "         [[-0.6992],\n",
            "          [-0.5217],\n",
            "          [-0.4574]],\n",
            "\n",
            "         [[-0.0751],\n",
            "          [-0.3024],\n",
            "          [-0.1810]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "v_out shape torch.Size([4, 8, 3, 1])\n",
            "v_out: tensor([[[[-0.4421],\n",
            "          [-0.4627],\n",
            "          [-0.4624]],\n",
            "\n",
            "         [[ 0.6036],\n",
            "          [ 0.6129],\n",
            "          [ 0.6127]],\n",
            "\n",
            "         [[ 0.0412],\n",
            "          [ 0.0243],\n",
            "          [ 0.0242]],\n",
            "\n",
            "         [[ 0.7564],\n",
            "          [ 0.8353],\n",
            "          [ 0.8392]],\n",
            "\n",
            "         [[-0.1312],\n",
            "          [-0.2036],\n",
            "          [-0.2027]],\n",
            "\n",
            "         [[ 0.3612],\n",
            "          [ 0.2993],\n",
            "          [ 0.3009]],\n",
            "\n",
            "         [[-0.8842],\n",
            "          [-0.6864],\n",
            "          [-0.6870]],\n",
            "\n",
            "         [[-0.2133],\n",
            "          [-0.2208],\n",
            "          [-0.2208]]],\n",
            "\n",
            "\n",
            "        [[[-0.3210],\n",
            "          [-0.2626],\n",
            "          [-0.2621]],\n",
            "\n",
            "         [[ 0.0236],\n",
            "          [ 0.3651],\n",
            "          [ 0.3870]],\n",
            "\n",
            "         [[-0.2618],\n",
            "          [-0.1045],\n",
            "          [-0.1084]],\n",
            "\n",
            "         [[ 0.6312],\n",
            "          [ 0.7927],\n",
            "          [ 0.7912]],\n",
            "\n",
            "         [[ 0.4339],\n",
            "          [ 0.1634],\n",
            "          [ 0.1761]],\n",
            "\n",
            "         [[-0.1243],\n",
            "          [ 0.0326],\n",
            "          [ 0.0319]],\n",
            "\n",
            "         [[-0.5422],\n",
            "          [-0.6141],\n",
            "          [-0.6156]],\n",
            "\n",
            "         [[-0.4952],\n",
            "          [-0.4053],\n",
            "          [-0.4049]]],\n",
            "\n",
            "\n",
            "        [[[-0.3404],\n",
            "          [-0.3404],\n",
            "          [-0.3404]],\n",
            "\n",
            "         [[-0.1434],\n",
            "          [-0.1434],\n",
            "          [-0.1434]],\n",
            "\n",
            "         [[-0.5624],\n",
            "          [-0.5624],\n",
            "          [-0.5624]],\n",
            "\n",
            "         [[ 0.8203],\n",
            "          [ 0.8203],\n",
            "          [ 0.8203]],\n",
            "\n",
            "         [[ 0.3699],\n",
            "          [ 0.3699],\n",
            "          [ 0.3699]],\n",
            "\n",
            "         [[ 0.3809],\n",
            "          [ 0.3809],\n",
            "          [ 0.3809]],\n",
            "\n",
            "         [[-0.5416],\n",
            "          [-0.5416],\n",
            "          [-0.5416]],\n",
            "\n",
            "         [[-0.3194],\n",
            "          [-0.3194],\n",
            "          [-0.3194]]],\n",
            "\n",
            "\n",
            "        [[[-0.1762],\n",
            "          [-0.1114],\n",
            "          [-0.1510]],\n",
            "\n",
            "         [[ 0.4854],\n",
            "          [ 0.4923],\n",
            "          [ 0.5596]],\n",
            "\n",
            "         [[-0.3959],\n",
            "          [-0.3753],\n",
            "          [-0.3075]],\n",
            "\n",
            "         [[ 0.8613],\n",
            "          [ 0.9664],\n",
            "          [ 0.8897]],\n",
            "\n",
            "         [[ 0.2179],\n",
            "          [ 0.1612],\n",
            "          [ 0.1723]],\n",
            "\n",
            "         [[ 0.5898],\n",
            "          [ 0.3616],\n",
            "          [ 0.3361]],\n",
            "\n",
            "         [[-0.6992],\n",
            "          [-0.6111],\n",
            "          [-0.5604]],\n",
            "\n",
            "         [[-0.0751],\n",
            "          [-0.1890],\n",
            "          [-0.1857]]]], device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
            "v_out shape before: torch.Size([4, 8, 3, 1])\n",
            "v_out shape: torch.Size([4, 3, 8])\n",
            "v_out reshaped: tensor([[[-0.4421,  0.6036,  0.0412,  0.7564, -0.1312,  0.3612, -0.8842,\n",
            "          -0.2133],\n",
            "         [-0.4627,  0.6129,  0.0243,  0.8353, -0.2036,  0.2993, -0.6864,\n",
            "          -0.2208],\n",
            "         [-0.4624,  0.6127,  0.0242,  0.8392, -0.2027,  0.3009, -0.6870,\n",
            "          -0.2208]],\n",
            "\n",
            "        [[-0.3210,  0.0236, -0.2618,  0.6312,  0.4339, -0.1243, -0.5422,\n",
            "          -0.4952],\n",
            "         [-0.2626,  0.3651, -0.1045,  0.7927,  0.1634,  0.0326, -0.6141,\n",
            "          -0.4053],\n",
            "         [-0.2621,  0.3870, -0.1084,  0.7912,  0.1761,  0.0319, -0.6156,\n",
            "          -0.4049]],\n",
            "\n",
            "        [[-0.3404, -0.1434, -0.5624,  0.8203,  0.3699,  0.3809, -0.5416,\n",
            "          -0.3194],\n",
            "         [-0.3404, -0.1434, -0.5624,  0.8203,  0.3699,  0.3809, -0.5416,\n",
            "          -0.3194],\n",
            "         [-0.3404, -0.1434, -0.5624,  0.8203,  0.3699,  0.3809, -0.5416,\n",
            "          -0.3194]],\n",
            "\n",
            "        [[-0.1762,  0.4854, -0.3959,  0.8613,  0.2179,  0.5898, -0.6992,\n",
            "          -0.0751],\n",
            "         [-0.1114,  0.4923, -0.3753,  0.9664,  0.1612,  0.3616, -0.6111,\n",
            "          -0.1890],\n",
            "         [-0.1510,  0.5596, -0.3075,  0.8897,  0.1723,  0.3361, -0.5604,\n",
            "          -0.1857]]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
            "output shape torch.Size([4, 3, 8])\n",
            "output: tensor([[[-0.0486,  0.1622,  0.1717, -0.1525,  0.9936,  0.1495,  0.1620,\n",
            "          -0.2677],\n",
            "         [ 0.0280,  0.0798,  0.1360, -0.1548,  0.9620,  0.1101,  0.1115,\n",
            "          -0.2429],\n",
            "         [ 0.0288,  0.0791,  0.1343, -0.1543,  0.9636,  0.1094,  0.1115,\n",
            "          -0.2438]],\n",
            "\n",
            "        [[-0.2540,  0.2457,  0.0244,  0.0699,  0.5358, -0.0759,  0.1166,\n",
            "           0.1651],\n",
            "         [-0.0986,  0.1277,  0.0235, -0.0235,  0.7655, -0.0018,  0.1589,\n",
            "           0.0350],\n",
            "         [-0.1027,  0.1339,  0.0267, -0.0218,  0.7709, -0.0025,  0.1684,\n",
            "           0.0455]],\n",
            "\n",
            "        [[-0.1814,  0.1470, -0.0231,  0.0748,  0.7125, -0.1557,  0.1305,\n",
            "          -0.0450],\n",
            "         [-0.1814,  0.1470, -0.0231,  0.0748,  0.7125, -0.1557,  0.1305,\n",
            "          -0.0450],\n",
            "         [-0.1814,  0.1470, -0.0231,  0.0748,  0.7125, -0.1557,  0.1305,\n",
            "          -0.0450]],\n",
            "\n",
            "        [[-0.0696,  0.1057,  0.0739,  0.0048,  1.0051, -0.0301,  0.3081,\n",
            "          -0.1054],\n",
            "         [-0.0041,  0.0340,  0.0182,  0.0061,  0.9603, -0.0809,  0.2803,\n",
            "          -0.0106],\n",
            "         [-0.0248,  0.0588,  0.0418, -0.0035,  0.9326, -0.0448,  0.2755,\n",
            "           0.0097]]], device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, feature_length):\n",
        "        super().__init__()\n",
        "        self.scale = nn.Parameter(torch.ones(feature_length))\n",
        "        self.offset = nn.Parameter(torch.zeros(feature_length))\n",
        "\n",
        "    def forward(self, activations):\n",
        "        mean = torch.mean(activations, -1, keepdim=True)\n",
        "        #print(\"mean:\", mean)\n",
        "        #print(\"activations - mean\", activations - mean)\n",
        "        variance = torch.var(activations, -1, keepdim=True, unbiased=False)\n",
        "        normalized_activations = (activations - mean) / torch.sqrt(variance + 1e-6)\n",
        "        return (normalized_activations * self.scale) + self.offset"
      ],
      "metadata": {
        "id": "-Si4-i6PTlFu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_layer_norm():\n",
        "    feature_length = 4\n",
        "    length_x = 3\n",
        "    batch_size = 5\n",
        "    layer_norm = LayerNorm(feature_length)\n",
        "\n",
        "    activations = torch.rand(batch_size, length_x, feature_length)\n",
        "\n",
        "    print(\"activations:\", activations)\n",
        "    print(\"layer_normed:\", layer_norm(activations))\n",
        "    assert layer_norm(activations).shape == activations.shape\n",
        "\n",
        "test_layer_norm()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyzgWHqrWXL4",
        "outputId": "5aa7af91-d7ab-43c8-f6bb-890c13cd1cee"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "activations: tensor([[[0.9585, 0.8650, 0.0683, 0.0543],\n",
            "         [0.5648, 0.4272, 0.2048, 0.5931],\n",
            "         [0.2845, 0.4615, 0.7539, 0.5990]],\n",
            "\n",
            "        [[0.1542, 0.8416, 0.1384, 0.8494],\n",
            "         [0.0991, 0.7173, 0.6886, 0.2655],\n",
            "         [0.8548, 0.9399, 0.1831, 0.6274]],\n",
            "\n",
            "        [[0.1678, 0.5979, 0.4609, 0.9057],\n",
            "         [0.7310, 0.5347, 0.3886, 0.9706],\n",
            "         [0.3451, 0.1739, 0.4810, 0.4873]],\n",
            "\n",
            "        [[0.2569, 0.6881, 0.7672, 0.9335],\n",
            "         [0.6350, 0.1567, 0.7177, 0.5660],\n",
            "         [0.8676, 0.5293, 0.5051, 0.2700]],\n",
            "\n",
            "        [[0.7937, 0.1208, 0.8246, 0.1129],\n",
            "         [0.9325, 0.3642, 0.3760, 0.2328],\n",
            "         [0.2098, 0.6366, 0.3749, 0.1389]]])\n",
            "layer_normed: tensor([[[ 1.1065,  0.8873, -0.9804, -1.0134],\n",
            "         [ 0.7645, -0.1323, -1.5806,  0.9485],\n",
            "         [-1.3885, -0.3652,  1.3244,  0.4293]],\n",
            "\n",
            "        [[-0.9773,  0.9887, -1.0224,  1.0110],\n",
            "         [-1.2861,  1.0284,  0.9210, -0.6633],\n",
            "         [ 0.6936,  0.9833, -1.5954, -0.0815]],\n",
            "\n",
            "        [[-1.3766,  0.2443, -0.2718,  1.4041],\n",
            "         [ 0.3424, -0.5563, -1.2254,  1.4393],\n",
            "         [-0.2093, -1.5510,  0.8555,  0.9047]],\n",
            "\n",
            "        [[-1.6195,  0.1067,  0.4235,  1.0892],\n",
            "         [ 0.5382, -1.6776,  0.9210,  0.2185],\n",
            "         [ 1.5237, -0.0642, -0.1778, -1.2817]],\n",
            "\n",
            "        [[ 0.9548, -0.9881,  1.0442, -1.0109],\n",
            "         [ 1.6939, -0.4165, -0.3728, -0.9046],\n",
            "         [-0.6807,  1.5491,  0.1822, -1.0506]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, hiddenLayerWidth, d_e, p_dropout):\n",
        "        super().__init__()\n",
        "        self.mlp1 = nn.Parameter(torch.rand(d_e, hiddenLayerWidth))\n",
        "        self.mlp2 = nn.Parameter(torch.rand(hiddenLayerWidth, d_e))\n",
        "        self.mlp1_bias = nn.Parameter(torch.zeros(hiddenLayerWidth))\n",
        "        self.mlp2_bias = nn.Parameter(torch.zeros(d_e))\n",
        "        self.dropout = nn.Dropout(p_dropout)\n",
        "\n",
        "    def forward(self, activations):\n",
        "        activations = torch.matmul(activations, self.mlp1) + self.mlp1_bias\n",
        "        activations = activations.relu()\n",
        "        activations = torch.matmul(activations, self.mlp2) + self.mlp2_bias\n",
        "        activations = self.dropout(activations)\n",
        "        return activations\n"
      ],
      "metadata": {
        "id": "X7rAEAkFoNI5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_feed_forward():\n",
        "    hiddenLayerWidth = 3\n",
        "    d_e = 4\n",
        "    feed_forward = FeedForward(hiddenLayerWidth, d_e, 0.1)\n",
        "    activations = torch.rand(10, 5, d_e)\n",
        "\n",
        "    print(\"activations:\", activations)\n",
        "    output = feed_forward(activations)\n",
        "    print(\"feed forward:\", output)\n",
        "    assert output.shape == activations.shape\n",
        "\n",
        "test_feed_forward()"
      ],
      "metadata": {
        "id": "mLQj6GjmUFN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e5dfa41-1ae8-4853-fbe2-d5f66f6563f9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "activations: tensor([[[0.7162, 0.8391, 0.7896, 0.7189],\n",
            "         [0.4911, 0.8424, 0.8687, 0.0647],\n",
            "         [0.5847, 0.6227, 0.4764, 0.1564],\n",
            "         [0.2567, 0.7449, 0.0281, 0.6179],\n",
            "         [0.3166, 0.7580, 0.0795, 0.9892]],\n",
            "\n",
            "        [[0.3658, 0.3713, 0.9095, 0.2346],\n",
            "         [0.4220, 0.1864, 0.4479, 0.7047],\n",
            "         [0.4035, 0.5706, 0.9836, 0.7179],\n",
            "         [0.6039, 0.7702, 0.2363, 0.8699],\n",
            "         [0.3465, 0.5148, 0.4237, 0.1254]],\n",
            "\n",
            "        [[0.5822, 0.1521, 0.6643, 0.8174],\n",
            "         [0.0329, 0.2912, 0.9359, 0.8245],\n",
            "         [0.2528, 0.2244, 0.9205, 0.2973],\n",
            "         [0.2273, 0.3442, 0.5416, 0.2200],\n",
            "         [0.6495, 0.2182, 0.4325, 0.9734]],\n",
            "\n",
            "        [[0.3090, 0.1305, 0.4369, 0.2272],\n",
            "         [0.3768, 0.0799, 0.2608, 0.3879],\n",
            "         [0.5184, 0.3851, 0.2945, 0.7441],\n",
            "         [0.4496, 0.0372, 0.6992, 0.9133],\n",
            "         [0.5403, 0.3095, 0.7622, 0.7765]],\n",
            "\n",
            "        [[0.7340, 0.5315, 0.3123, 0.1216],\n",
            "         [0.5905, 0.7827, 0.5246, 0.2818],\n",
            "         [0.3251, 0.9486, 0.7465, 0.8361],\n",
            "         [0.2152, 0.9253, 0.1532, 0.9265],\n",
            "         [0.2236, 0.7171, 0.8762, 0.0173]],\n",
            "\n",
            "        [[0.8524, 0.3550, 0.3433, 0.9243],\n",
            "         [0.8896, 0.5362, 0.7230, 0.5878],\n",
            "         [0.9095, 0.1898, 0.3825, 0.5456],\n",
            "         [0.3005, 0.6267, 0.9540, 0.6107],\n",
            "         [0.5523, 0.0066, 0.2676, 0.8121]],\n",
            "\n",
            "        [[0.9950, 0.5809, 0.3101, 0.2751],\n",
            "         [0.7424, 0.8780, 0.5843, 0.2509],\n",
            "         [0.9813, 0.0220, 0.9977, 0.5921],\n",
            "         [0.8650, 0.0410, 0.4478, 0.3561],\n",
            "         [0.9486, 0.0837, 0.5711, 0.3485]],\n",
            "\n",
            "        [[0.4403, 0.4605, 0.8871, 0.5350],\n",
            "         [0.1355, 0.8206, 0.3544, 0.7126],\n",
            "         [0.6834, 0.8336, 0.2763, 0.3298],\n",
            "         [0.4444, 0.8771, 0.9834, 0.1246],\n",
            "         [0.5706, 0.4069, 0.6391, 0.0773]],\n",
            "\n",
            "        [[0.1389, 0.9018, 0.4024, 0.0963],\n",
            "         [0.7001, 0.1936, 0.8411, 0.3009],\n",
            "         [0.3786, 0.3271, 0.6998, 0.0040],\n",
            "         [0.6517, 0.6713, 0.5993, 0.2971],\n",
            "         [0.9258, 0.1954, 0.3357, 0.1849]],\n",
            "\n",
            "        [[0.7486, 0.9722, 0.1511, 0.8022],\n",
            "         [0.8970, 0.7180, 0.6911, 0.3254],\n",
            "         [0.2980, 0.8007, 0.2312, 0.7990],\n",
            "         [0.9744, 0.0737, 0.7119, 0.0933],\n",
            "         [0.4882, 0.1985, 0.4847, 0.8580]]])\n",
            "feed forward: tensor([[[3.4194, 2.5743, 2.5988, 1.7923],\n",
            "         [2.8555, 2.1619, 2.1881, 1.4603],\n",
            "         [2.3472, 1.7681, 1.7770, 1.2803],\n",
            "         [1.5843, 1.1892, 1.1950, 0.8654],\n",
            "         [1.9157, 1.4348, 0.0000, 1.0266]],\n",
            "\n",
            "        [[2.2165, 0.0000, 1.7073, 1.0750],\n",
            "         [1.7578, 1.3173, 1.3321, 0.9077],\n",
            "         [2.7950, 2.1107, 2.1484, 1.3534],\n",
            "         [2.5625, 1.9202, 1.9279, 1.4099],\n",
            "         [1.7360, 1.3109, 1.3227, 0.9140]],\n",
            "\n",
            "        [[2.2954, 0.0000, 1.7409, 1.1818],\n",
            "         [1.8200, 0.0000, 0.0000, 0.7562],\n",
            "         [1.8908, 1.4327, 0.0000, 0.8712],\n",
            "         [1.5100, 1.1423, 1.1613, 0.7398],\n",
            "         [2.2818, 1.7049, 0.0000, 1.2262]],\n",
            "\n",
            "        [[1.2886, 0.9703, 0.9822, 0.6590],\n",
            "         [1.2092, 0.9044, 0.9093, 0.6572],\n",
            "         [1.9898, 1.4892, 0.0000, 1.0804],\n",
            "         [2.0137, 1.5101, 1.5347, 0.9910],\n",
            "         [2.4999, 1.8793, 1.9044, 1.2652]],\n",
            "\n",
            "        [[2.2965, 1.7240, 1.7222, 1.3190],\n",
            "         [2.6480, 1.9955, 2.0083, 1.4266],\n",
            "         [2.8631, 2.1620, 2.1964, 1.4130],\n",
            "         [1.9910, 1.4968, 1.5126, 0.0000],\n",
            "         [2.2369, 1.7006, 1.7324, 1.0728]],\n",
            "\n",
            "        [[2.6664, 1.9910, 1.9946, 1.4954],\n",
            "         [3.2425, 2.4353, 2.4498, 1.7547],\n",
            "         [2.4667, 1.8425, 1.8412, 1.4126],\n",
            "         [2.6016, 1.9682, 2.0066, 1.2391],\n",
            "         [1.6125, 1.1999, 1.2047, 0.8877]],\n",
            "\n",
            "        [[2.8668, 2.1474, 2.1409, 1.6728],\n",
            "         [3.0777, 2.3184, 2.3300, 1.6790],\n",
            "         [0.0000, 2.3528, 2.3721, 1.6638],\n",
            "         [2.2176, 1.6574, 1.6567, 1.2668],\n",
            "         [2.5522, 1.9102, 1.9122, 1.4401]],\n",
            "\n",
            "        [[2.5452, 1.9212, 1.9518, 1.2559],\n",
            "         [1.8795, 1.4187, 1.4411, 0.9288],\n",
            "         [2.5977, 1.9519, 1.9540, 1.4656],\n",
            "         [2.9724, 2.2524, 0.0000, 1.4874],\n",
            "         [2.2330, 1.6838, 1.6969, 1.1886]],\n",
            "\n",
            "        [[1.7811, 1.3523, 1.3705, 0.8990],\n",
            "         [2.5415, 1.9127, 0.0000, 1.3308],\n",
            "         [1.8489, 1.3992, 1.4181, 0.9338],\n",
            "         [0.0000, 2.0488, 2.0623, 1.4631],\n",
            "         [2.2988, 0.0000, 1.7116, 1.3538]],\n",
            "\n",
            "        [[2.9177, 0.0000, 2.1877, 1.6510],\n",
            "         [3.3182, 2.4960, 2.5075, 1.8163],\n",
            "         [2.0310, 1.5267, 1.5413, 1.0644],\n",
            "         [2.6449, 1.9843, 1.9882, 1.4808],\n",
            "         [1.9925, 1.4920, 1.5081, 1.0325]]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.multi_head_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], p_dropout, verbose)\n",
        "        self.layer_norm1 = LayerNorm(d_z)\n",
        "        self.feed_forward = FeedForward(d_mlp, d_z, p_dropout)\n",
        "        self.layer_norm2 = LayerNorm(d_z)\n",
        "\n",
        "    def forward(self, z, padding_mask):\n",
        "        z = self.layer_norm1(z)\n",
        "        z = z + self.multi_head_attention(z, z, padding_mask)\n",
        "        z = self.layer_norm2(z)\n",
        "        z = z + self.feed_forward(z)\n",
        "        return z"
      ],
      "metadata": {
        "id": "ikM15oD-qghT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        for i in range(num_layers):\n",
        "            encoder_layer = EncoderLayer(num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose)\n",
        "            self.layers.append(encoder_layer)\n",
        "        self.layers = nn.ModuleList(self.layers)\n",
        "        self.final_norm = LayerNorm(d_z)\n",
        "\n",
        "    def forward(self, z, padding_mask):\n",
        "        for layer in self.layers:\n",
        "            z = layer(z, padding_mask)\n",
        "        return self.final_norm(z)"
      ],
      "metadata": {
        "id": "zKGc6Xwr46Vh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.multi_head_self_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['MASKED'], p_dropout, verbose)\n",
        "        self.layer_norm1 = LayerNorm(d_x)\n",
        "        self.multi_head_global_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], p_dropout, verbose)\n",
        "        self.layer_norm2 = LayerNorm(d_x)\n",
        "        self.feed_forward = FeedForward(d_mlp, d_x, p_dropout)\n",
        "        self.layer_norm3 = LayerNorm(d_x)\n",
        "\n",
        "    def forward(self, z, x, src_mask, tgt_mask):\n",
        "        x = self.layer_norm1(x)\n",
        "        x = x + self.multi_head_self_attention(x, x, tgt_mask)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = x + self.multi_head_global_attention(z, x, src_mask)\n",
        "        x = self.layer_norm3(x)\n",
        "        x = x + self.feed_forward(x)\n",
        "        return x\n",
        "\n",
        "    def disable_subsequent_mask(self):\n",
        "        self.multi_head_self_attention.disable_subsequent_mask()"
      ],
      "metadata": {
        "id": "GSqElGm56xaK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        for i in range(num_layers):\n",
        "            decoder_layer = DecoderLayer(num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose)\n",
        "            self.layers.append(decoder_layer)\n",
        "        self.layers = nn.ModuleList(self.layers)\n",
        "        self.final_norm = LayerNorm(d_x)\n",
        "\n",
        "    def forward(self, z, x, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(z, x, src_mask, tgt_mask)\n",
        "        return self.final_norm(x)\n",
        "\n",
        "    def disable_subsequent_mask(self):\n",
        "        for layer in self.layers:\n",
        "            layer.multi_head_self_attention.disable_subsequent_mask()"
      ],
      "metadata": {
        "id": "LGX007WN8Bw6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoderTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, d_e, vocab_size, max_sequence_length, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.src_embedding = Embedding(vocab_size, d_e)\n",
        "        self.tgt_embedding = Embedding(vocab_size, d_e)\n",
        "        self.unembedding = Unembedding(vocab_size, d_e)\n",
        "        self.embedding_dropout = nn.Dropout(p_dropout)\n",
        "        self.positionalEmbedding = PositionalEmbedding(d_e, max_sequence_length)\n",
        "        self.encoder = Encoder(num_encoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose)\n",
        "        self.decoder = Decoder(num_decoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose)\n",
        "\n",
        "    def forward(self, z, x, src_mask, tgt_mask):\n",
        "        z = self.src_embedding(z) + self.positionalEmbedding(z)\n",
        "        z = self.embedding_dropout(z)\n",
        "        z = self.encoder(z, src_mask)\n",
        "        x = self.tgt_embedding(x) + self.positionalEmbedding(x)\n",
        "        x = self.decoder(z, x, src_mask, tgt_mask)\n",
        "        #print(\"x after decoder:\", x.shape)\n",
        "        x = self.unembedding(x)\n",
        "        #print(\"x after unembedding:\", x.shape)\n",
        "        return x\n",
        "\n",
        "    def disable_subsequent_mask(self):\n",
        "        self.decoder.disable_subsequent_mask()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ixXJDrPF8RU9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enRawName = \"drive/MyDrive/colab data/multi30kEnTrain.txt\"\n",
        "deRawName = \"drive/MyDrive/colab data/multi30kDeTrain.txt\"\n",
        "en30kVal = \"drive/MyDrive/colab data/multi30kEnVal.txt\"\n",
        "de30kVal = \"drive/MyDrive/colab data/multi30kDeVal.txt\"\n",
        "englishCleanName = \"data/english_tokens.pkl\"\n",
        "germanCleanName = \"data/german_tokens.pkl\"\n",
        "englishSortedName = \"data/englishSorted.pkl\"\n",
        "germanSortedName = \"data/germanSorted.pkl\"\n",
        "\n",
        "truncEn = \"drive/MyDrive/colab data/truncEn.pkl\"\n",
        "truncDe = \"drive/MyDrive/colab data/truncDe.pkl\"\n",
        "\n",
        "enTokenizerName = \"drive/MyDrive/colab data/enTokenizer.pkl\"\n",
        "deTokenizerName = \"drive/MyDrive/colab data/deTokenizer.pkl\"\n",
        "pairsName = \"drive/MyDrive/colab data/pairs.pkl\"\n",
        "folder = \"drive/MyDrive/colab data/\"\n",
        "\n",
        "enTrainingFileName = folder + \"enTraining\"\n",
        "deTrainingFileName = folder + \"deTraining\"\n",
        "enTestFileName = folder + \"enTest\"\n",
        "deTestFileName = folder + \"deTest\"\n",
        "enValFileName = folder + \"enValidation\"\n",
        "deValFileName = folder + \"deValidation\"\n",
        "\n",
        "enCombinedFileName = folder + \"enCombined\"\n",
        "deCombinedFileName = folder + \"deCombined\""
      ],
      "metadata": {
        "id": "NqAcQrmPg4y0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc6Y11QfhEAv",
        "outputId": "5d49f0e2-dcbb-436a-e499-bfd247885a8a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class SentenceDataset(Dataset):\n",
        "\n",
        "#     TOKENIZER_SUFFIX = \"_tokenizer\"\n",
        "#     BOS_TOKEN = \"[SOS]\"\n",
        "#     EOS_TOKEN = \"[EOS]\"\n",
        "#     PAD_TOKEN = \"[PAD]\"\n",
        "#     UNK_TOKEN = \"[UNK]\"\n",
        "\n",
        "#     def __init__(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, sequence_start_index, sequence_end_index):\n",
        "#         src_sequences = self.to_sequences(self.load_doc(src_filename), sequence_start_index, sequence_end_index)\n",
        "#         tgt_sequences = self.to_sequences(self.load_doc(tgt_filename), sequence_start_index, sequence_end_index)\n",
        "#         src_sequences = [self.add_special_tokens(sequence) for sequence in src_sequences]\n",
        "#         tgt_sequences = [self.add_special_tokens(sequence) for sequence in tgt_sequences]\n",
        "#         self.src_tokenizer, self.tgt_tokenizer = self.setup_tokenizers(src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, src_filename + SentenceDataset.TOKENIZER_SUFFIX, tgt_filename + SentenceDataset.TOKENIZER_SUFFIX)\n",
        "#         # src_tokenized = self.src_tokenizer.encode_batch(src_sequences)\n",
        "#         # tgt_tokenized = self.tgt_tokenizer.encode_batch(tgt_sequences)\n",
        "#         # src_tensors = [torch.IntTensor(sequence.ids) for sequence in src_tokenized]\n",
        "#         # tgt_tensor = [torch.IntTensor(sequence.ids) for sequence in tgt_tokenized]\n",
        "#         self.pairs = self.pair_sequences(src_sequences, tgt_sequences)\n",
        "#         #print(\"pairs\", self.pairs)\n",
        "\n",
        "#     # load doc into memory\n",
        "#     def load_doc(self, filename):\n",
        "#         # open the file as read only\n",
        "#         file = open(filename, mode='rt')\n",
        "#         # read all text\n",
        "#         text = file.read()\n",
        "#         # close the file\n",
        "#         file.close()\n",
        "#         return text\n",
        "\n",
        "#     def add_special_tokens(self, sequence):\n",
        "#         sequence = self.BOS_TOKEN + \" \" + sequence + \" \" + self.EOS_TOKEN\n",
        "#         return sequence\n",
        "\n",
        "#     def pair_sequences(self, src_sequences, tgt_sequences):\n",
        "#         paired_sequences = list(zip(src_sequences, tgt_sequences))\n",
        "#         sorted_pairs = sorted(paired_sequences, key=lambda x: len(x[0]))\n",
        "#         return sorted_pairs\n",
        "\n",
        "#     # split a loaded document into sequences\n",
        "#     def to_sequences(self, doc, sequence_start_index, sequence_end_index):\n",
        "#         sequences = doc.strip().split('\\n')\n",
        "#         return sequences[sequence_start_index:sequence_end_index]\n",
        "\n",
        "#     def setup_tokenizers(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, src_tokenizer_name, tgt_tokenizer_name):\n",
        "#         print(\"creating tokenizer for \" + src_filename)\n",
        "#         src_tokenizer = Tokenizer(BPE(unk_token=SentenceDataset.UNK_TOKEN))\n",
        "#         src_tokenizer.pre_tokenizer = Whitespace()\n",
        "#         # src_tokenizer.post_processor = TemplateProcessing(\n",
        "#         #     single=\"[BOS] $A [EOS]\",\n",
        "#         #     special_tokens=[(\"[BOS]\", 0), (\"[EOS]\", 1)],\n",
        "#         # )\n",
        "#         trainer = BpeTrainer(vocab_size = src_vocab_size, special_tokens=[SentenceDataset.BOS_TOKEN, SentenceDataset.EOS_TOKEN, SentenceDataset.PAD_TOKEN, SentenceDataset.UNK_TOKEN])\n",
        "#         src_tokenizer.train([src_filename], trainer=trainer)\n",
        "#         pickle.dump(src_tokenizer, open(src_tokenizer_name, \"wb\"))\n",
        "\n",
        "#         print(\"creating tokenizer for \" + tgt_filename)\n",
        "#         tgt_tokenizer = Tokenizer(BPE(unk_token=SentenceDataset.UNK_TOKEN))\n",
        "#         tgt_tokenizer.pre_tokenizer = Whitespace()\n",
        "#         trainer = BpeTrainer(vocab_size = tgt_vocab_size, special_tokens=[SentenceDataset.BOS_TOKEN, SentenceDataset.EOS_TOKEN, SentenceDataset.PAD_TOKEN, SentenceDataset.UNK_TOKEN])\n",
        "#         tgt_tokenizer.train([tgt_filename], trainer=trainer)\n",
        "#         pickle.dump(tgt_tokenizer, open(tgt_tokenizer_name, \"wb\"))\n",
        "#         return src_tokenizer, tgt_tokenizer\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.pairs)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         src_seq, tgt_seq = self.pairs[index]\n",
        "#         return src_seq, tgt_seq\n"
      ],
      "metadata": {
        "id": "PkCWRs4-khoT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SequencePairDataset(Dataset):\n",
        "    BOS_TOKEN = \"[SOS]\"\n",
        "    EOS_TOKEN = \"[EOS]\"\n",
        "    PAD_TOKEN = \"[PAD]\"\n",
        "    UNK_TOKEN = \"[UNK]\"\n",
        "    PAD_ID = 2\n",
        "\n",
        "    def __init__(self, src_text, tgt_text, start_index, end_index):\n",
        "        src_sequences = self.to_sequences(src_text, start_index, end_index)\n",
        "        #tgt_sequences = self.to_sequences(tgt_text, start_index, end_index)\n",
        "        tgt_sequences = self.to_sequences(tgt_text, start_index, end_index)\n",
        "        #src_sequences = [self.add_special_tokens(sequence) for sequence in src_sequences]\n",
        "        #tgt_sequences = [self.add_special_tokens(sequence) for sequence in tgt_sequences]\n",
        "        self.pairs = self.pair_sequences(src_sequences, tgt_sequences)\n",
        "\n",
        "    def pair_sequences(self, src_sequences, tgt_sequences):\n",
        "        paired_sequences = list(zip(src_sequences, tgt_sequences))\n",
        "        sorted_pairs = sorted(paired_sequences, key=lambda x: len(x[0]))\n",
        "        return sorted_pairs\n",
        "\n",
        "    # split a loaded document into sequences\n",
        "    def to_sequences(self, doc, sequence_start_index, sequence_end_index):\n",
        "        sequences = doc.strip().split('\\n')\n",
        "        return sequences[sequence_start_index : sequence_end_index]\n",
        "\n",
        "    def add_special_tokens(self, sequence):\n",
        "        sequence = self.BOS_TOKEN + \" \" + sequence + \" \" + self.EOS_TOKEN\n",
        "        return sequence\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        src_seq, tgt_seq = self.pairs[index]\n",
        "        return src_seq, tgt_seq"
      ],
      "metadata": {
        "id": "vVeybRtnKtJX"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainAndValidationSequenceDatasets():\n",
        "    def __init__(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, train_start_index, train_end_index, val_start_index, val_end_index):\n",
        "        src_text = self.load_doc(src_filename)\n",
        "        tgt_text = self.load_doc(tgt_filename)\n",
        "        self.train_dataset = SequencePairDataset(src_text, tgt_text, train_start_index, train_end_index)\n",
        "        self.val_dataset = SequencePairDataset(src_text, tgt_text, val_start_index, val_end_index)\n",
        "\n",
        "        # load doc into memory\n",
        "    def load_doc(self, filename):\n",
        "        # open the file as read only\n",
        "        file = open(filename, mode='rt')\n",
        "        # read all text\n",
        "        text = file.read()\n",
        "        # close the file\n",
        "        file.close()\n",
        "        return text"
      ],
      "metadata": {
        "id": "IJciEqbpJajo"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "class PadCollate:\n",
        "    TOKENIZER_SUFFIX = \"_tokenizer\"\n",
        "\n",
        "    def __init__(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size):\n",
        "        self.src_tokenizer, self.tgt_tokenizer = self.setup_tokenizers(src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, src_filename + self.TOKENIZER_SUFFIX, tgt_filename + self.TOKENIZER_SUFFIX)\n",
        "\n",
        "    def setup_tokenizers(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, src_tokenizer_name, tgt_tokenizer_name):\n",
        "        print(\"creating tokenizer for \" + src_filename)\n",
        "        src_tokenizer = Tokenizer(BPE(unk_token=SequencePairDataset.UNK_TOKEN))\n",
        "        src_tokenizer.pre_tokenizer = Whitespace()\n",
        "        # src_tokenizer.post_processor = TemplateProcessing(\n",
        "        #     single=\"[BOS] $A [EOS]\",\n",
        "        #     special_tokens=[(\"[BOS]\", 0), (\"[EOS]\", 1)],\n",
        "        # )\n",
        "        trainer = BpeTrainer(vocab_size = src_vocab_size, special_tokens=[SequencePairDataset.BOS_TOKEN, SequencePairDataset.EOS_TOKEN, SequencePairDataset.PAD_TOKEN, SequencePairDataset.UNK_TOKEN])\n",
        "        src_tokenizer.train([src_filename], trainer=trainer)\n",
        "        pickle.dump(src_tokenizer, open(src_tokenizer_name, \"wb\"))\n",
        "\n",
        "        print(\"creating tokenizer for \" + tgt_filename)\n",
        "        tgt_tokenizer = Tokenizer(BPE(unk_token=SequencePairDataset.UNK_TOKEN))\n",
        "        tgt_tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = BpeTrainer(vocab_size = tgt_vocab_size, special_tokens=[SequencePairDataset.BOS_TOKEN, SequencePairDataset.EOS_TOKEN, SequencePairDataset.PAD_TOKEN, SequencePairDataset.UNK_TOKEN])\n",
        "        tgt_tokenizer.train([tgt_filename], trainer=trainer)\n",
        "        tgt_tokenizer.post_processor = TemplateProcessing(\n",
        "            single=\"[BOS] $A [EOS]\",\n",
        "            special_tokens=[(\"[BOS]\", 0), (\"[EOS]\", 1)],\n",
        "        )\n",
        "        pickle.dump(tgt_tokenizer, open(tgt_tokenizer_name, \"wb\"))\n",
        "        return src_tokenizer, tgt_tokenizer\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # max_len_src = max([len(pair[0].split()) for pair in batch])\n",
        "        # max_len_tgt = max([len(pair[1].split()) for pair in batch])\n",
        "\n",
        "        #tgt_sequence_lengths\n",
        "\n",
        "        self.src_tokenizer.no_padding()\n",
        "        self.tgt_tokenizer.no_padding()\n",
        "\n",
        "        self.src_tokenizer.no_truncation()\n",
        "        self.tgt_tokenizer.no_truncation()\n",
        "\n",
        "        src_tokenized = self.src_tokenizer.encode_batch([pair[0] for pair in batch])\n",
        "        tgt_tokenized = self.tgt_tokenizer.encode_batch([pair[1] for pair in batch])\n",
        "\n",
        "        max_len_src = max([len(sequence) for sequence in src_tokenized])\n",
        "        max_len_tgt = max([len(sequence) for sequence in tgt_tokenized])\n",
        "\n",
        "        # print(\"max len src:\", max_len_src)\n",
        "        # print(\"max len tgt:\", max_len_tgt)\n",
        "\n",
        "        self.src_tokenizer.enable_padding(pad_id = SequencePairDataset.PAD_ID, pad_token = SequencePairDataset.PAD_TOKEN)\n",
        "        self.src_tokenizer.enable_truncation(max_length=max_len_src)\n",
        "        self.tgt_tokenizer.enable_padding(pad_id = SequencePairDataset.PAD_ID, pad_token = SequencePairDataset.PAD_TOKEN)\n",
        "        self.tgt_tokenizer.enable_truncation(max_length=max_len_tgt)\n",
        "\n",
        "        # print(\"src batch:\", [pair[0] for pair in batch])\n",
        "        # print(\"tgt batch:\", [pair[1] for pair in batch])\n",
        "\n",
        "        src_tokenized = self.src_tokenizer.encode_batch([pair[0] for pair in batch])\n",
        "        tgt_tokenized = self.tgt_tokenizer.encode_batch([pair[1] for pair in batch])\n",
        "        # src_tokenized = [sequence.ids for sequence in src_tokenized]\n",
        "        # tgt_tokenized = [sequence.ids for sequence in tgt_tokenized]\n",
        "        # src_tensors = torch.IntTensor(src_tokenized)\n",
        "        # tgt_tensor = torch.IntTensor(tgt_tokenized)\n",
        "\n",
        "        return src_tokenized, tgt_tokenized"
      ],
      "metadata": {
        "id": "P1wzP-DDLZ1X"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_encoder_layers = 4\n",
        "num_decoder_layers = 4\n",
        "num_heads = 8\n",
        "d_attn = 32\n",
        "d_x = 256\n",
        "d_z = 256\n",
        "d_out = 256\n",
        "d_mid = 256\n",
        "d_mlp = 512\n",
        "d_e = 256\n",
        "vocab_size = 10000\n",
        "max_sequence_length = 100\n",
        "p_dropout = 0.1"
      ],
      "metadata": {
        "id": "vFPxq_hkffx3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_validation_sequence_datasets = TrainAndValidationSequenceDatasets(enRawName, deRawName, vocab_size, vocab_size, 0, 20000, 20000, 25000)\n",
        "train_dataset = train_and_validation_sequence_datasets.train_dataset\n",
        "val_dataset = train_and_validation_sequence_datasets.val_dataset"
      ],
      "metadata": {
        "id": "uDnN_IgsNIJD"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.__getitem__(0))"
      ],
      "metadata": {
        "id": "L3WwzUFdRbk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bdf3593-8273-4232-b2a0-bb5ffc9c4e85"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('A man on the sea.', 'Ein Mann auf dem Meer.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pad_collate = PadCollate(enRawName, deRawName, vocab_size, vocab_size)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128, collate_fn = pad_collate)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=128, collate_fn = pad_collate)"
      ],
      "metadata": {
        "id": "zLayKBKBLAYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff7b3b72-98a5-4c2d-e23d-478ac4a25e97"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating tokenizer for drive/MyDrive/colab data/multi30kEnTrain.txt\n",
            "creating tokenizer for drive/MyDrive/colab data/multi30kDeTrain.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for src, tgt in train_dataloader:\n",
        "    print(src[0].ids, tgt[0].ids)\n",
        "    # print(\"decoded\", sequenceDataset.src_tokenizer.decode_batch([sequence.ids for sequence in src]))\n",
        "    # print(\"tgt\", tgt)\n",
        "\n",
        "    # print(\"mask:\", src[0].attention_mask)\n",
        "    break"
      ],
      "metadata": {
        "id": "7mnqolodQCYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa8657b-8b97-4306-e07b-34baa596c41f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30, 93, 89, 94, 1602, 15, 2] [0, 30, 93, 89, 94, 1602, 15, 1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(x, tokenizer):\n",
        "    x = torch.softmax(x, -1)\n",
        "    #print(\"x softmax:\", x)\n",
        "    x = torch.argmax(x, dim=-1)\n",
        "    x = x.tolist()\n",
        "    print(\"argmax x:\", x)\n",
        "    return tokenizer.decode(x)"
      ],
      "metadata": {
        "id": "ERDbLlQVzJUx"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_decode(tokenizer):\n",
        "    x = torch.tensor([[0, 5], [10, 20]], dtype=torch.float32)\n",
        "    words = decode(x, tokenizer)\n",
        "    print(words)\n",
        "\n",
        "test_decode(pad_collate.tgt_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSetg38X8TAo",
        "outputId": "eee6f25f-a71f-4020-c6f7-a01a7d3719ae"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax x: [1, 1]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(25)\n",
        "encoder_decoder_transformer = EncoderDecoderTransformer(num_encoder_layers, num_decoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, d_e, vocab_size, max_sequence_length, 0.1, False).to(device)\n",
        "opt = optim.Adam(encoder_decoder_transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "loss_function = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "epochs = 1000\n",
        "state_dict_filename = folder + \"encoder_decoder_transformer_state_dict_\" + datetime.today().strftime('%Y-%m-%d %H')\n",
        "# Large models need this to actually train\n",
        "for p in encoder_decoder_transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "#labelSmoothing = LabelSmoothing(2000, PADDING_IDX, 0.1)\n",
        "training_step = 0\n",
        "validation_step = 0\n",
        "best_val_loss = 100\n",
        "num_fails = 0\n",
        "for i in range(epochs):\n",
        "    epoch_time_start = time.time()\n",
        "    dataloader_iter = iter(train_dataloader)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    for src_batch, tgt_batch in dataloader_iter:\n",
        "        # print(\"x:\", sequence_x)\n",
        "        # print(\"z:\", sequence_z)\n",
        "        # sequence_x, sequence_z = sequenceDataset.__getitem__(i)\n",
        "        src_tokens = torch.IntTensor([sequence.ids for sequence in src_batch]).to(device)\n",
        "        encoder_input = src_tokens\n",
        "        train_tgt_tokens = torch.IntTensor([sequence.ids for sequence in tgt_batch]).to(device)\n",
        "        decoder_input = train_tgt_tokens[:, :-1]\n",
        "        decoder_desired_output_train = train_tgt_tokens[:, 1:]\n",
        "        src_masks = torch.IntTensor([sequence.attention_mask for sequence in src_batch]).to(device)\n",
        "        tgt_masks = torch.IntTensor([sequence.attention_mask for sequence in tgt_batch])[:, :-1].to(device)\n",
        "        # print(\"src masks\", src_masks)\n",
        "        # print(\"tgt masks\", tgt_masks)\n",
        "        train_output = encoder_decoder_transformer(encoder_input, decoder_input, src_masks, tgt_masks)\n",
        "        #print(output.shape)\n",
        "        # print(\"output\", output.shape)\n",
        "        output_transpose = train_output.transpose(-1, -2) # output needs to be N, C, other dimension for torch cross entropy\n",
        "        loss = loss_function(output_transpose, decoder_desired_output_train.long())\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        train_losses.append(loss.item())\n",
        "        if (training_step % 20 == 0):\n",
        "            print(\"Completed training step\", training_step)\n",
        "        training_step += 1\n",
        "\n",
        "    for src_batch, tgt_batch in val_dataloader:\n",
        "        src_tokens = torch.IntTensor([sequence.ids for sequence in src_batch]).to(device)\n",
        "        encoder_input = src_tokens\n",
        "        val_tgt_tokens = torch.IntTensor([sequence.ids for sequence in tgt_batch]).to(device)\n",
        "        decoder_input = val_tgt_tokens[:, :-1]\n",
        "        decoder_desired_output_val = val_tgt_tokens[:, 1:]\n",
        "        src_masks = torch.IntTensor([sequence.attention_mask for sequence in src_batch]).to(device)\n",
        "        tgt_masks = torch.IntTensor([sequence.attention_mask for sequence in tgt_batch])[:, :-1].to(device)\n",
        "        val_output = encoder_decoder_transformer(encoder_input, decoder_input, src_masks, tgt_masks)\n",
        "        output_transpose = val_output.transpose(-1, -2) # output needs to be N, C, other dimension for torch cross entropy\n",
        "        loss = loss_function(output_transpose, decoder_desired_output_val.long())\n",
        "        val_losses.append(loss.item())\n",
        "        if (validation_step % 20 == 0):\n",
        "            print(\"Completed validation step\", validation_step)\n",
        "        validation_step += 1\n",
        "\n",
        "    print(\"epoch\", i, \"took\", time.time() - epoch_time_start)\n",
        "    print(\"avg training loss:\", sum(train_losses) / len(train_losses))\n",
        "    avg_val_loss = sum(val_losses)/ len(val_losses)\n",
        "    print(\"avg validation loss:\", avg_val_loss)\n",
        "    expected_train_output = pad_collate.tgt_tokenizer.decode(decoder_desired_output_train[0].tolist())\n",
        "    print(\"expected train output\", expected_train_output)\n",
        "    decoded_output = decode(train_output[0], pad_collate.tgt_tokenizer)\n",
        "    print(\"decoded train output:\", decoded_output)\n",
        "    expected_val_output = pad_collate.tgt_tokenizer.decode(decoder_desired_output_val[0].tolist())\n",
        "    print(\"expected validation output\", expected_val_output)\n",
        "    decoded_output = decode(val_output[0], pad_collate.tgt_tokenizer)\n",
        "    print(\"decoded validation output:\", decoded_output)\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(encoder_decoder_transformer.state_dict(), state_dict_filename)\n",
        "        print(\"Saved model state dict to\", state_dict_filename)\n",
        "        num_fails = 0\n",
        "    else:\n",
        "        print(\"Average validation loss did not decrease from \", best_val_loss)\n",
        "        num_fails += 1\n",
        "        print(\"Failed to decrease the average validation loss\", num_fails, \"times.\")\n",
        "        if num_fails >= 2:\n",
        "            print(\"Stopping training\")\n",
        "            break\n",
        "    print()\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCHMIluxq6Su",
        "outputId": "9b8b31bf-0fae-4d41-a9bf-f4f32f4d18c5"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed training step 0\n",
            "Completed training step 20\n",
            "Completed training step 40\n",
            "Completed training step 60\n",
            "Completed training step 80\n",
            "Completed training step 100\n",
            "Completed training step 120\n",
            "Completed training step 140\n",
            "Completed validation step 0\n",
            "Completed validation step 20\n",
            "epoch 0 took 16.32571768760681\n",
            "avg training loss: 6.361244116619134\n",
            "avg validation loss: 5.0605284690856935\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: \n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: \n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 160\n",
            "Completed training step 180\n",
            "Completed training step 200\n",
            "Completed training step 220\n",
            "Completed training step 240\n",
            "Completed training step 260\n",
            "Completed training step 280\n",
            "Completed training step 300\n",
            "Completed validation step 40\n",
            "Completed validation step 60\n",
            "epoch 1 took 15.505475759506226\n",
            "avg training loss: 4.531251205760203\n",
            "avg validation loss: 4.44070503115654\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 100, 111, 111, 111, 111, 111, 114, 111, 111, 111, 12, 12, 12, 12, 111, 114, 12, 12, 111, 12, 14, 14, 14, 14, 111, 12, 12, 114, 14, 111, 12, 14, 111, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann in einem einem einem einem einem und einem einem einem , , , , einem und , , einem , . . . . einem , , und . einem , . einem .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 100, 100, 111, 111, 111, 111, 12, 12, 114, 12, 111, 12, 12, 12, 12, 12, 12, 114, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann in in einem einem einem einem , , und , einem , , , , , , und , , , , , , , , , ,\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 320\n",
            "Completed training step 340\n",
            "Completed training step 360\n",
            "Completed training step 380\n",
            "Completed training step 400\n",
            "Completed training step 420\n",
            "Completed training step 440\n",
            "Completed training step 460\n",
            "Completed validation step 80\n",
            "Completed validation step 100\n",
            "epoch 2 took 14.857792377471924\n",
            "avg training loss: 4.1449793417742296\n",
            "avg validation loss: 4.25086140036583\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 100, 111, 100, 114, 114, 111, 12, 12, 111, 12, 12, 12, 12, 111, 111, 12, 12, 114, 111, 111, 1, 14, 12, 14, 111, 111, 12, 12, 12, 111, 12, 111, 1, 12, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann in einem in und und einem , , einem , , , , einem einem , , und einem einem . , . einem einem , , , einem , einem ,\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 100, 111, 119, 114, 111, 12, 12, 114, 114, 111, 12, 111, 12, 12, 111, 111, 12, 114, 12, 12, 12, 114, 114, 14, 111, 114, 14, 12, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann in einem mit und einem , , und und einem , einem , , einem einem , und , , , und und . einem und . ,\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 480\n",
            "Completed training step 500\n",
            "Completed training step 520\n",
            "Completed training step 540\n",
            "Completed training step 560\n",
            "Completed training step 580\n",
            "Completed training step 600\n",
            "Completed training step 620\n",
            "Completed validation step 120\n",
            "Completed validation step 140\n",
            "epoch 3 took 14.777223348617554\n",
            "avg training loss: 3.9249687118894734\n",
            "avg validation loss: 4.090122073888779\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 100, 111, 114, 265, 114, 111, 265, 114, 111, 265, 124, 265, 265, 114, 111, 114, 114, 114, 111, 111, 245, 114, 281, 14, 111, 114, 12, 124, 14, 111, 281, 281, 103, 114, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann in einem und Hemd und einem Hemd und einem Hemd Mann Hemd Hemd und einem und und und einem einem Strae und weien . einem und , Mann . einem weien weien ein und\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 100, 111, 100, 114, 111, 294, 114, 265, 114, 111, 114, 111, 265, 12, 103, 103, 124, 124, 12, 111, 124, 114, 103, 114, 111, 103, 2, 114, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann in einem in und einem roten und Hemd und einem und einem Hemd , ein ein Mann Mann , einem Mann und ein und einem ein und\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 640\n",
            "Completed training step 660\n",
            "Completed training step 680\n",
            "Completed training step 700\n",
            "Completed training step 720\n",
            "Completed training step 740\n",
            "Completed training step 760\n",
            "Completed training step 780\n",
            "Completed validation step 160\n",
            "Completed validation step 180\n",
            "epoch 4 took 14.826215505599976\n",
            "avg training loss: 3.7662283781987087\n",
            "avg validation loss: 3.973433864116669\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 114, 265, 114, 111, 265, 114, 111, 114, 124, 265, 265, 114, 103, 265, 114, 114, 121, 111, 281, 114, 245, 114, 121, 103, 12, 13, 114, 111, 245, 245, 111, 114, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem und Hemd und einem Hemd und einem und Mann Hemd Hemd und ein Hemd und und der einem weien und Strae und der ein , - und einem Strae Strae einem und\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 111, 114, 114, 111, 281, 114, 276, 114, 111, 114, 111, 124, 114, 103, 111, 124, 124, 114, 111, 245, 114, 114, 114, 111, 281, 14, 114, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit einem und und einem weien und blauen und einem und einem Mann und ein einem Mann Mann und einem Strae und und und einem weien . und\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 800\n",
            "Completed training step 820\n",
            "Completed training step 840\n",
            "Completed training step 860\n",
            "Completed training step 880\n",
            "Completed training step 900\n",
            "Completed training step 920\n",
            "Completed training step 940\n",
            "Completed validation step 200\n",
            "Completed validation step 220\n",
            "epoch 5 took 14.820434331893921\n",
            "avg training loss: 3.64971745849415\n",
            "avg validation loss: 3.88056937456131\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 114, 265, 114, 103, 265, 114, 103, 265, 124, 281, 114, 114, 103, 281, 114, 114, 103, 111, 281, 114, 245, 114, 103, 117, 114, 13, 114, 111, 281, 245, 138, 114, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem und Hemd und ein Hemd und ein Hemd Mann weien und und ein weien und und ein einem weien und Strae und ein auf und - und einem weien Strae Frau und\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 100, 111, 114, 100, 111, 294, 114, 265, 114, 111, 114, 111, 138, 103, 121, 111, 124, 124, 100, 111, 138, 114, 103, 114, 111, 281, 14, 114, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann in einem und in einem roten und Hemd und einem und einem Frau ein der einem Mann Mann in einem Frau und ein und einem weien . und\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 960\n",
            "Completed training step 980\n",
            "Completed training step 1000\n",
            "Completed training step 1020\n",
            "Completed training step 1040\n",
            "Completed training step 1060\n",
            "Completed training step 1080\n",
            "Completed validation step 240\n",
            "Completed validation step 260\n",
            "epoch 6 took 14.97376275062561\n",
            "avg training loss: 3.556563360675885\n",
            "avg validation loss: 3.8203918278217315\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 114, 265, 114, 111, 265, 114, 121, 114, 138, 265, 13, 114, 111, 281, 114, 114, 103, 111, 124, 114, 448, 14, 103, 103, 114, 13, 114, 111, 124, 13, 124, 114, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem und Hemd und einem Hemd und der und Frau Hemd - und einem weien und und ein einem Mann und Hintergrund . ein ein und - und einem Mann - Mann und\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 100, 111, 114, 100, 111, 276, 114, 124, 114, 103, 100, 111, 138, 114, 121, 103, 124, 124, 114, 111, 138, 114, 124, 114, 111, 448, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann in einem und in einem blauen und Mann und ein in einem Frau und der ein Mann Mann und einem Frau und Mann und einem Hintergrund . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 1100\n",
            "Completed training step 1120\n",
            "Completed training step 1140\n",
            "Completed training step 1160\n",
            "Completed training step 1180\n",
            "Completed training step 1200\n",
            "Completed training step 1220\n",
            "Completed training step 1240\n",
            "Completed validation step 280\n",
            "Completed validation step 300\n",
            "epoch 7 took 14.995952606201172\n",
            "avg training loss: 3.478930588740452\n",
            "avg validation loss: 3.7600525677204133\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 114, 265, 114, 111, 265, 114, 121, 265, 138, 265, 13, 114, 121, 265, 114, 12, 121, 111, 281, 114, 448, 14, 152, 103, 103, 13, 114, 111, 138, 448, 14, 114, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem und Hemd und einem Hemd und der Hemd Frau Hemd - und der Hemd und , der einem weien und Hintergrund . die ein ein - und einem Frau Hintergrund . und\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 100, 111, 114, 117, 111, 294, 12, 281, 114, 103, 117, 111, 138, 117, 250, 103, 124, 124, 117, 111, 138, 14, 103, 14, 111, 1, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann in einem und auf einem roten , weien und ein auf einem Frau auf whrend ein Mann Mann auf einem Frau . ein . einem . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 1260\n",
            "Completed training step 1280\n",
            "Completed training step 1300\n",
            "Completed training step 1320\n",
            "Completed training step 1340\n",
            "Completed training step 1360\n",
            "Completed training step 1380\n",
            "Completed training step 1400\n",
            "Completed validation step 320\n",
            "Completed validation step 340\n",
            "epoch 8 took 15.248398542404175\n",
            "avg training loss: 3.4061348043429622\n",
            "avg validation loss: 3.7280298829078675\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 114, 265, 114, 111, 265, 114, 121, 103, 138, 265, 13, 114, 103, 294, 114, 12, 152, 111, 281, 14, 448, 14, 152, 177, 107, 114, 12, 111, 138, 245, 103, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem und Hemd und einem Hemd und der ein Frau Hemd - und ein roten und , die einem weien . Hintergrund . die sich eine und , einem Frau Strae ein .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 100, 111, 114, 100, 111, 124, 114, 265, 114, 103, 100, 111, 124, 12, 250, 103, 124, 124, 100, 111, 138, 14, 124, 14, 111, 1, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann in einem und in einem Mann und Hemd und ein in einem Mann , whrend ein Mann Mann in einem Frau . Mann . einem . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 1420\n",
            "Completed training step 1440\n",
            "Completed training step 1460\n",
            "Completed training step 1480\n",
            "Completed training step 1500\n",
            "Completed training step 1520\n",
            "Completed training step 1540\n",
            "Completed training step 1560\n",
            "Completed validation step 360\n",
            "Completed validation step 380\n",
            "epoch 9 took 15.765737533569336\n",
            "avg training loss: 3.339046680243911\n",
            "avg validation loss: 3.6618165910243987\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 114, 265, 114, 111, 265, 114, 121, 103, 138, 265, 13, 114, 103, 265, 114, 12, 152, 111, 294, 119, 547, 114, 119, 103, 103, 138, 114, 111, 138, 245, 14, 119, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem und Hemd und einem Hemd und der ein Frau Hemd - und ein Hemd und , die einem roten mit Boden und mit ein ein Frau und einem Frau Strae . mit\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 100, 111, 114, 100, 111, 124, 114, 138, 114, 244, 103, 111, 138, 114, 250, 103, 124, 124, 100, 111, 138, 14, 103, 14, 111, 1, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann in einem und in einem Mann und Frau und hlt ein einem Frau und whrend ein Mann Mann in einem Frau . ein . einem . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 1580\n",
            "Completed training step 1600\n",
            "Completed training step 1620\n",
            "Completed training step 1640\n",
            "Completed training step 1660\n",
            "Completed training step 1680\n",
            "Completed training step 1700\n",
            "Completed training step 1720\n",
            "Completed validation step 400\n",
            "Completed validation step 420\n",
            "epoch 10 took 14.87092661857605\n",
            "avg training loss: 3.276856348013422\n",
            "avg validation loss: 3.6176025390625\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 114, 265, 114, 111, 111, 114, 121, 103, 138, 138, 13, 114, 111, 138, 12, 12, 119, 111, 281, 119, 448, 119, 152, 103, 119, 138, 119, 111, 138, 547, 14, 119, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem und Hemd und einem einem und der ein Frau Frau - und einem Frau , , mit einem weien mit Hintergrund mit die ein mit Frau mit einem Frau Boden . mit\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 100, 111, 114, 117, 111, 124, 13, 124, 114, 125, 117, 111, 138, 117, 250, 103, 124, 124, 12, 111, 138, 14, 487, 14, 14, 1, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann in einem und auf einem Mann - Mann und einer auf einem Frau auf whrend ein Mann Mann , einem Frau . andere . . . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 1740\n",
            "Completed training step 1760\n",
            "Completed training step 1780\n",
            "Completed training step 1800\n",
            "Completed training step 1820\n",
            "Completed training step 1840\n",
            "Completed training step 1860\n",
            "Completed training step 1880\n",
            "Completed validation step 440\n",
            "Completed validation step 460\n",
            "epoch 11 took 14.849900960922241\n",
            "avg training loss: 3.220224441236751\n",
            "avg validation loss: 3.582765406370163\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 114, 348, 114, 111, 265, 114, 121, 117, 138, 138, 13, 114, 103, 138, 114, 12, 152, 111, 281, 119, 440, 114, 250, 103, 119, 138, 12, 111, 138, 245, 14, 114, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem und schwarzen und einem Hemd und der auf Frau Frau - und ein Frau und , die einem weien mit Strand und whrend ein mit Frau , einem Frau Strae . und\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 100, 111, 114, 176, 111, 124, 13, 124, 114, 103, 176, 111, 138, 12, 250, 103, 124, 124, 100, 111, 138, 13, 103, 14, 111, 474, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann in einem und vor einem Mann - Mann und ein vor einem Frau , whrend ein Mann Mann in einem Frau - ein . einem Tisch . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 1900\n",
            "Completed training step 1920\n",
            "Completed training step 1940\n",
            "Completed training step 1960\n",
            "Completed training step 1980\n",
            "Completed training step 2000\n",
            "Completed training step 2020\n",
            "Completed training step 2040\n",
            "Completed validation step 480\n",
            "Completed validation step 500\n",
            "epoch 12 took 14.988765954971313\n",
            "avg training loss: 3.1689078792644914\n",
            "avg validation loss: 3.565178805589676\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 114, 492, 114, 111, 265, 114, 121, 119, 138, 138, 13, 114, 107, 265, 12, 12, 119, 111, 294, 117, 440, 119, 119, 103, 107, 138, 119, 111, 138, 13, 138, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem und Hut und einem Hemd und der mit Frau Frau - und eine Hemd , , mit einem roten auf Strand mit mit ein eine Frau mit einem Frau - Frau .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 100, 111, 114, 176, 111, 124, 13, 124, 114, 103, 106, 111, 138, 12, 250, 103, 124, 124, 100, 111, 138, 14, 103, 14, 14, 124, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann in einem und vor einem Mann - Mann und ein an einem Frau , whrend ein Mann Mann in einem Frau . ein . . Mann . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 2060\n",
            "Completed training step 2080\n",
            "Completed training step 2100\n",
            "Completed training step 2120\n",
            "Completed training step 2140\n",
            "Completed training step 2160\n",
            "Completed training step 2180\n",
            "Completed validation step 520\n",
            "Completed validation step 540\n",
            "epoch 13 took 14.865275382995605\n",
            "avg training loss: 3.1214705075427984\n",
            "avg validation loss: 3.530901676416397\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 776, 492, 114, 111, 265, 114, 121, 177, 138, 741, 13, 114, 103, 265, 114, 12, 152, 111, 281, 117, 440, 119, 121, 103, 107, 138, 117, 111, 138, 13, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem Haaren Hut und einem Hemd und der sich Frau Hose - und ein Hemd und , die einem weien auf Strand mit der ein eine Frau auf einem Frau - . .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 100, 111, 114, 176, 111, 124, 13, 124, 13, 125, 106, 111, 138, 176, 250, 103, 124, 124, 100, 111, 138, 13, 103, 14, 111, 474, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann in einem und vor einem Mann - Mann - einer an einem Frau vor whrend ein Mann Mann in einem Frau - ein . einem Tisch . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 2200\n",
            "Completed training step 2220\n",
            "Completed training step 2240\n",
            "Completed training step 2260\n",
            "Completed training step 2280\n",
            "Completed training step 2300\n",
            "Completed training step 2320\n",
            "Completed training step 2340\n",
            "Completed validation step 560\n",
            "Completed validation step 580\n",
            "epoch 14 took 14.923197507858276\n",
            "avg training loss: 3.0782182216644287\n",
            "avg validation loss: 3.4978928923606873\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 1058, 492, 114, 111, 1002, 114, 121, 107, 138, 741, 13, 114, 103, 265, 12, 12, 152, 111, 281, 117, 440, 12, 152, 103, 119, 138, 119, 111, 281, 440, 153, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem Mtze Hut und einem Oberkrper und der eine Frau Hose - und ein Hemd , , die einem weien auf Strand , die ein mit Frau mit einem weien Strand zu .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 100, 111, 230, 176, 111, 124, 114, 124, 114, 125, 106, 111, 138, 103, 250, 107, 124, 124, 100, 111, 138, 100, 103, 14, 111, 474, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann in einem sitzt vor einem Mann und Mann und einer an einem Frau ein whrend eine Mann Mann in einem Frau in ein . einem Tisch . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 2360\n",
            "Completed training step 2380\n",
            "Completed training step 2400\n",
            "Completed training step 2420\n",
            "Completed training step 2440\n",
            "Completed training step 2460\n",
            "Completed training step 2480\n",
            "Completed training step 2500\n",
            "Completed validation step 600\n",
            "Completed validation step 620\n",
            "epoch 15 took 14.96431040763855\n",
            "avg training loss: 3.036240984679787\n",
            "avg validation loss: 3.47667510509491\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 114, 492, 114, 111, 1002, 12, 121, 103, 138, 741, 13, 114, 103, 454, 12, 12, 152, 111, 294, 119, 440, 119, 119, 103, 107, 119, 119, 111, 245, 314, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem und Hut und einem Oberkrper , der ein Frau Hose - und ein Jacke , , die einem roten mit Strand mit mit ein eine mit mit einem Strae Hand . .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 111, 114, 176, 111, 474, 114, 906, 114, 125, 176, 111, 534, 12, 250, 103, 124, 124, 114, 111, 138, 114, 487, 14, 14, 474, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit einem und vor einem Tisch und Schild und einer vor einem Menschenmenge , whrend ein Mann Mann und einem Frau und andere . . Tisch . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 2520\n",
            "Completed training step 2540\n",
            "Completed training step 2560\n",
            "Completed training step 2580\n",
            "Completed training step 2600\n",
            "Completed training step 2620\n",
            "Completed training step 2640\n",
            "Completed training step 2660\n",
            "Completed validation step 640\n",
            "Completed validation step 660\n",
            "epoch 16 took 15.063194513320923\n",
            "avg training loss: 2.996865814658487\n",
            "avg validation loss: 3.4493321120738982\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 114, 776, 114, 111, 1002, 114, 121, 107, 138, 138, 13, 114, 103, 773, 12, 12, 117, 111, 281, 12, 440, 114, 121, 107, 107, 138, 119, 111, 138, 813, 138, 12, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem und Haaren und einem Oberkrper und der eine Frau Frau - und ein Hosen , , auf einem weien , Strand und der eine eine Frau mit einem Frau Tag Frau ,\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 111, 114, 176, 111, 124, 114, 906, 114, 103, 100, 111, 138, 103, 250, 103, 124, 124, 100, 111, 138, 114, 103, 14, 14, 474, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit einem und vor einem Mann und Schild und ein in einem Frau ein whrend ein Mann Mann in einem Frau und ein . . Tisch . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 2680\n",
            "Completed training step 2700\n",
            "Completed training step 2720\n",
            "Completed training step 2740\n",
            "Completed training step 2760\n",
            "Completed training step 2780\n",
            "Completed training step 2800\n",
            "Completed training step 2820\n",
            "Completed validation step 680\n",
            "Completed validation step 700\n",
            "epoch 17 took 15.646933794021606\n",
            "avg training loss: 2.956051979854608\n",
            "avg validation loss: 3.4362169921398165\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 125, 294, 114, 111, 1002, 303, 121, 107, 138, 138, 133, 303, 125, 454, 303, 12, 117, 111, 281, 117, 440, 119, 119, 103, 107, 138, 119, 111, 138, 440, 153, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem einer roten und einem Oberkrper geht der eine Frau Frau ten geht einer Jacke geht , auf einem weien auf Strand mit mit ein eine Frau mit einem Frau Strand zu .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 111, 114, 176, 111, 124, 114, 906, 114, 125, 100, 111, 138, 100, 250, 103, 124, 124, 100, 111, 138, 220, 103, 14, 728, 474, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit einem und vor einem Mann und Schild und einer in einem Frau in whrend ein Mann Mann in einem Frau steht ein . ihm Tisch . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 2840\n",
            "Completed training step 2860\n",
            "Completed training step 2880\n",
            "Completed training step 2900\n",
            "Completed training step 2920\n",
            "Completed training step 2940\n",
            "Completed training step 2960\n",
            "Completed training step 2980\n",
            "Completed validation step 720\n",
            "Completed validation step 740\n",
            "epoch 18 took 15.357452630996704\n",
            "avg training loss: 2.9163832102611567\n",
            "avg validation loss: 3.426403743028641\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 776, 492, 114, 111, 773, 12, 121, 119, 138, 741, 133, 114, 103, 741, 12, 12, 117, 111, 281, 117, 440, 119, 121, 103, 103, 119, 119, 111, 281, 440, 13, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem Haaren Hut und einem Hosen , der mit Frau Hose ten und ein Hose , , auf einem weien auf Strand mit der ein ein mit mit einem weien Strand - .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 111, 220, 176, 111, 474, 13, 906, 100, 244, 176, 111, 138, 100, 250, 103, 124, 124, 100, 111, 138, 176, 103, 100, 14, 474, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit einem steht vor einem Tisch - Schild in hlt vor einem Frau in whrend ein Mann Mann in einem Frau vor ein in . Tisch . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 3000\n",
            "Completed training step 3020\n",
            "Completed training step 3040\n",
            "Completed training step 3060\n",
            "Completed training step 3080\n",
            "Completed training step 3100\n",
            "Completed training step 3120\n",
            "Completed validation step 760\n",
            "Completed validation step 780\n",
            "epoch 19 took 14.75353717803955\n",
            "avg training loss: 2.8832194288824775\n",
            "avg validation loss: 3.393906408548355\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 1058, 294, 114, 111, 1002, 12, 121, 107, 138, 741, 741, 114, 103, 773, 303, 12, 117, 111, 281, 117, 440, 12, 121, 103, 107, 245, 119, 111, 294, 547, 137, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem Mtze roten und einem Oberkrper , der eine Frau Hose Hose und ein Hosen geht , auf einem weien auf Strand , der ein eine Strae mit einem roten Boden te .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 111, 114, 176, 111, 474, 13, 906, 13, 125, 103, 111, 138, 114, 250, 103, 979, 124, 100, 111, 138, 220, 103, 220, 111, 474, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit einem und vor einem Tisch - Schild - einer ein einem Frau und whrend ein Buch Mann in einem Frau steht ein steht einem Tisch . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 3140\n",
            "Completed training step 3160\n",
            "Completed training step 3180\n",
            "Completed training step 3200\n",
            "Completed training step 3220\n",
            "Completed training step 3240\n",
            "Completed training step 3260\n",
            "Completed training step 3280\n",
            "Completed validation step 800\n",
            "Completed validation step 820\n",
            "epoch 20 took 15.00142240524292\n",
            "avg training loss: 2.845440049080332\n",
            "avg validation loss: 3.374560606479645\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 276, 294, 114, 111, 1002, 114, 121, 103, 138, 741, 741, 114, 111, 773, 114, 12, 117, 111, 294, 12, 440, 12, 117, 103, 103, 601, 119, 111, 294, 101, 741, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem blauen roten und einem Oberkrper und der ein Frau Hose Hose und einem Hosen und , auf einem roten , Strand , auf ein ein groe mit einem roten en Hose .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 111, 114, 176, 111, 474, 114, 906, 114, 125, 176, 125, 534, 103, 250, 103, 124, 124, 100, 111, 138, 220, 103, 14, 14, 811, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit einem und vor einem Tisch und Schild und einer vor einer Menschenmenge ein whrend ein Mann Mann in einem Frau steht ein . . Restaurant . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 3300\n",
            "Completed training step 3320\n",
            "Completed training step 3340\n",
            "Completed training step 3360\n",
            "Completed training step 3380\n",
            "Completed training step 3400\n",
            "Completed training step 3420\n",
            "Completed training step 3440\n",
            "Completed validation step 840\n",
            "Completed validation step 860\n",
            "epoch 21 took 14.942862272262573\n",
            "avg training loss: 2.8092726248844415\n",
            "avg validation loss: 3.366582679748535\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 776, 776, 114, 111, 1002, 114, 121, 177, 1377, 741, 1614, 114, 103, 773, 114, 12, 117, 111, 281, 119, 440, 114, 119, 103, 103, 601, 119, 111, 401, 1962, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem Haaren Haaren und einem Oberkrper und der sich blaue Hose Armen und ein Hosen und , auf einem weien mit Strand und mit ein ein groe mit einem groen Gegend . .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 111, 114, 176, 111, 906, 114, 906, 114, 111, 176, 111, 138, 114, 250, 103, 124, 124, 114, 111, 138, 220, 736, 14, 728, 906, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit einem und vor einem Schild und Schild und einem vor einem Frau und whrend ein Mann Mann und einem Frau steht liest . ihm Schild . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 3460\n",
            "Completed training step 3480\n",
            "Completed training step 3500\n",
            "Completed training step 3520\n",
            "Completed training step 3540\n",
            "Completed training step 3560\n",
            "Completed training step 3580\n",
            "Completed training step 3600\n",
            "Completed validation step 880\n",
            "Completed validation step 900\n",
            "epoch 22 took 14.915108680725098\n",
            "avg training loss: 2.772944544530978\n",
            "avg validation loss: 3.3593734741210937\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 776, 776, 114, 111, 1002, 12, 121, 177, 138, 741, 133, 114, 103, 773, 12, 12, 152, 111, 294, 117, 440, 119, 121, 103, 119, 119, 119, 111, 407, 547, 254, 12, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem Haaren Haaren und einem Oberkrper , der sich Frau Hose ten und ein Hosen , , die einem roten auf Strand mit der ein mit mit mit einem grnen Boden ist ,\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 632, 114, 176, 111, 906, 114, 906, 114, 244, 103, 125, 138, 103, 250, 103, 847, 124, 220, 111, 138, 220, 103, 14, 728, 906, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit Brille und vor einem Schild und Schild und hlt ein einer Frau ein whrend ein Mikrofon Mann steht einem Frau steht ein . ihm Schild . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 3620\n",
            "Completed training step 3640\n",
            "Completed training step 3660\n",
            "Completed training step 3680\n",
            "Completed training step 3700\n",
            "Completed training step 3720\n",
            "Completed training step 3740\n",
            "Completed training step 3760\n",
            "Completed validation step 920\n",
            "Completed validation step 940\n",
            "epoch 23 took 15.002849340438843\n",
            "avg training loss: 2.739788397102599\n",
            "avg validation loss: 3.3668308913707734\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 776, 776, 114, 111, 1002, 119, 121, 103, 138, 741, 741, 114, 111, 773, 319, 12, 117, 111, 294, 12, 440, 12, 121, 103, 103, 601, 119, 111, 525, 547, 75, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem Haaren Haaren und einem Oberkrper mit der ein Frau Hose Hose und einem Hosen trgt , auf einem roten , Strand , der ein ein groe mit einem kleinen Boden s .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 111, 114, 119, 111, 906, 114, 906, 114, 244, 176, 111, 138, 103, 121, 107, 124, 124, 114, 111, 138, 114, 244, 114, 728, 906, 114, 114, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit einem und mit einem Schild und Schild und hlt vor einem Frau ein der eine Mann Mann und einem Frau und hlt und ihm Schild und und\n",
            "Average validation loss did not decrease from  3.3593734741210937\n",
            "Failed to decrease the average validation loss 1 times.\n",
            "\n",
            "\n",
            "Completed training step 3780\n",
            "Completed training step 3800\n",
            "Completed training step 3820\n",
            "Completed training step 3840\n",
            "Completed training step 3860\n",
            "Completed training step 3880\n",
            "Completed training step 3900\n",
            "Completed training step 3920\n",
            "Completed validation step 960\n",
            "Completed validation step 980\n",
            "epoch 24 took 14.868979215621948\n",
            "avg training loss: 2.7064739108844926\n",
            "avg validation loss: 3.356090414524078\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 776, 776, 114, 111, 1002, 114, 121, 103, 1377, 741, 741, 114, 103, 773, 114, 12, 250, 189, 294, 117, 440, 114, 121, 102, 107, 138, 119, 111, 451, 731, 13, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem Haaren Haaren und einem Oberkrper und der ein blaue Hose Hose und ein Hosen und , whrend dem roten auf Strand und der er eine Frau mit einem Jungen Gesicht - .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 632, 114, 176, 111, 124, 114, 906, 114, 125, 103, 111, 138, 103, 250, 103, 124, 124, 114, 111, 138, 220, 103, 220, 728, 124, 745, 103, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit Brille und vor einem Mann und Schild und einer ein einem Frau ein whrend ein Mann Mann und einem Frau steht ein steht ihm Mann spricht ein\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 3940\n",
            "Completed training step 3960\n",
            "Completed training step 3980\n",
            "Completed training step 4000\n",
            "Completed training step 4020\n",
            "Completed training step 4040\n",
            "Completed training step 4060\n",
            "Completed training step 4080\n",
            "Completed validation step 1000\n",
            "Completed validation step 1020\n",
            "epoch 25 took 15.40646767616272\n",
            "avg training loss: 2.672772998263122\n",
            "avg validation loss: 3.3400970458984376\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 776, 776, 114, 111, 1002, 12, 121, 177, 1377, 741, 741, 114, 103, 773, 319, 12, 117, 111, 294, 12, 440, 119, 152, 103, 103, 601, 119, 111, 1377, 101, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem Haaren Haaren und einem Oberkrper , der sich blaue Hose Hose und ein Hosen trgt , auf einem roten , Strand mit die ein ein groe mit einem blaue en . .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 632, 114, 176, 111, 124, 114, 906, 114, 125, 103, 111, 138, 103, 250, 103, 124, 124, 114, 111, 138, 220, 103, 220, 728, 474, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit Brille und vor einem Mann und Schild und einer ein einem Frau ein whrend ein Mann Mann und einem Frau steht ein steht ihm Tisch . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 4100\n",
            "Completed training step 4120\n",
            "Completed training step 4140\n",
            "Completed training step 4160\n",
            "Completed training step 4180\n",
            "Completed training step 4200\n",
            "Completed training step 4220\n",
            "Completed validation step 1040\n",
            "Completed validation step 1060\n",
            "epoch 26 took 15.828000783920288\n",
            "avg training loss: 2.6401621034950207\n",
            "avg validation loss: 3.334441876411438\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 776, 776, 114, 111, 1002, 114, 121, 117, 1377, 741, 741, 114, 103, 773, 319, 12, 117, 111, 294, 117, 440, 12, 121, 102, 103, 601, 119, 111, 525, 440, 302, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem Haaren Haaren und einem Oberkrper und der auf blaue Hose Hose und ein Hosen trgt , auf einem roten auf Strand , der er ein groe mit einem kleinen Strand spielen .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 632, 114, 176, 111, 906, 13, 906, 114, 125, 103, 111, 138, 103, 250, 103, 847, 124, 114, 111, 138, 220, 103, 114, 728, 906, 12, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit Brille und vor einem Schild - Schild und einer ein einem Frau ein whrend ein Mikrofon Mann und einem Frau steht ein und ihm Schild , .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 4240\n",
            "Completed training step 4260\n",
            "Completed training step 4280\n",
            "Completed training step 4300\n",
            "Completed training step 4320\n",
            "Completed training step 4340\n",
            "Completed training step 4360\n",
            "Completed training step 4380\n",
            "Completed validation step 1080\n",
            "Completed validation step 1100\n",
            "epoch 27 took 14.789293050765991\n",
            "avg training loss: 2.609915425063698\n",
            "avg validation loss: 3.322485363483429\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 776, 776, 114, 111, 1002, 12, 121, 177, 138, 741, 71, 114, 111, 773, 12, 12, 117, 111, 294, 154, 440, 119, 152, 102, 144, 1377, 119, 111, 407, 547, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem Haaren Haaren und einem Oberkrper , der sich Frau Hose o und einem Hosen , , auf einem roten am Strand mit die er einen blaue mit einem grnen Boden . .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 632, 119, 176, 111, 124, 114, 906, 114, 111, 103, 111, 138, 12, 273, 103, 979, 124, 220, 111, 138, 13, 736, 12, 111, 124, 14, 103, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit Brille mit vor einem Mann und Schild und einem ein einem Frau , das ein Buch Mann steht einem Frau - liest , einem Mann . ein\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 4400\n",
            "Completed training step 4420\n",
            "Completed training step 4440\n",
            "Completed training step 4460\n",
            "Completed training step 4480\n",
            "Completed training step 4500\n",
            "Completed training step 4520\n",
            "Completed training step 4540\n",
            "Completed validation step 1120\n",
            "Completed validation step 1140\n",
            "epoch 28 took 14.761419296264648\n",
            "avg training loss: 2.5769130211726874\n",
            "avg validation loss: 3.2896863758563994\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 125, 776, 776, 114, 111, 177, 114, 121, 107, 1377, 741, 1315, 114, 103, 773, 319, 12, 117, 111, 294, 117, 440, 119, 117, 103, 107, 601, 119, 111, 1377, 777, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einer Haaren Haaren und einem sich und der eine blaue Hose Tasche und ein Hosen trgt , auf einem roten auf Strand mit auf ein eine groe mit einem blaue Wald . .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 632, 114, 176, 111, 847, 114, 906, 114, 111, 176, 111, 138, 103, 250, 103, 747, 124, 100, 111, 138, 114, 736, 220, 111, 474, 114, 103, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit Brille und vor einem Mikrofon und Schild und einem vor einem Frau ein whrend ein anderer Mann in einem Frau und liest steht einem Tisch und ein\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 4560\n",
            "Completed training step 4580\n",
            "Completed training step 4600\n",
            "Completed training step 4620\n",
            "Completed training step 4640\n",
            "Completed training step 4660\n",
            "Completed training step 4680\n",
            "Completed training step 4700\n",
            "Completed validation step 1160\n",
            "Completed validation step 1180\n",
            "epoch 29 took 14.928977251052856\n",
            "avg training loss: 2.547247062063521\n",
            "avg validation loss: 3.277233636379242\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 923, 776, 776, 114, 111, 1002, 12, 121, 107, 1377, 741, 1614, 114, 107, 773, 319, 12, 117, 111, 525, 154, 440, 119, 250, 102, 107, 1377, 119, 111, 1377, 333, 75, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit langen Haaren Haaren und einem Oberkrper , der eine blaue Hose Armen und eine Hosen trgt , auf einem kleinen am Strand mit whrend er eine blaue mit einem blaue Kleid s .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 632, 114, 176, 111, 906, 114, 906, 114, 111, 176, 111, 461, 103, 103, 103, 979, 124, 114, 111, 138, 13, 736, 220, 728, 906, 114, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit Brille und vor einem Schild und Schild und einem vor einem anderen ein ein ein Buch Mann und einem Frau - liest steht ihm Schild und .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 4720\n",
            "Completed training step 4740\n",
            "Completed training step 4760\n",
            "Completed training step 4780\n",
            "Completed training step 4800\n",
            "Completed training step 4820\n",
            "Completed training step 4840\n",
            "Completed training step 4860\n",
            "Completed validation step 1200\n",
            "Completed validation step 1220\n",
            "epoch 30 took 14.939804553985596\n",
            "avg training loss: 2.5166077796061326\n",
            "avg validation loss: 3.2612131536006927\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 923, 776, 776, 114, 111, 1002, 12, 121, 107, 209, 741, 368, 114, 103, 773, 319, 12, 117, 111, 294, 117, 440, 12, 152, 102, 107, 601, 119, 111, 117, 1003, 117, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit langen Haaren Haaren und einem Oberkrper , der eine Person Hose ige und ein Hosen trgt , auf einem roten auf Strand , die er eine groe mit einem auf Hgel auf .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 632, 114, 176, 111, 906, 13, 906, 114, 125, 103, 125, 138, 103, 250, 103, 979, 124, 114, 111, 138, 13, 103, 100, 728, 461, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit Brille und vor einem Schild - Schild und einer ein einer Frau ein whrend ein Buch Mann und einem Frau - ein in ihm anderen . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 4880\n",
            "Completed training step 4900\n",
            "Completed training step 4920\n",
            "Completed training step 4940\n",
            "Completed training step 4960\n",
            "Completed training step 4980\n",
            "Completed training step 5000\n",
            "Completed training step 5020\n",
            "Completed validation step 1240\n",
            "Completed validation step 1260\n",
            "epoch 31 took 14.928425788879395\n",
            "avg training loss: 2.48917546393765\n",
            "avg validation loss: 3.235913932323456\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 923, 776, 776, 114, 111, 1002, 12, 121, 117, 1377, 741, 1614, 114, 107, 773, 12, 12, 117, 189, 294, 154, 440, 12, 152, 102, 107, 601, 119, 189, 451, 75, 75, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit langen Haaren Haaren und einem Oberkrper , der auf blaue Hose Armen und eine Hosen , , auf dem roten am Strand , die er eine groe mit dem Jungen s s .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 632, 230, 176, 111, 474, 114, 906, 114, 111, 103, 111, 138, 100, 250, 103, 979, 124, 100, 111, 138, 14, 103, 230, 111, 461, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit Brille sitzt vor einem Tisch und Schild und einem ein einem Frau in whrend ein Buch Mann in einem Frau . ein sitzt einem anderen . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 5040\n",
            "Completed training step 5060\n",
            "Completed training step 5080\n",
            "Completed training step 5100\n",
            "Completed training step 5120\n",
            "Completed training step 5140\n",
            "Completed training step 5160\n",
            "Completed training step 5180\n",
            "Completed validation step 1280\n",
            "Completed validation step 1300\n",
            "epoch 32 took 14.98180627822876\n",
            "avg training loss: 2.4602105055645014\n",
            "avg validation loss: 3.2059256434440613\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 111, 923, 776, 114, 111, 1002, 12, 121, 103, 1377, 741, 851, 114, 103, 773, 319, 12, 117, 111, 525, 154, 440, 12, 121, 102, 103, 522, 119, 111, 294, 440, 75, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit einem langen Haaren und einem Oberkrper , der ein blaue Hose Shorts und ein Hosen trgt , auf einem kleinen am Strand , der er ein kleine mit einem roten Strand s .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 632, 12, 176, 111, 847, 114, 906, 114, 125, 103, 111, 138, 103, 250, 103, 847, 124, 114, 111, 138, 14, 736, 14, 728, 847, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit Brille , vor einem Mikrofon und Schild und einer ein einem Frau ein whrend ein Mikrofon Mann und einem Frau . liest . ihm Mikrofon . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 5200\n",
            "Completed training step 5220\n",
            "Completed training step 5240\n",
            "Completed training step 5260\n",
            "Completed training step 5280\n",
            "Completed training step 5300\n",
            "Completed training step 5320\n",
            "Completed validation step 1320\n",
            "Completed validation step 1340\n",
            "epoch 33 took 15.57840633392334\n",
            "avg training loss: 2.4241067991135226\n",
            "avg validation loss: 3.192596381902695\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 923, 776, 776, 114, 111, 1002, 12, 121, 107, 1377, 741, 1452, 114, 107, 773, 319, 12, 117, 111, 294, 154, 440, 119, 121, 102, 107, 1377, 119, 111, 461, 440, 1, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit langen Haaren Haaren und einem Oberkrper , der eine blaue Hose hose und eine Hosen trgt , auf einem roten am Strand mit der er eine blaue mit einem anderen Strand .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 632, 114, 176, 111, 847, 13, 847, 114, 111, 176, 111, 461, 103, 250, 103, 747, 124, 1510, 111, 138, 14, 103, 14, 728, 979, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit Brille und vor einem Mikrofon - Mikrofon und einem vor einem anderen ein whrend ein anderer Mann zusieht einem Frau . ein . ihm Buch . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 5340\n",
            "Completed training step 5360\n",
            "Completed training step 5380\n",
            "Completed training step 5400\n",
            "Completed training step 5420\n",
            "Completed training step 5440\n",
            "Completed training step 5460\n",
            "Completed training step 5480\n",
            "Completed validation step 1360\n",
            "Completed validation step 1380\n",
            "epoch 34 took 15.661166906356812\n",
            "avg training loss: 2.3920351525021206\n",
            "avg validation loss: 3.184039610624313\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 923, 776, 776, 114, 111, 1002, 12, 121, 107, 1377, 741, 1452, 114, 103, 773, 303, 12, 117, 189, 633, 154, 440, 12, 114, 102, 107, 351, 119, 111, 461, 547, 75, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit langen Haaren Haaren und einem Oberkrper , der eine blaue Hose hose und ein Hosen geht , auf dem Gehweg am Strand , und er eine junge mit einem anderen Boden s .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 632, 230, 176, 111, 847, 114, 847, 114, 111, 103, 111, 461, 103, 250, 103, 847, 124, 100, 111, 461, 13, 103, 14, 728, 979, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit Brille sitzt vor einem Mikrofon und Mikrofon und einem ein einem anderen ein whrend ein Mikrofon Mann in einem anderen - ein . ihm Buch . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 5500\n",
            "Completed training step 5520\n",
            "Completed training step 5540\n",
            "Completed training step 5560\n",
            "Completed training step 5580\n",
            "Completed training step 5600\n",
            "Completed training step 5620\n",
            "Completed training step 5640\n",
            "Completed validation step 1400\n",
            "Completed validation step 1420\n",
            "epoch 35 took 14.841278553009033\n",
            "avg training loss: 2.3620938465094112\n",
            "avg validation loss: 3.1724325120449066\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 923, 923, 776, 114, 111, 1002, 12, 121, 107, 1377, 741, 851, 114, 107, 773, 319, 12, 117, 111, 633, 154, 440, 119, 121, 102, 107, 1377, 119, 111, 451, 440, 444, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit langen langen Haaren und einem Oberkrper , der eine blaue Hose Shorts und eine Hosen trgt , auf einem Gehweg am Strand mit der er eine blaue mit einem Jungen Strand sind .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 632, 114, 176, 111, 847, 114, 847, 114, 125, 103, 111, 2375, 103, 103, 103, 847, 124, 14, 728, 138, 14, 103, 14, 728, 847, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit Brille und vor einem Mikrofon und Mikrofon und einer ein einem Tasse ein ein ein Mikrofon Mann . ihm Frau . ein . ihm Mikrofon . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 5660\n",
            "Completed training step 5680\n",
            "Completed training step 5700\n",
            "Completed training step 5720\n",
            "Completed training step 5740\n",
            "Completed training step 5760\n",
            "Completed training step 5780\n",
            "Completed training step 5800\n",
            "Completed validation step 1440\n",
            "Completed validation step 1460\n",
            "epoch 36 took 14.837905645370483\n",
            "avg training loss: 2.334672077446227\n",
            "avg validation loss: 3.164637100696564\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 923, 776, 776, 114, 111, 1002, 114, 121, 107, 138, 741, 1452, 114, 107, 773, 114, 12, 117, 111, 633, 154, 440, 114, 121, 102, 107, 245, 119, 111, 245, 65, 75, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit langen Haaren Haaren und einem Oberkrper und der eine Frau Hose hose und eine Hosen und , auf einem Gehweg am Strand und der er eine Strae mit einem Strae i s .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 632, 230, 176, 111, 847, 114, 847, 114, 125, 225, 111, 138, 103, 250, 103, 847, 124, 1510, 728, 138, 1510, 736, 1510, 728, 847, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit Brille sitzt vor einem Mikrofon und Mikrofon und einer aus einem Frau ein whrend ein Mikrofon Mann zusieht ihm Frau zusieht liest zusieht ihm Mikrofon . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 5820\n",
            "Completed training step 5840\n",
            "Completed training step 5860\n",
            "Completed training step 5880\n",
            "Completed training step 5900\n",
            "Completed training step 5920\n",
            "Completed training step 5940\n",
            "Completed training step 5960\n",
            "Completed validation step 1480\n",
            "Completed validation step 1500\n",
            "epoch 37 took 14.934734582901001\n",
            "avg training loss: 2.308911230154098\n",
            "avg validation loss: 3.178131139278412\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 923, 923, 776, 114, 111, 1002, 114, 121, 107, 1377, 741, 851, 114, 107, 773, 114, 12, 154, 111, 633, 154, 440, 12, 117, 102, 107, 5729, 119, 111, 47, 440, 75, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit langen langen Haaren und einem Oberkrper und der eine blaue Hose Shorts und eine Hosen und , am einem Gehweg am Strand , auf er eine Sandburg mit einem R Strand s .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 632, 114, 176, 111, 124, 114, 847, 100, 736, 395, 111, 1639, 12, 121, 103, 847, 124, 114, 1379, 138, 736, 736, 14, 728, 847, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit Brille und vor einem Mann und Mikrofon in liest etwas einem Schrze , der ein Mikrofon Mann und Bier Frau liest liest . ihm Mikrofon . .\n",
            "Average validation loss did not decrease from  3.164637100696564\n",
            "Failed to decrease the average validation loss 1 times.\n",
            "\n",
            "\n",
            "Completed training step 5980\n",
            "Completed training step 6000\n",
            "Completed training step 6020\n",
            "Completed training step 6040\n",
            "Completed training step 6060\n",
            "Completed training step 6080\n",
            "Completed training step 6100\n",
            "Completed training step 6120\n",
            "Completed validation step 1520\n",
            "Completed validation step 1540\n",
            "epoch 38 took 14.85468578338623\n",
            "avg training loss: 2.285211611705221\n",
            "avg validation loss: 3.186483401060104\n",
            "expected train output Ein Mann mit mittel langen Haaren und nacktem Oberkrper , der eine rote Be anie und blaue Shorts trgt , auf einem Knie am Strand , wie er eine Sandburg mit einer R inne formt .\n",
            "argmax x: [109, 124, 119, 923, 923, 776, 12, 125, 1002, 12, 121, 107, 1377, 741, 851, 114, 107, 773, 319, 12, 117, 111, 451, 154, 440, 12, 648, 102, 107, 1377, 119, 111, 47, 440, 12, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: Ein Mann mit langen langen Haaren , einer Oberkrper , der eine blaue Hose Shorts und eine Hosen trgt , auf einem Jungen am Strand , wie er eine blaue mit einem R Strand , .\n",
            "expected validation output Ein Mann mit Brille sitzt vor einem Apple - Computer und trinkt aus einer Tasse , whrend ein anderer Mann mit einer Kappe und Brille vor einem Mikrofon steht .\n",
            "argmax x: [109, 124, 119, 632, 114, 176, 111, 124, 114, 906, 114, 125, 225, 111, 461, 103, 121, 103, 847, 124, 114, 111, 138, 14, 736, 1510, 728, 906, 14, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded validation output: Ein Mann mit Brille und vor einem Mann und Schild und einer aus einem anderen ein der ein Mikrofon Mann und einem Frau . liest zusieht ihm Schild . .\n",
            "Average validation loss did not decrease from  3.164637100696564\n",
            "Failed to decrease the average validation loss 2 times.\n",
            "Stopping training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_decoder_transformer = EncoderDecoderTransformer(num_encoder_layers, num_decoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, d_e, vocab_size, max_sequence_length, p_dropout, False).to(device)\n"
      ],
      "metadata": {
        "id": "VLT6fo1IjhG3"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#state_dict_filename = folder + \"encoder_decoder_transformer_state_dict_\" + \"2024-08-02 15\"\n",
        "state_dict = torch.load(state_dict_filename, map_location = device)\n",
        "print(state_dict.keys())\n",
        "encoder_decoder_transformer.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGBx0brYMJE6",
        "outputId": "4aa324fd-afa6-493a-c7e0-b697011f14ab"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['src_embedding.table.weight', 'tgt_embedding.table.weight', 'unembedding.weight.weight', 'unembedding.weight.bias', 'positionalEmbedding.table.weight', 'encoder.layers.0.multi_head_attention.weight_query.weight', 'encoder.layers.0.multi_head_attention.weight_query.bias', 'encoder.layers.0.multi_head_attention.weight_key.weight', 'encoder.layers.0.multi_head_attention.weight_key.bias', 'encoder.layers.0.multi_head_attention.weight_value.weight', 'encoder.layers.0.multi_head_attention.weight_value.bias', 'encoder.layers.0.multi_head_attention.weight_out.weight', 'encoder.layers.0.multi_head_attention.weight_out.bias', 'encoder.layers.0.layer_norm1.scale', 'encoder.layers.0.layer_norm1.offset', 'encoder.layers.0.feed_forward.mlp1', 'encoder.layers.0.feed_forward.mlp2', 'encoder.layers.0.feed_forward.mlp1_bias', 'encoder.layers.0.feed_forward.mlp2_bias', 'encoder.layers.0.layer_norm2.scale', 'encoder.layers.0.layer_norm2.offset', 'encoder.layers.1.multi_head_attention.weight_query.weight', 'encoder.layers.1.multi_head_attention.weight_query.bias', 'encoder.layers.1.multi_head_attention.weight_key.weight', 'encoder.layers.1.multi_head_attention.weight_key.bias', 'encoder.layers.1.multi_head_attention.weight_value.weight', 'encoder.layers.1.multi_head_attention.weight_value.bias', 'encoder.layers.1.multi_head_attention.weight_out.weight', 'encoder.layers.1.multi_head_attention.weight_out.bias', 'encoder.layers.1.layer_norm1.scale', 'encoder.layers.1.layer_norm1.offset', 'encoder.layers.1.feed_forward.mlp1', 'encoder.layers.1.feed_forward.mlp2', 'encoder.layers.1.feed_forward.mlp1_bias', 'encoder.layers.1.feed_forward.mlp2_bias', 'encoder.layers.1.layer_norm2.scale', 'encoder.layers.1.layer_norm2.offset', 'encoder.layers.2.multi_head_attention.weight_query.weight', 'encoder.layers.2.multi_head_attention.weight_query.bias', 'encoder.layers.2.multi_head_attention.weight_key.weight', 'encoder.layers.2.multi_head_attention.weight_key.bias', 'encoder.layers.2.multi_head_attention.weight_value.weight', 'encoder.layers.2.multi_head_attention.weight_value.bias', 'encoder.layers.2.multi_head_attention.weight_out.weight', 'encoder.layers.2.multi_head_attention.weight_out.bias', 'encoder.layers.2.layer_norm1.scale', 'encoder.layers.2.layer_norm1.offset', 'encoder.layers.2.feed_forward.mlp1', 'encoder.layers.2.feed_forward.mlp2', 'encoder.layers.2.feed_forward.mlp1_bias', 'encoder.layers.2.feed_forward.mlp2_bias', 'encoder.layers.2.layer_norm2.scale', 'encoder.layers.2.layer_norm2.offset', 'encoder.layers.3.multi_head_attention.weight_query.weight', 'encoder.layers.3.multi_head_attention.weight_query.bias', 'encoder.layers.3.multi_head_attention.weight_key.weight', 'encoder.layers.3.multi_head_attention.weight_key.bias', 'encoder.layers.3.multi_head_attention.weight_value.weight', 'encoder.layers.3.multi_head_attention.weight_value.bias', 'encoder.layers.3.multi_head_attention.weight_out.weight', 'encoder.layers.3.multi_head_attention.weight_out.bias', 'encoder.layers.3.layer_norm1.scale', 'encoder.layers.3.layer_norm1.offset', 'encoder.layers.3.feed_forward.mlp1', 'encoder.layers.3.feed_forward.mlp2', 'encoder.layers.3.feed_forward.mlp1_bias', 'encoder.layers.3.feed_forward.mlp2_bias', 'encoder.layers.3.layer_norm2.scale', 'encoder.layers.3.layer_norm2.offset', 'encoder.final_norm.scale', 'encoder.final_norm.offset', 'decoder.layers.0.multi_head_self_attention.weight_query.weight', 'decoder.layers.0.multi_head_self_attention.weight_query.bias', 'decoder.layers.0.multi_head_self_attention.weight_key.weight', 'decoder.layers.0.multi_head_self_attention.weight_key.bias', 'decoder.layers.0.multi_head_self_attention.weight_value.weight', 'decoder.layers.0.multi_head_self_attention.weight_value.bias', 'decoder.layers.0.multi_head_self_attention.weight_out.weight', 'decoder.layers.0.multi_head_self_attention.weight_out.bias', 'decoder.layers.0.layer_norm1.scale', 'decoder.layers.0.layer_norm1.offset', 'decoder.layers.0.multi_head_global_attention.weight_query.weight', 'decoder.layers.0.multi_head_global_attention.weight_query.bias', 'decoder.layers.0.multi_head_global_attention.weight_key.weight', 'decoder.layers.0.multi_head_global_attention.weight_key.bias', 'decoder.layers.0.multi_head_global_attention.weight_value.weight', 'decoder.layers.0.multi_head_global_attention.weight_value.bias', 'decoder.layers.0.multi_head_global_attention.weight_out.weight', 'decoder.layers.0.multi_head_global_attention.weight_out.bias', 'decoder.layers.0.layer_norm2.scale', 'decoder.layers.0.layer_norm2.offset', 'decoder.layers.0.feed_forward.mlp1', 'decoder.layers.0.feed_forward.mlp2', 'decoder.layers.0.feed_forward.mlp1_bias', 'decoder.layers.0.feed_forward.mlp2_bias', 'decoder.layers.0.layer_norm3.scale', 'decoder.layers.0.layer_norm3.offset', 'decoder.layers.1.multi_head_self_attention.weight_query.weight', 'decoder.layers.1.multi_head_self_attention.weight_query.bias', 'decoder.layers.1.multi_head_self_attention.weight_key.weight', 'decoder.layers.1.multi_head_self_attention.weight_key.bias', 'decoder.layers.1.multi_head_self_attention.weight_value.weight', 'decoder.layers.1.multi_head_self_attention.weight_value.bias', 'decoder.layers.1.multi_head_self_attention.weight_out.weight', 'decoder.layers.1.multi_head_self_attention.weight_out.bias', 'decoder.layers.1.layer_norm1.scale', 'decoder.layers.1.layer_norm1.offset', 'decoder.layers.1.multi_head_global_attention.weight_query.weight', 'decoder.layers.1.multi_head_global_attention.weight_query.bias', 'decoder.layers.1.multi_head_global_attention.weight_key.weight', 'decoder.layers.1.multi_head_global_attention.weight_key.bias', 'decoder.layers.1.multi_head_global_attention.weight_value.weight', 'decoder.layers.1.multi_head_global_attention.weight_value.bias', 'decoder.layers.1.multi_head_global_attention.weight_out.weight', 'decoder.layers.1.multi_head_global_attention.weight_out.bias', 'decoder.layers.1.layer_norm2.scale', 'decoder.layers.1.layer_norm2.offset', 'decoder.layers.1.feed_forward.mlp1', 'decoder.layers.1.feed_forward.mlp2', 'decoder.layers.1.feed_forward.mlp1_bias', 'decoder.layers.1.feed_forward.mlp2_bias', 'decoder.layers.1.layer_norm3.scale', 'decoder.layers.1.layer_norm3.offset', 'decoder.layers.2.multi_head_self_attention.weight_query.weight', 'decoder.layers.2.multi_head_self_attention.weight_query.bias', 'decoder.layers.2.multi_head_self_attention.weight_key.weight', 'decoder.layers.2.multi_head_self_attention.weight_key.bias', 'decoder.layers.2.multi_head_self_attention.weight_value.weight', 'decoder.layers.2.multi_head_self_attention.weight_value.bias', 'decoder.layers.2.multi_head_self_attention.weight_out.weight', 'decoder.layers.2.multi_head_self_attention.weight_out.bias', 'decoder.layers.2.layer_norm1.scale', 'decoder.layers.2.layer_norm1.offset', 'decoder.layers.2.multi_head_global_attention.weight_query.weight', 'decoder.layers.2.multi_head_global_attention.weight_query.bias', 'decoder.layers.2.multi_head_global_attention.weight_key.weight', 'decoder.layers.2.multi_head_global_attention.weight_key.bias', 'decoder.layers.2.multi_head_global_attention.weight_value.weight', 'decoder.layers.2.multi_head_global_attention.weight_value.bias', 'decoder.layers.2.multi_head_global_attention.weight_out.weight', 'decoder.layers.2.multi_head_global_attention.weight_out.bias', 'decoder.layers.2.layer_norm2.scale', 'decoder.layers.2.layer_norm2.offset', 'decoder.layers.2.feed_forward.mlp1', 'decoder.layers.2.feed_forward.mlp2', 'decoder.layers.2.feed_forward.mlp1_bias', 'decoder.layers.2.feed_forward.mlp2_bias', 'decoder.layers.2.layer_norm3.scale', 'decoder.layers.2.layer_norm3.offset', 'decoder.layers.3.multi_head_self_attention.weight_query.weight', 'decoder.layers.3.multi_head_self_attention.weight_query.bias', 'decoder.layers.3.multi_head_self_attention.weight_key.weight', 'decoder.layers.3.multi_head_self_attention.weight_key.bias', 'decoder.layers.3.multi_head_self_attention.weight_value.weight', 'decoder.layers.3.multi_head_self_attention.weight_value.bias', 'decoder.layers.3.multi_head_self_attention.weight_out.weight', 'decoder.layers.3.multi_head_self_attention.weight_out.bias', 'decoder.layers.3.layer_norm1.scale', 'decoder.layers.3.layer_norm1.offset', 'decoder.layers.3.multi_head_global_attention.weight_query.weight', 'decoder.layers.3.multi_head_global_attention.weight_query.bias', 'decoder.layers.3.multi_head_global_attention.weight_key.weight', 'decoder.layers.3.multi_head_global_attention.weight_key.bias', 'decoder.layers.3.multi_head_global_attention.weight_value.weight', 'decoder.layers.3.multi_head_global_attention.weight_value.bias', 'decoder.layers.3.multi_head_global_attention.weight_out.weight', 'decoder.layers.3.multi_head_global_attention.weight_out.bias', 'decoder.layers.3.layer_norm2.scale', 'decoder.layers.3.layer_norm2.offset', 'decoder.layers.3.feed_forward.mlp1', 'decoder.layers.3.feed_forward.mlp2', 'decoder.layers.3.feed_forward.mlp1_bias', 'decoder.layers.3.feed_forward.mlp2_bias', 'decoder.layers.3.layer_norm3.scale', 'decoder.layers.3.layer_norm3.offset', 'decoder.final_norm.scale', 'decoder.final_norm.offset'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_with_one_sample(model):\n",
        "    dataloader_iter = iter(train_dataloader)\n",
        "    for src_batch, tgt_batch in dataloader_iter:\n",
        "        # print(\"x:\", sequence_x)\n",
        "        # print(\"z:\", sequence_z)\n",
        "        # sequence_x, sequence_z = sequenceDataset.__getitem__(i)\n",
        "        src_batch = [src_batch[0]]\n",
        "        tgt_batch = [tgt_batch[0]]\n",
        "        src_tokens = torch.IntTensor([sequence.ids for sequence in src_batch]).to(device)\n",
        "        encoder_input = src_tokens\n",
        "        print(\"encoder input\", encoder_input)\n",
        "        decoded_input = pad_collate.src_tokenizer.decode(encoder_input[0].tolist())\n",
        "        print(\"decoded input\", decoded_input)\n",
        "        train_tgt_tokens = torch.IntTensor([sequence.ids for sequence in tgt_batch]).to(device)\n",
        "        decoder_input = train_tgt_tokens[:, :-1]\n",
        "        print(\"decoder input\", decoder_input)\n",
        "        decoder_desired_output_train = train_tgt_tokens[:, 1:]\n",
        "        src_masks = torch.IntTensor([sequence.attention_mask for sequence in src_batch]).to(device)\n",
        "        tgt_masks = torch.IntTensor([sequence.attention_mask for sequence in tgt_batch])[:, :-1].to(device)\n",
        "        # print(\"src masks\", src_masks)\n",
        "        # print(\"tgt masks\", tgt_masks)\n",
        "        train_output = encoder_decoder_transformer(encoder_input, decoder_input, src_masks, tgt_masks)\n",
        "        print(train_output.shape)\n",
        "        decoded_output = decode(train_output[0], pad_collate.tgt_tokenizer)\n",
        "        print(\"decoded output\", decoded_output)\n",
        "        break\n",
        "\n",
        "test_model_with_one_sample(encoder_decoder_transformer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bertgYLZzZ7m",
        "outputId": "3d579f81-a422-4eb2-a04d-cbf8c57805a5"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder input tensor([[  30,   93,   89,   94, 1602,   15,    2,    2]], device='cuda:0',\n",
            "       dtype=torch.int32)\n",
            "decoded input A man on the sea .\n",
            "decoder input tensor([[   0,   30,   93,   89,   94, 1602,   15,    1,    2]],\n",
            "       device='cuda:0', dtype=torch.int32)\n",
            "torch.Size([1, 9, 10000])\n",
            "argmax x: [30, 93, 89, 94, 1602, 15, 1, 2, 2]\n",
            "decoded output A man on the sea .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(encoder_decoder_transformer.state_dict(), state_dict_filename)"
      ],
      "metadata": {
        "id": "kuI-KXWBa1Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_from_tokens(model, input, src_tokenizer, tgt_tokenizer):\n",
        "    model.disable_subsequent_mask()\n",
        "    src_tokenizer.no_padding()\n",
        "    tgt_tokenizer.no_padding()\n",
        "\n",
        "    src_tokenizer.no_truncation()\n",
        "    tgt_tokenizer.no_truncation()\n",
        "    src_sequence = input\n",
        "    print(src_sequence)\n",
        "    src_sequence = src_tokenizer.encode(src_sequence)\n",
        "    print(src_sequence)\n",
        "    print(src_tokenizer.decode(src_sequence.ids))\n",
        "    src_sequence = torch.IntTensor(src_sequence.ids).unsqueeze(0).to(device)\n",
        "    print(\"src tokens\", src_sequence)\n",
        "    tgt_sequence = torch.IntTensor([0]).unsqueeze(0).to(device)\n",
        "    src_mask = torch.ones(src_sequence.shape, dtype=torch.int32).to(device)\n",
        "    print(\"decoder input\", tgt_sequence)\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        length_gen = 10\n",
        "        for i in range(length_gen):\n",
        "            tgt_mask = torch.ones(tgt_sequence.shape, dtype=torch.int32).to(device)\n",
        "            prediction = model(src_sequence, tgt_sequence, src_mask, tgt_mask)\n",
        "            #print(\"prediction:\", prediction)\n",
        "            prediction = torch.softmax(prediction, -1)\n",
        "            #print(\"softmax prediction:\", prediction.shape)\n",
        "            prediction = torch.argmax(prediction, dim=-1)\n",
        "            print(\"argmax prediction:\", prediction)\n",
        "            print(\"actual prediction:\", tgt_tokenizer.decode(prediction[0].tolist()))\n",
        "            last_token = prediction[0][-1]\n",
        "            tgt_sequence = torch.cat((tgt_sequence, last_token.unsqueeze(0).unsqueeze(0)), dim=-1)\n",
        "            if last_token == 1:\n",
        "                break\n",
        "    return tgt_sequence\n",
        "\n",
        "tgt_sequence = predict_from_tokens(encoder_decoder_transformer, \"A man on the sea\", pad_collate.src_tokenizer, pad_collate.tgt_tokenizer)\n",
        "print(tgt_sequence)\n",
        "print(pad_collate.tgt_tokenizer.decode(tgt_sequence[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVEp1SmrhVuf",
        "outputId": "3f8f998f-c1ff-4581-af23-5ded5dc7fbb4"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A man on the sea\n",
            "Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "A man on the sea\n",
            "src tokens tensor([[  30,   93,   89,   94, 1602]], device='cuda:0', dtype=torch.int32)\n",
            "decoder input tensor([[0]], device='cuda:0', dtype=torch.int32)\n",
            "argmax prediction: tensor([[109]], device='cuda:0')\n",
            "actual prediction: Ein\n",
            "argmax prediction: tensor([[124, 124]], device='cuda:0')\n",
            "actual prediction: Mann Mann\n",
            "argmax prediction: tensor([[154, 124, 154]], device='cuda:0')\n",
            "actual prediction: am Mann am\n",
            "argmax prediction: tensor([[109,   2, 117, 440]], device='cuda:0')\n",
            "actual prediction: Ein auf Strand\n",
            "argmax prediction: tensor([[109,   2,  12, 440,  12]], device='cuda:0')\n",
            "actual prediction: Ein , Strand ,\n",
            "argmax prediction: tensor([[121, 117, 117, 440, 117, 121]], device='cuda:0')\n",
            "actual prediction: der auf auf Strand auf der\n",
            "argmax prediction: tensor([[121, 117, 117, 440, 117, 121, 117]], device='cuda:0')\n",
            "actual prediction: der auf auf Strand auf der auf\n",
            "argmax prediction: tensor([[ 121,    2,  154,  440,  154,  121, 1869,  121]], device='cuda:0')\n",
            "actual prediction: der am Strand am der Wellen der\n",
            "argmax prediction: tensor([[ 121,    2,  154,  440,  154,  121, 1869,  121, 1869]],\n",
            "       device='cuda:0')\n",
            "actual prediction: der am Strand am der Wellen der Wellen\n",
            "argmax prediction: tensor([[ 121,    2,  154,  440,  154,  121, 1869,  121, 1869,  154]],\n",
            "       device='cuda:0')\n",
            "actual prediction: der am Strand am der Wellen der Wellen am\n",
            "tensor([[   0,  109,  124,  154,  440,   12,  121,  117,  121, 1869,  154]],\n",
            "       device='cuda:0')\n",
            "Ein Mann am Strand , der auf der Wellen am\n"
          ]
        }
      ]
    }
  ]
}