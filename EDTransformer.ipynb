{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNg9hlAtFYKM+oyAs1DzXEX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JackWittmayer/Transformer-Implementation/blob/main/EDTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37L0J9nog5yz",
        "outputId": "4d3c2fa2-beb3-4edc-e0b4-2a73964184d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VNf0Rr3ogy-4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import os\n",
        "import pickle\n",
        "from unicodedata import normalize\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import log_softmax, pad\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(25)\n",
        "random.seed(25)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "2Hs0kEZYlwc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c94ef30-70c3-466a-9a35-9fbccecfd670"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_X = torch.tensor([[3, 2, 0, 1], [1, 2, 3, 0]], dtype=torch.int32).to(device)\n",
        "SAMPLE_Z = torch.tensor([4, 1, 7, 6], dtype=torch.int32).to(device)"
      ],
      "metadata": {
        "id": "bMXiZFPpEtZa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printIfVerbose(verbose, tag, value):\n",
        "    if verbose:\n",
        "        print(tag, value)"
      ],
      "metadata": {
        "id": "6gsV7Wgv2DKh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        super().__init__()\n",
        "        self.table = nn.Embedding(vocab_size, embedding_size).to(device)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        embeddings = self.table(sequence)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "Pk7xtwam89Hh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_embedding():\n",
        "    torch.manual_seed(25)\n",
        "    vocab_size = 4\n",
        "    embedding = Embedding(vocab_size, 4)\n",
        "    print(\"weight:\", embedding.table.weight)\n",
        "    print(\"SAMPLE_X: \", SAMPLE_X)\n",
        "    output = embedding(SAMPLE_X)\n",
        "    print(\"output:\", output)\n",
        "    for j in range(len(output)):\n",
        "        #print(\"sample:\", sample)\n",
        "        for i in range(vocab_size):\n",
        "            assert output[j, i, :].eq(embedding.table.weight[SAMPLE_X[j, i]]).all()\n",
        "test_embedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StJF4qlIBYVj",
        "outputId": "0d596e11-d1b7-4ab5-8c79-98378d45c252"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight: Parameter containing:\n",
            "tensor([[ 0.0877, -0.6113,  0.3441, -1.2916],\n",
            "        [-0.5874,  0.8060,  1.3200,  0.4826],\n",
            "        [ 1.6671, -0.2342,  0.1074,  1.7852],\n",
            "        [ 0.7874, -0.2466,  0.2384, -0.6746]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "SAMPLE_X:  tensor([[3, 2, 0, 1],\n",
            "        [1, 2, 3, 0]], device='cuda:0', dtype=torch.int32)\n",
            "output: tensor([[[ 0.7874, -0.2466,  0.2384, -0.6746],\n",
            "         [ 1.6671, -0.2342,  0.1074,  1.7852],\n",
            "         [ 0.0877, -0.6113,  0.3441, -1.2916],\n",
            "         [-0.5874,  0.8060,  1.3200,  0.4826]],\n",
            "\n",
            "        [[-0.5874,  0.8060,  1.3200,  0.4826],\n",
            "         [ 1.6671, -0.2342,  0.1074,  1.7852],\n",
            "         [ 0.7874, -0.2466,  0.2384, -0.6746],\n",
            "         [ 0.0877, -0.6113,  0.3441, -1.2916]]], device='cuda:0',\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Unembedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Linear(embedding_size, vocab_size).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.weight(x)"
      ],
      "metadata": {
        "id": "EpoJIpc_CaX6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_unembedding():\n",
        "    torch.manual_seed(25)\n",
        "    vocab_size = 10\n",
        "    embedding_size = 4\n",
        "    sequence_length = 4\n",
        "    batch_size = 2\n",
        "    input = torch.rand(batch_size, sequence_length, embedding_size).to(device)\n",
        "    unembedding = Unembedding(vocab_size, embedding_size)\n",
        "\n",
        "    print(\"weight:\", unembedding.weight)\n",
        "    print(\"input: \", input)\n",
        "    output = unembedding(input)\n",
        "    print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, sequence_length, vocab_size)\n",
        "test_unembedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLZ0-F4xNOMf",
        "outputId": "f83116d1-523c-4981-f99c-f16d0142524a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight: Linear(in_features=4, out_features=10, bias=True)\n",
            "input:  tensor([[[0.7518, 0.1929, 0.0629, 0.9118],\n",
            "         [0.3828, 0.2990, 0.5933, 0.2911],\n",
            "         [0.2416, 0.5582, 0.0481, 0.3497],\n",
            "         [0.3520, 0.9528, 0.0284, 0.8488]],\n",
            "\n",
            "        [[0.3947, 0.5181, 0.9726, 0.8813],\n",
            "         [0.0056, 0.3056, 0.9384, 0.7949],\n",
            "         [0.4399, 0.1766, 0.8739, 0.1425],\n",
            "         [0.4682, 0.6254, 0.3040, 0.7923]]], device='cuda:0')\n",
            "output: tensor([[[-4.9334e-01,  3.9030e-01, -3.4348e-03,  2.0479e-01, -1.7155e-01,\n",
            "           3.0325e-01,  9.5298e-01, -9.2740e-01,  3.5209e-01, -2.1405e-02],\n",
            "         [-6.2972e-02, -6.9980e-02, -5.4304e-02,  1.2675e-01, -5.5075e-01,\n",
            "           1.8844e-01,  8.6408e-01, -5.4956e-01,  4.7789e-01,  7.8078e-02],\n",
            "         [-2.9110e-01,  3.9284e-02,  7.2926e-02,  2.0875e-01, -3.4683e-01,\n",
            "           1.1962e-01,  7.2445e-01, -5.6804e-01,  4.2547e-01,  1.0732e-02],\n",
            "         [-3.4147e-01,  6.4171e-02, -5.8167e-02,  1.8160e-01, -2.1651e-01,\n",
            "          -1.4458e-01,  1.0255e+00, -9.2857e-01,  5.6889e-01, -7.0657e-02]],\n",
            "\n",
            "        [[ 3.2716e-02, -1.6396e-01, -2.2999e-01,  1.5606e-01, -5.7223e-01,\n",
            "          -1.2983e-01,  1.2599e+00, -8.8230e-01,  6.9367e-01,  1.1626e-01],\n",
            "         [ 7.3857e-03, -2.2392e-01, -6.1284e-02,  3.6410e-01, -6.3714e-01,\n",
            "          -1.2969e-01,  1.0509e+00, -6.4156e-01,  6.5060e-01,  2.7214e-01],\n",
            "         [ 8.3425e-02, -1.5596e-01, -1.1466e-01,  6.0901e-02, -6.8152e-01,\n",
            "           2.4368e-01,  8.9560e-01, -4.8380e-01,  4.9643e-01,  1.0562e-01],\n",
            "         [-2.5832e-01,  7.1240e-02, -8.8499e-02,  1.6288e-01, -3.2179e-01,\n",
            "          -1.3083e-04,  1.0437e+00, -8.7687e-01,  5.3716e-01, -1.0231e-02]]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, embedding_size, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.table = nn.Embedding(max_sequence_length, embedding_size).to(device)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        positions = torch.zeros(sequence.shape, dtype=torch.int32)\n",
        "        positions[:, ::] = torch.arange(0, sequence.shape[-1])\n",
        "        #print(\"positions\", positions)\n",
        "        positional_embeddings = self.table(positions.to(device))\n",
        "        return positional_embeddings"
      ],
      "metadata": {
        "id": "5xpYM9Zf_KLw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_positional_embedding():\n",
        "    embedding_size = 8\n",
        "    max_sequence_length = 10\n",
        "    batch_size = 2\n",
        "    positional_embedding = PositionalEmbedding(embedding_size, max_sequence_length)\n",
        "    output = positional_embedding(SAMPLE_X)\n",
        "    print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, SAMPLE_X.shape[-1], embedding_size)\n",
        "test_positional_embedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l1Z_ZH8Pb_P",
        "outputId": "f45acaf3-b2f3-49c8-88fe-521d6785311a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output: tensor([[[ 0.3412, -0.2888, -0.4162, -1.2433,  0.3364, -2.1897, -0.2086,\n",
            "           0.0196],\n",
            "         [ 0.2461, -0.0812, -0.4464, -1.2595,  0.5963, -1.3647, -0.7684,\n",
            "           0.3472],\n",
            "         [-0.0142,  0.1426,  2.0701, -0.1623, -0.4448,  0.8318, -0.2930,\n",
            "          -0.2068],\n",
            "         [-0.8096,  1.2487,  0.5594, -0.3657,  0.5478, -1.4327, -1.4111,\n",
            "          -0.4237]],\n",
            "\n",
            "        [[ 0.3412, -0.2888, -0.4162, -1.2433,  0.3364, -2.1897, -0.2086,\n",
            "           0.0196],\n",
            "         [ 0.2461, -0.0812, -0.4464, -1.2595,  0.5963, -1.3647, -0.7684,\n",
            "           0.3472],\n",
            "         [-0.0142,  0.1426,  2.0701, -0.1623, -0.4448,  0.8318, -0.2930,\n",
            "          -0.2068],\n",
            "         [-0.8096,  1.2487,  0.5594, -0.3657,  0.5478, -1.4327, -1.4111,\n",
            "          -0.4237]]], device='cuda:0', grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(queries, keys, values, mask, dropout, verbose):\n",
        "    printIfVerbose(verbose, \"queries:\", queries)\n",
        "    printIfVerbose(verbose, \"keys:\", keys)\n",
        "    printIfVerbose(verbose, \"values:\", values)\n",
        "    keys_transposed = torch.transpose(keys, -2, -1)\n",
        "    printIfVerbose(verbose, \"keys_transposed:\", keys_transposed)\n",
        "    scores = torch.matmul(queries, keys_transposed)\n",
        "    #assert scores.shape == (keys.shape[0], keys.shape[-1], queries.shape[-1])\n",
        "    printIfVerbose(verbose, \"scores:\", scores)\n",
        "    printIfVerbose(verbose, \"scores:\", scores.shape)\n",
        "    printIfVerbose(verbose, \"masks:\", mask.shape)\n",
        "    scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    printIfVerbose(verbose, \"masked scores:\", scores)\n",
        "    d_attn = keys.shape[-1]\n",
        "    scaled_scores = scores / math.sqrt(d_attn)\n",
        "    printIfVerbose(verbose, \"scaled_scores:\", scaled_scores)\n",
        "    softmax_scores = torch.softmax(scaled_scores, -1)\n",
        "    softmax_scores = dropout(softmax_scores)\n",
        "    printIfVerbose(verbose, \"softmax_scores:\", softmax_scores)\n",
        "    printIfVerbose(verbose, \"softmax_socres shape:\", softmax_scores.shape)\n",
        "    printIfVerbose(verbose, \"values:\", values)\n",
        "    v_out = torch.matmul(softmax_scores, values)\n",
        "    return v_out"
      ],
      "metadata": {
        "id": "DL3t3E_ThGF2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_attention():\n",
        "    d_attn = 4\n",
        "    length_x = 4\n",
        "    length_z = 3\n",
        "    batch_size = 2\n",
        "    d_out = 2\n",
        "\n",
        "    queries = torch.rand(batch_size, length_x, d_attn)\n",
        "    keys = torch.rand(batch_size, length_z, d_attn)\n",
        "    values = torch.rand(batch_size, length_z, d_out)\n",
        "    mask = torch.tril(torch.ones(length_x, length_z) == 1)\n",
        "    padding_mask = torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0]], dtype=torch.int32)\n",
        "\n",
        "    v_out = attention(queries, keys, values, mask, nn.Dropout(0.1), True)\n",
        "    #print(\"output:\", v_out)\n",
        "    assert v_out.shape == (batch_size, length_x, d_out)\n",
        "test_attention()"
      ],
      "metadata": {
        "id": "KhGXLYp6i61z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f10ce257-ce28-48ed-ac4a-d635b67db0e2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "queries: tensor([[[0.4961, 0.6278, 0.3572, 0.5220],\n",
            "         [0.1997, 0.5286, 0.4723, 0.0238],\n",
            "         [0.1838, 0.2010, 0.1765, 0.8587],\n",
            "         [0.7776, 0.1199, 0.8638, 0.1066]],\n",
            "\n",
            "        [[0.1084, 0.8448, 0.7043, 0.9275],\n",
            "         [0.3953, 0.2704, 0.6228, 0.6078],\n",
            "         [0.7686, 0.3296, 0.4959, 0.0065],\n",
            "         [0.9125, 0.8358, 0.6698, 0.4129]]])\n",
            "keys: tensor([[[0.0129, 0.5052, 0.5967, 0.3134],\n",
            "         [0.1648, 0.4834, 0.2368, 0.7654],\n",
            "         [0.9255, 0.3393, 0.5612, 0.0953]],\n",
            "\n",
            "        [[0.5582, 0.5739, 0.5244, 0.6292],\n",
            "         [0.7426, 0.3134, 0.7793, 0.9385],\n",
            "         [0.1588, 0.3427, 0.3863, 0.2306]]])\n",
            "values: tensor([[[0.1533, 0.0876],\n",
            "         [0.9218, 0.8859],\n",
            "         [0.6448, 0.5202]],\n",
            "\n",
            "        [[0.3174, 0.8487],\n",
            "         [0.8658, 0.5804],\n",
            "         [0.1021, 0.1329]]])\n",
            "keys_transposed: tensor([[[0.0129, 0.1648, 0.9255],\n",
            "         [0.5052, 0.4834, 0.3393],\n",
            "         [0.5967, 0.2368, 0.5612],\n",
            "         [0.3134, 0.7654, 0.0953]],\n",
            "\n",
            "        [[0.5582, 0.7426, 0.1588],\n",
            "         [0.5739, 0.3134, 0.3427],\n",
            "         [0.5244, 0.7793, 0.3863],\n",
            "         [0.6292, 0.9385, 0.2306]]])\n",
            "scores: tensor([[[0.7003, 0.8693, 0.9223],\n",
            "         [0.5590, 0.4186, 0.6315],\n",
            "         [0.4784, 0.8265, 0.4192],\n",
            "         [0.6194, 0.4722, 1.2552]],\n",
            "\n",
            "        [[1.4984, 1.7646, 0.7927],\n",
            "         [1.0850, 1.4341, 0.5363],\n",
            "         [0.8825, 1.0667, 0.4282],\n",
            "         [1.6002, 1.8490, 0.7854]]])\n",
            "scores: torch.Size([2, 4, 3])\n",
            "masks: torch.Size([4, 3])\n",
            "masked scores: tensor([[[ 7.0027e-01, -1.0000e+09, -1.0000e+09],\n",
            "         [ 5.5897e-01,  4.1855e-01, -1.0000e+09],\n",
            "         [ 4.7836e-01,  8.2648e-01,  4.1922e-01],\n",
            "         [ 6.1941e-01,  4.7223e-01,  1.2552e+00]],\n",
            "\n",
            "        [[ 1.4984e+00, -1.0000e+09, -1.0000e+09],\n",
            "         [ 1.0850e+00,  1.4341e+00, -1.0000e+09],\n",
            "         [ 8.8246e-01,  1.0667e+00,  4.2816e-01],\n",
            "         [ 1.6002e+00,  1.8490e+00,  7.8537e-01]]])\n",
            "scaled_scores: tensor([[[ 3.5014e-01, -5.0000e+08, -5.0000e+08],\n",
            "         [ 2.7948e-01,  2.0928e-01, -5.0000e+08],\n",
            "         [ 2.3918e-01,  4.1324e-01,  2.0961e-01],\n",
            "         [ 3.0970e-01,  2.3612e-01,  6.2761e-01]],\n",
            "\n",
            "        [[ 7.4918e-01, -5.0000e+08, -5.0000e+08],\n",
            "         [ 5.4248e-01,  7.1705e-01, -5.0000e+08],\n",
            "         [ 4.4123e-01,  5.3335e-01,  2.1408e-01],\n",
            "         [ 8.0008e-01,  9.2452e-01,  3.9269e-01]]])\n",
            "softmax_scores: tensor([[[1.1111, 0.0000, 0.0000],\n",
            "         [0.5751, 0.5361, 0.0000],\n",
            "         [0.3515, 0.4183, 0.3413],\n",
            "         [0.3364, 0.3125, 0.0000]],\n",
            "\n",
            "        [[1.1111, 0.0000, 0.0000],\n",
            "         [0.5072, 0.6039, 0.0000],\n",
            "         [0.0000, 0.4211, 0.3060],\n",
            "         [0.0000, 0.4497, 0.2642]]])\n",
            "softmax_socres shape: torch.Size([2, 4, 3])\n",
            "values: tensor([[[0.1533, 0.0876],\n",
            "         [0.9218, 0.8859],\n",
            "         [0.6448, 0.5202]],\n",
            "\n",
            "        [[0.3174, 0.8487],\n",
            "         [0.8658, 0.5804],\n",
            "         [0.1021, 0.1329]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "class MaskStrategy(Enum):\n",
        "    UNMASKED = 1\n",
        "    MASKED = 2"
      ],
      "metadata": {
        "id": "wUJ-CbaCB9g-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, num_heads, d_attn, d_x, d_z, d_out, d_mid, maskStrategy, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.num_heads = num_heads\n",
        "        self.d_attn = d_attn\n",
        "        self.d_x = d_x\n",
        "        self.d_z = d_z\n",
        "        self.d_out = d_out\n",
        "        self.d_mid = d_mid\n",
        "        self.maskStrategy = maskStrategy\n",
        "        self.weight_query = nn.Parameter(torch.rand(num_heads, d_x, d_attn))\n",
        "        self.weight_key = nn.Parameter(torch.rand(num_heads, d_z, d_attn))\n",
        "        self.weight_value = nn.Parameter(torch.rand(num_heads, d_z, d_mid))\n",
        "        self.weight_out = nn.Parameter(torch.rand(d_mid * num_heads, d_out))\n",
        "        self.bias_query = nn.Parameter(torch.zeros(num_heads, d_attn))\n",
        "        self.bias_key = nn.Parameter(torch.zeros(num_heads, d_attn))\n",
        "        self.bias_value = nn.Parameter(torch.zeros(num_heads, d_mid))\n",
        "        self.bias_out = nn.Parameter(torch.zeros(d_out))\n",
        "        self.dropout = nn.Dropout(p_dropout)\n",
        "\n",
        "    def forward(self, z, x, padding_mask):\n",
        "        length_z = z.shape[-2]\n",
        "        length_x = x.shape[-2]\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        queries = torch.matmul(x.unsqueeze(1), self.weight_query) + self.bias_query[None, :, None, :]\n",
        "        keys = torch.matmul(z.unsqueeze(1), self.weight_key) + self.bias_key[None, :, None, :]\n",
        "        values = torch.matmul(z.unsqueeze(1), self.weight_value) + self.bias_value[None, :, None, :]\n",
        "\n",
        "        assert queries.shape == (batch_size, self.num_heads, length_x, self.d_attn)\n",
        "        assert keys.shape == (batch_size, self.num_heads, length_z, self.d_attn)\n",
        "        assert values.shape == (batch_size, self.num_heads, length_z, self.d_mid)\n",
        "\n",
        "        if self.maskStrategy == MaskStrategy['UNMASKED']:\n",
        "            mask = padding_mask.unsqueeze(-2)\n",
        "        elif self.maskStrategy == MaskStrategy['MASKED']:\n",
        "            padding_mask = padding_mask.unsqueeze(-2)\n",
        "            mask = torch.tril(torch.ones(length_x, length_z) == 1).to(device)\n",
        "            printIfVerbose(self.verbose, \"padding mask:\", padding_mask.shape)\n",
        "            printIfVerbose(self.verbose, \"mask tril\", mask)\n",
        "            mask = mask & padding_mask\n",
        "            printIfVerbose(self.verbose, \"merged mask:\", mask)\n",
        "        mask = mask.unsqueeze(1)\n",
        "        printIfVerbose(self.verbose, \"mask\", mask)\n",
        "        printIfVerbose(self.verbose, \"mask\", mask.shape)\n",
        "        v_out = attention(queries, keys, values, mask, self.dropout, self.verbose)\n",
        "        printIfVerbose(self.verbose, \"v_out shape\", v_out.shape)\n",
        "        assert v_out.shape == (batch_size, self.num_heads, length_x, self.d_mid)\n",
        "        printIfVerbose(self.verbose, \"v_out:\", v_out)\n",
        "        printIfVerbose(self.verbose, \"v_out shape before:\", v_out.shape)\n",
        "        v_out = v_out.reshape(batch_size, v_out.shape[-2], -1)\n",
        "        printIfVerbose(self.verbose, \"v_out shape:\", v_out.shape)\n",
        "        printIfVerbose(self.verbose, \"weight_out shape:\", self.weight_out.shape)\n",
        "        printIfVerbose(self.verbose, \"v_out reshaped:\", v_out)\n",
        "        output = torch.matmul(v_out, self.weight_out) + self.bias_out\n",
        "        printIfVerbose(self.verbose, \"output shape\", output.shape)\n",
        "        assert output.shape == (batch_size, length_x, self.d_out)\n",
        "        return output\n",
        "\n",
        "    def disable_subsequent_mask(self):\n",
        "        self.maskStrategy = MaskStrategy['UNMASKED']\n",
        "\n",
        "    def enable_subsequent_mask(self):\n",
        "        self.maskStrategy = MaskStrategy['MASKED']\n"
      ],
      "metadata": {
        "id": "CX2A1i9Z4lVO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_headed_attention_encoder_fixed():\n",
        "    num_heads = 1\n",
        "    d_attn = 3\n",
        "    d_x = 4\n",
        "    d_z = 4\n",
        "    d_out = 1\n",
        "    d_mid = 3\n",
        "    length_z = 3\n",
        "    batch_size = 1\n",
        "    padding_mask = torch.tensor([[1, 1, 0]], dtype=torch.int32).to(device)\n",
        "\n",
        "    multi_headed_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], 0.1, True).to(device)\n",
        "    multi_headed_attention.weight_query = nn.Parameter(torch.tensor([[[1, 0, 1], [1, 0, 0], [0, 0, 1], [0, 1, 1]]], dtype = torch.float32).to(device))\n",
        "    multi_headed_attention.weight_key = nn.Parameter(torch.tensor([[[0, 0, 1], [1, 1, 0], [0, 1, 0], [1, 1, 0]]], dtype = torch.float32).to(device))\n",
        "    multi_headed_attention.weight_value = nn.Parameter(torch.tensor([[[0, 2, 0], [0, 3, 0], [1, 0, 3], [1, 1, 0]]], dtype = torch.float32).to(device))\n",
        "    multi_headed_attention.weight_out = nn.Parameter(torch.tensor([[1], [0], [1]], dtype = torch.float32).to(device))\n",
        "    z = torch.tensor([[[1, 0, 1, 0], [0, 2, 0, 2], [1, 1, 1, 1]]], dtype=torch.float32).to(device)\n",
        "    #print(\"z:\", z\n",
        "    output = multi_headed_attention(z, z, padding_mask)\n",
        "    #print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, length_z, d_out)\n",
        "test_multi_headed_attention_encoder_fixed()"
      ],
      "metadata": {
        "id": "kJuRy-3dTMY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c0d306a-b44e-4ab8-b0c5-023d7181ae94"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mask tensor([[[[1, 1, 0]]]], device='cuda:0', dtype=torch.int32)\n",
            "mask torch.Size([1, 1, 1, 3])\n",
            "queries: tensor([[[[1., 0., 2.],\n",
            "          [2., 2., 2.],\n",
            "          [2., 1., 3.]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "keys: tensor([[[[0., 1., 1.],\n",
            "          [4., 4., 0.],\n",
            "          [2., 3., 1.]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "values: tensor([[[[1., 2., 3.],\n",
            "          [2., 8., 0.],\n",
            "          [2., 6., 3.]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "keys_transposed: tensor([[[[0., 4., 2.],\n",
            "          [1., 4., 3.],\n",
            "          [1., 0., 1.]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "scores: tensor([[[[ 2.,  4.,  4.],\n",
            "          [ 4., 16., 12.],\n",
            "          [ 4., 12., 10.]]]], device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
            "scores: torch.Size([1, 1, 3, 3])\n",
            "masks: torch.Size([1, 1, 1, 3])\n",
            "masked scores: tensor([[[[ 2.0000e+00,  4.0000e+00, -1.0000e+09],\n",
            "          [ 4.0000e+00,  1.6000e+01, -1.0000e+09],\n",
            "          [ 4.0000e+00,  1.2000e+01, -1.0000e+09]]]], device='cuda:0',\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "scaled_scores: tensor([[[[ 1.1547e+00,  2.3094e+00, -5.7735e+08],\n",
            "          [ 2.3094e+00,  9.2376e+00, -5.7735e+08],\n",
            "          [ 2.3094e+00,  6.9282e+00, -5.7735e+08]]]], device='cuda:0',\n",
            "       grad_fn=<DivBackward0>)\n",
            "softmax_scores: tensor([[[[2.6626e-01, 0.0000e+00, 0.0000e+00],\n",
            "          [1.0876e-03, 1.1100e+00, 0.0000e+00],\n",
            "          [1.0854e-02, 1.1003e+00, 0.0000e+00]]]], device='cuda:0',\n",
            "       grad_fn=<NativeDropoutBackward0>)\n",
            "softmax_socres shape: torch.Size([1, 1, 3, 3])\n",
            "values: tensor([[[[1., 2., 3.],\n",
            "          [2., 8., 0.],\n",
            "          [2., 6., 3.]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "v_out shape torch.Size([1, 1, 3, 3])\n",
            "v_out: tensor([[[[2.6626e-01, 5.3251e-01, 7.9877e-01],\n",
            "          [2.2211e+00, 8.8824e+00, 3.2627e-03],\n",
            "          [2.2114e+00, 8.8238e+00, 3.2561e-02]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "v_out shape before: torch.Size([1, 1, 3, 3])\n",
            "v_out shape: torch.Size([1, 3, 3])\n",
            "weight_out shape: torch.Size([3, 1])\n",
            "v_out reshaped: tensor([[[2.6626e-01, 5.3251e-01, 7.9877e-01],\n",
            "         [2.2211e+00, 8.8824e+00, 3.2627e-03],\n",
            "         [2.2114e+00, 8.8238e+00, 3.2561e-02]]], device='cuda:0',\n",
            "       grad_fn=<ViewBackward0>)\n",
            "output shape torch.Size([1, 3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_headed_attention_encoder():\n",
        "    num_heads = 3\n",
        "    d_attn = 3\n",
        "    d_x = 4\n",
        "    d_z = 4\n",
        "    d_out = 1\n",
        "    d_mid = 3\n",
        "    length_z = 3\n",
        "    batch_size = 3\n",
        "    padding_mask = torch.tensor([[1, 1, 0], [1, 1, 0], [1, 1, 1]], dtype=torch.int32).to(device)\n",
        "\n",
        "    multi_headed_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], 0.1, True).to(device)\n",
        "    z = torch.tensor([[[1, 0, 1, 0], [0, 2, 0, 2], [1, 1, 1, 1]],\n",
        "                      [[1, 0, 1, 0], [0, 2, 0, 2], [1, 1, 1, 1]],\n",
        "                      [[1, 0, 1, 0], [0, 2, 0, 2], [1, 1, 1, 1]]], dtype=torch.float32).to(device)\n",
        "    #print(\"z:\", z\n",
        "    output = multi_headed_attention(z, z, padding_mask)\n",
        "    #print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, length_z, d_out)\n",
        "test_multi_headed_attention_encoder()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9CYHYJbMObt",
        "outputId": "59d74d67-53f6-4a84-a3e8-b75155c1141c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mask tensor([[[[1, 1, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 1]]]], device='cuda:0', dtype=torch.int32)\n",
            "mask torch.Size([3, 1, 1, 3])\n",
            "queries: tensor([[[[1.7927, 1.6278, 0.9480],\n",
            "          [2.1136, 2.5402, 0.2631],\n",
            "          [2.8494, 2.8979, 1.0795]],\n",
            "\n",
            "         [[1.0601, 1.6175, 0.5138],\n",
            "          [0.9931, 2.5237, 0.7244],\n",
            "          [1.5567, 2.8793, 0.8760]],\n",
            "\n",
            "         [[0.6222, 0.7464, 0.5499],\n",
            "          [2.0090, 2.2731, 1.6859],\n",
            "          [1.6268, 1.8830, 1.3928]]],\n",
            "\n",
            "\n",
            "        [[[1.7927, 1.6278, 0.9480],\n",
            "          [2.1136, 2.5402, 0.2631],\n",
            "          [2.8494, 2.8979, 1.0795]],\n",
            "\n",
            "         [[1.0601, 1.6175, 0.5138],\n",
            "          [0.9931, 2.5237, 0.7244],\n",
            "          [1.5567, 2.8793, 0.8760]],\n",
            "\n",
            "         [[0.6222, 0.7464, 0.5499],\n",
            "          [2.0090, 2.2731, 1.6859],\n",
            "          [1.6268, 1.8830, 1.3928]]],\n",
            "\n",
            "\n",
            "        [[[1.7927, 1.6278, 0.9480],\n",
            "          [2.1136, 2.5402, 0.2631],\n",
            "          [2.8494, 2.8979, 1.0795]],\n",
            "\n",
            "         [[1.0601, 1.6175, 0.5138],\n",
            "          [0.9931, 2.5237, 0.7244],\n",
            "          [1.5567, 2.8793, 0.8760]],\n",
            "\n",
            "         [[0.6222, 0.7464, 0.5499],\n",
            "          [2.0090, 2.2731, 1.6859],\n",
            "          [1.6268, 1.8830, 1.3928]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "keys: tensor([[[[0.7384, 0.6150, 0.7196],\n",
            "          [2.3034, 2.2820, 2.6755],\n",
            "          [1.8901, 1.7560, 2.0574]],\n",
            "\n",
            "         [[1.4544, 1.2114, 1.1873],\n",
            "          [2.1187, 2.2619, 2.4142],\n",
            "          [2.5138, 2.3423, 2.3944]],\n",
            "\n",
            "         [[0.9823, 0.5387, 1.2198],\n",
            "          [1.9080, 1.4184, 1.3380],\n",
            "          [1.9363, 1.2479, 1.8888]]],\n",
            "\n",
            "\n",
            "        [[[0.7384, 0.6150, 0.7196],\n",
            "          [2.3034, 2.2820, 2.6755],\n",
            "          [1.8901, 1.7560, 2.0574]],\n",
            "\n",
            "         [[1.4544, 1.2114, 1.1873],\n",
            "          [2.1187, 2.2619, 2.4142],\n",
            "          [2.5138, 2.3423, 2.3944]],\n",
            "\n",
            "         [[0.9823, 0.5387, 1.2198],\n",
            "          [1.9080, 1.4184, 1.3380],\n",
            "          [1.9363, 1.2479, 1.8888]]],\n",
            "\n",
            "\n",
            "        [[[0.7384, 0.6150, 0.7196],\n",
            "          [2.3034, 2.2820, 2.6755],\n",
            "          [1.8901, 1.7560, 2.0574]],\n",
            "\n",
            "         [[1.4544, 1.2114, 1.1873],\n",
            "          [2.1187, 2.2619, 2.4142],\n",
            "          [2.5138, 2.3423, 2.3944]],\n",
            "\n",
            "         [[0.9823, 0.5387, 1.2198],\n",
            "          [1.9080, 1.4184, 1.3380],\n",
            "          [1.9363, 1.2479, 1.8888]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "values: tensor([[[[0.9785, 1.0904, 0.6010],\n",
            "          [1.0702, 2.1112, 2.2211],\n",
            "          [1.5136, 2.1459, 1.7116]],\n",
            "\n",
            "         [[1.8108, 0.6690, 1.3338],\n",
            "          [1.2587, 1.8673, 2.0773],\n",
            "          [2.4402, 1.6027, 2.3725]],\n",
            "\n",
            "         [[1.1319, 0.4028, 1.7046],\n",
            "          [1.9398, 3.3149, 1.3146],\n",
            "          [2.1018, 2.0603, 2.3619]]],\n",
            "\n",
            "\n",
            "        [[[0.9785, 1.0904, 0.6010],\n",
            "          [1.0702, 2.1112, 2.2211],\n",
            "          [1.5136, 2.1459, 1.7116]],\n",
            "\n",
            "         [[1.8108, 0.6690, 1.3338],\n",
            "          [1.2587, 1.8673, 2.0773],\n",
            "          [2.4402, 1.6027, 2.3725]],\n",
            "\n",
            "         [[1.1319, 0.4028, 1.7046],\n",
            "          [1.9398, 3.3149, 1.3146],\n",
            "          [2.1018, 2.0603, 2.3619]]],\n",
            "\n",
            "\n",
            "        [[[0.9785, 1.0904, 0.6010],\n",
            "          [1.0702, 2.1112, 2.2211],\n",
            "          [1.5136, 2.1459, 1.7116]],\n",
            "\n",
            "         [[1.8108, 0.6690, 1.3338],\n",
            "          [1.2587, 1.8673, 2.0773],\n",
            "          [2.4402, 1.6027, 2.3725]],\n",
            "\n",
            "         [[1.1319, 0.4028, 1.7046],\n",
            "          [1.9398, 3.3149, 1.3146],\n",
            "          [2.1018, 2.0603, 2.3619]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "keys_transposed: tensor([[[[0.7384, 2.3034, 1.8901],\n",
            "          [0.6150, 2.2820, 1.7560],\n",
            "          [0.7196, 2.6755, 2.0574]],\n",
            "\n",
            "         [[1.4544, 2.1187, 2.5138],\n",
            "          [1.2114, 2.2619, 2.3423],\n",
            "          [1.1873, 2.4142, 2.3944]],\n",
            "\n",
            "         [[0.9823, 1.9080, 1.9363],\n",
            "          [0.5387, 1.4184, 1.2479],\n",
            "          [1.2198, 1.3380, 1.8888]]],\n",
            "\n",
            "\n",
            "        [[[0.7384, 2.3034, 1.8901],\n",
            "          [0.6150, 2.2820, 1.7560],\n",
            "          [0.7196, 2.6755, 2.0574]],\n",
            "\n",
            "         [[1.4544, 2.1187, 2.5138],\n",
            "          [1.2114, 2.2619, 2.3423],\n",
            "          [1.1873, 2.4142, 2.3944]],\n",
            "\n",
            "         [[0.9823, 1.9080, 1.9363],\n",
            "          [0.5387, 1.4184, 1.2479],\n",
            "          [1.2198, 1.3380, 1.8888]]],\n",
            "\n",
            "\n",
            "        [[[0.7384, 2.3034, 1.8901],\n",
            "          [0.6150, 2.2820, 1.7560],\n",
            "          [0.7196, 2.6755, 2.0574]],\n",
            "\n",
            "         [[1.4544, 2.1187, 2.5138],\n",
            "          [1.2114, 2.2619, 2.3423],\n",
            "          [1.1873, 2.4142, 2.3944]],\n",
            "\n",
            "         [[0.9823, 1.9080, 1.9363],\n",
            "          [0.5387, 1.4184, 1.2479],\n",
            "          [1.2198, 1.3380, 1.8888]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "scores: tensor([[[[ 3.0071, 10.3802,  8.1971],\n",
            "          [ 3.3123, 11.3691,  8.9969],\n",
            "          [ 4.6632, 16.0647, 12.6956]],\n",
            "\n",
            "         [[ 4.1113,  7.1449,  7.6838],\n",
            "          [ 5.3617,  9.5612, 10.1423],\n",
            "          [ 6.7922, 11.9255, 12.7549]],\n",
            "\n",
            "         [[ 1.6841,  2.9817,  3.1750],\n",
            "          [ 5.2545,  9.3131,  9.9111],\n",
            "          [ 4.3113,  7.6383,  8.1305]]],\n",
            "\n",
            "\n",
            "        [[[ 3.0071, 10.3802,  8.1971],\n",
            "          [ 3.3123, 11.3691,  8.9969],\n",
            "          [ 4.6632, 16.0647, 12.6956]],\n",
            "\n",
            "         [[ 4.1113,  7.1449,  7.6838],\n",
            "          [ 5.3617,  9.5612, 10.1423],\n",
            "          [ 6.7922, 11.9255, 12.7549]],\n",
            "\n",
            "         [[ 1.6841,  2.9817,  3.1750],\n",
            "          [ 5.2545,  9.3131,  9.9111],\n",
            "          [ 4.3113,  7.6383,  8.1305]]],\n",
            "\n",
            "\n",
            "        [[[ 3.0071, 10.3802,  8.1971],\n",
            "          [ 3.3123, 11.3691,  8.9969],\n",
            "          [ 4.6632, 16.0647, 12.6956]],\n",
            "\n",
            "         [[ 4.1113,  7.1449,  7.6838],\n",
            "          [ 5.3617,  9.5612, 10.1423],\n",
            "          [ 6.7922, 11.9255, 12.7549]],\n",
            "\n",
            "         [[ 1.6841,  2.9817,  3.1750],\n",
            "          [ 5.2545,  9.3131,  9.9111],\n",
            "          [ 4.3113,  7.6383,  8.1305]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "scores: torch.Size([3, 3, 3, 3])\n",
            "masks: torch.Size([3, 1, 1, 3])\n",
            "masked scores: tensor([[[[ 3.0071e+00,  1.0380e+01, -1.0000e+09],\n",
            "          [ 3.3123e+00,  1.1369e+01, -1.0000e+09],\n",
            "          [ 4.6632e+00,  1.6065e+01, -1.0000e+09]],\n",
            "\n",
            "         [[ 4.1113e+00,  7.1449e+00, -1.0000e+09],\n",
            "          [ 5.3617e+00,  9.5612e+00, -1.0000e+09],\n",
            "          [ 6.7922e+00,  1.1926e+01, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.6841e+00,  2.9817e+00, -1.0000e+09],\n",
            "          [ 5.2545e+00,  9.3131e+00, -1.0000e+09],\n",
            "          [ 4.3113e+00,  7.6383e+00, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 3.0071e+00,  1.0380e+01, -1.0000e+09],\n",
            "          [ 3.3123e+00,  1.1369e+01, -1.0000e+09],\n",
            "          [ 4.6632e+00,  1.6065e+01, -1.0000e+09]],\n",
            "\n",
            "         [[ 4.1113e+00,  7.1449e+00, -1.0000e+09],\n",
            "          [ 5.3617e+00,  9.5612e+00, -1.0000e+09],\n",
            "          [ 6.7922e+00,  1.1926e+01, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.6841e+00,  2.9817e+00, -1.0000e+09],\n",
            "          [ 5.2545e+00,  9.3131e+00, -1.0000e+09],\n",
            "          [ 4.3113e+00,  7.6383e+00, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 3.0071e+00,  1.0380e+01,  8.1971e+00],\n",
            "          [ 3.3123e+00,  1.1369e+01,  8.9969e+00],\n",
            "          [ 4.6632e+00,  1.6065e+01,  1.2696e+01]],\n",
            "\n",
            "         [[ 4.1113e+00,  7.1449e+00,  7.6838e+00],\n",
            "          [ 5.3617e+00,  9.5612e+00,  1.0142e+01],\n",
            "          [ 6.7922e+00,  1.1926e+01,  1.2755e+01]],\n",
            "\n",
            "         [[ 1.6841e+00,  2.9817e+00,  3.1750e+00],\n",
            "          [ 5.2545e+00,  9.3131e+00,  9.9111e+00],\n",
            "          [ 4.3113e+00,  7.6383e+00,  8.1305e+00]]]], device='cuda:0',\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "scaled_scores: tensor([[[[ 1.7361e+00,  5.9930e+00, -5.7735e+08],\n",
            "          [ 1.9124e+00,  6.5639e+00, -5.7735e+08],\n",
            "          [ 2.6923e+00,  9.2750e+00, -5.7735e+08]],\n",
            "\n",
            "         [[ 2.3737e+00,  4.1251e+00, -5.7735e+08],\n",
            "          [ 3.0956e+00,  5.5202e+00, -5.7735e+08],\n",
            "          [ 3.9215e+00,  6.8852e+00, -5.7735e+08]],\n",
            "\n",
            "         [[ 9.7231e-01,  1.7215e+00, -5.7735e+08],\n",
            "          [ 3.0337e+00,  5.3769e+00, -5.7735e+08],\n",
            "          [ 2.4892e+00,  4.4100e+00, -5.7735e+08]]],\n",
            "\n",
            "\n",
            "        [[[ 1.7361e+00,  5.9930e+00, -5.7735e+08],\n",
            "          [ 1.9124e+00,  6.5639e+00, -5.7735e+08],\n",
            "          [ 2.6923e+00,  9.2750e+00, -5.7735e+08]],\n",
            "\n",
            "         [[ 2.3737e+00,  4.1251e+00, -5.7735e+08],\n",
            "          [ 3.0956e+00,  5.5202e+00, -5.7735e+08],\n",
            "          [ 3.9215e+00,  6.8852e+00, -5.7735e+08]],\n",
            "\n",
            "         [[ 9.7231e-01,  1.7215e+00, -5.7735e+08],\n",
            "          [ 3.0337e+00,  5.3769e+00, -5.7735e+08],\n",
            "          [ 2.4892e+00,  4.4100e+00, -5.7735e+08]]],\n",
            "\n",
            "\n",
            "        [[[ 1.7361e+00,  5.9930e+00,  4.7326e+00],\n",
            "          [ 1.9124e+00,  6.5639e+00,  5.1944e+00],\n",
            "          [ 2.6923e+00,  9.2750e+00,  7.3298e+00]],\n",
            "\n",
            "         [[ 2.3737e+00,  4.1251e+00,  4.4362e+00],\n",
            "          [ 3.0956e+00,  5.5202e+00,  5.8557e+00],\n",
            "          [ 3.9215e+00,  6.8852e+00,  7.3641e+00]],\n",
            "\n",
            "         [[ 9.7231e-01,  1.7215e+00,  1.8331e+00],\n",
            "          [ 3.0337e+00,  5.3769e+00,  5.7222e+00],\n",
            "          [ 2.4892e+00,  4.4100e+00,  4.6941e+00]]]], device='cuda:0',\n",
            "       grad_fn=<DivBackward0>)\n",
            "softmax_scores: tensor([[[[0.0155, 1.0956, 0.0000],\n",
            "          [0.0105, 1.1006, 0.0000],\n",
            "          [0.0015, 1.1096, 0.0000]],\n",
            "\n",
            "         [[0.1643, 0.9468, 0.0000],\n",
            "          [0.0904, 1.0208, 0.0000],\n",
            "          [0.0545, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.3567, 0.0000, 0.0000],\n",
            "          [0.0973, 1.0138, 0.0000],\n",
            "          [0.1420, 0.9691, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.0155, 1.0956, 0.0000],\n",
            "          [0.0000, 1.1006, 0.0000],\n",
            "          [0.0015, 1.1096, 0.0000]],\n",
            "\n",
            "         [[0.1643, 0.9468, 0.0000],\n",
            "          [0.0904, 1.0208, 0.0000],\n",
            "          [0.0545, 1.0566, 0.0000]],\n",
            "\n",
            "         [[0.3567, 0.7544, 0.0000],\n",
            "          [0.0973, 1.0138, 0.0000],\n",
            "          [0.1420, 0.9691, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.0000, 0.8562, 0.2428],\n",
            "          [0.0084, 0.8792, 0.2235],\n",
            "          [0.0013, 0.9710, 0.1388]],\n",
            "\n",
            "         [[0.0760, 0.4377, 0.5974],\n",
            "          [0.0395, 0.4467, 0.6248],\n",
            "          [0.0215, 0.4168, 0.6728]],\n",
            "\n",
            "         [[0.2028, 0.4289, 0.4795],\n",
            "          [0.0425, 0.4430, 0.6256],\n",
            "          [0.0658, 0.4489, 0.5964]]]], device='cuda:0',\n",
            "       grad_fn=<NativeDropoutBackward0>)\n",
            "softmax_socres shape: torch.Size([3, 3, 3, 3])\n",
            "values: tensor([[[[0.9785, 1.0904, 0.6010],\n",
            "          [1.0702, 2.1112, 2.2211],\n",
            "          [1.5136, 2.1459, 1.7116]],\n",
            "\n",
            "         [[1.8108, 0.6690, 1.3338],\n",
            "          [1.2587, 1.8673, 2.0773],\n",
            "          [2.4402, 1.6027, 2.3725]],\n",
            "\n",
            "         [[1.1319, 0.4028, 1.7046],\n",
            "          [1.9398, 3.3149, 1.3146],\n",
            "          [2.1018, 2.0603, 2.3619]]],\n",
            "\n",
            "\n",
            "        [[[0.9785, 1.0904, 0.6010],\n",
            "          [1.0702, 2.1112, 2.2211],\n",
            "          [1.5136, 2.1459, 1.7116]],\n",
            "\n",
            "         [[1.8108, 0.6690, 1.3338],\n",
            "          [1.2587, 1.8673, 2.0773],\n",
            "          [2.4402, 1.6027, 2.3725]],\n",
            "\n",
            "         [[1.1319, 0.4028, 1.7046],\n",
            "          [1.9398, 3.3149, 1.3146],\n",
            "          [2.1018, 2.0603, 2.3619]]],\n",
            "\n",
            "\n",
            "        [[[0.9785, 1.0904, 0.6010],\n",
            "          [1.0702, 2.1112, 2.2211],\n",
            "          [1.5136, 2.1459, 1.7116]],\n",
            "\n",
            "         [[1.8108, 0.6690, 1.3338],\n",
            "          [1.2587, 1.8673, 2.0773],\n",
            "          [2.4402, 1.6027, 2.3725]],\n",
            "\n",
            "         [[1.1319, 0.4028, 1.7046],\n",
            "          [1.9398, 3.3149, 1.3146],\n",
            "          [2.1018, 2.0603, 2.3619]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "v_out shape torch.Size([3, 3, 3, 3])\n",
            "v_out: tensor([[[[1.1876, 2.3299, 2.4428],\n",
            "          [1.1881, 2.3350, 2.4509],\n",
            "          [1.1889, 2.3442, 2.4655]],\n",
            "\n",
            "         [[1.4893, 1.8779, 2.1860],\n",
            "          [1.4485, 1.9665, 2.2409],\n",
            "          [0.0988, 0.0365, 0.0728]],\n",
            "\n",
            "         [[0.4037, 0.1437, 0.6080],\n",
            "          [2.0767, 3.3998, 1.4986],\n",
            "          [2.0407, 3.2698, 1.5160]]],\n",
            "\n",
            "\n",
            "        [[[1.1876, 2.3299, 2.4428],\n",
            "          [1.1778, 2.3236, 2.4446],\n",
            "          [1.1889, 2.3442, 2.4655]],\n",
            "\n",
            "         [[1.4893, 1.8779, 2.1860],\n",
            "          [1.4485, 1.9665, 2.2409],\n",
            "          [1.4287, 2.0094, 2.2675]],\n",
            "\n",
            "         [[1.8672, 2.6446, 1.5997],\n",
            "          [2.0767, 3.3998, 1.4986],\n",
            "          [2.0407, 3.2698, 1.5160]]],\n",
            "\n",
            "\n",
            "        [[[1.2837, 2.3286, 2.3173],\n",
            "          [1.2874, 2.3449, 2.3405],\n",
            "          [1.2505, 2.3492, 2.3950]],\n",
            "\n",
            "         [[2.1464, 1.8257, 2.4280],\n",
            "          [2.1587, 1.8621, 2.4631],\n",
            "          [2.2054, 1.8710, 2.4907]],\n",
            "\n",
            "         [[2.0692, 2.4912, 2.0419],\n",
            "          [2.2223, 2.7745, 2.1324],\n",
            "          [2.1988, 2.7434, 2.1110]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "v_out shape before: torch.Size([3, 3, 3, 3])\n",
            "v_out shape: torch.Size([3, 3, 9])\n",
            "weight_out shape: torch.Size([9, 1])\n",
            "v_out reshaped: tensor([[[1.1876, 2.3299, 2.4428, 1.1881, 2.3350, 2.4509, 1.1889, 2.3442,\n",
            "          2.4655],\n",
            "         [1.4893, 1.8779, 2.1860, 1.4485, 1.9665, 2.2409, 0.0988, 0.0365,\n",
            "          0.0728],\n",
            "         [0.4037, 0.1437, 0.6080, 2.0767, 3.3998, 1.4986, 2.0407, 3.2698,\n",
            "          1.5160]],\n",
            "\n",
            "        [[1.1876, 2.3299, 2.4428, 1.1778, 2.3236, 2.4446, 1.1889, 2.3442,\n",
            "          2.4655],\n",
            "         [1.4893, 1.8779, 2.1860, 1.4485, 1.9665, 2.2409, 1.4287, 2.0094,\n",
            "          2.2675],\n",
            "         [1.8672, 2.6446, 1.5997, 2.0767, 3.3998, 1.4986, 2.0407, 3.2698,\n",
            "          1.5160]],\n",
            "\n",
            "        [[1.2837, 2.3286, 2.3173, 1.2874, 2.3449, 2.3405, 1.2505, 2.3492,\n",
            "          2.3950],\n",
            "         [2.1464, 1.8257, 2.4280, 2.1587, 1.8621, 2.4631, 2.2054, 1.8710,\n",
            "          2.4907],\n",
            "         [2.0692, 2.4912, 2.0419, 2.2223, 2.7745, 2.1324, 2.1988, 2.7434,\n",
            "          2.1110]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "output shape torch.Size([3, 3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_headed_attention_encoder_decoder():\n",
        "    num_heads = 1\n",
        "    d_attn = 4\n",
        "    d_x = 1\n",
        "    d_z = 1\n",
        "    d_out = 1\n",
        "    d_mid = 1\n",
        "    length_x = 3\n",
        "    length_z = 3\n",
        "    batch_size = 1\n",
        "    padding_mask = torch.tensor([[1, 1, 0]], dtype=torch.int32).to(device)\n",
        "\n",
        "    multi_headed_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], 0.1, True).to(device)\n",
        "    x = torch.rand(batch_size, length_x, d_x).to(device)\n",
        "    z = torch.rand(batch_size, length_z, d_z).to(device)\n",
        "    output = multi_headed_attention(z, x, padding_mask)\n",
        "    print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, length_x, d_out)\n",
        "test_multi_headed_attention_encoder_decoder()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbYma85ffEHr",
        "outputId": "a76c51ee-5e49-4726-e017-954af8d319c7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mask tensor([[[[1, 1, 0]]]], device='cuda:0', dtype=torch.int32)\n",
            "mask torch.Size([1, 1, 1, 3])\n",
            "queries: tensor([[[[0.1054, 0.0944, 0.3552, 0.3840],\n",
            "          [0.1248, 0.1118, 0.4207, 0.4548],\n",
            "          [0.1795, 0.1608, 0.6050, 0.6541]]]], device='cuda:0',\n",
            "       grad_fn=<AddBackward0>)\n",
            "keys: tensor([[[[0.1384, 0.1812, 0.1878, 0.1922],\n",
            "          [0.0376, 0.0492, 0.0510, 0.0522],\n",
            "          [0.3202, 0.4191, 0.4344, 0.4447]]]], device='cuda:0',\n",
            "       grad_fn=<AddBackward0>)\n",
            "values: tensor([[[[0.0233],\n",
            "          [0.0063],\n",
            "          [0.0539]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "keys_transposed: tensor([[[[0.1384, 0.0376, 0.3202],\n",
            "          [0.1812, 0.0492, 0.4191],\n",
            "          [0.1878, 0.0510, 0.4344],\n",
            "          [0.1922, 0.0522, 0.4447]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "scores: tensor([[[[0.1722, 0.0468, 0.3983],\n",
            "          [0.2039, 0.0554, 0.4718],\n",
            "          [0.2933, 0.0797, 0.6785]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "scores: torch.Size([1, 1, 3, 3])\n",
            "masks: torch.Size([1, 1, 1, 3])\n",
            "masked scores: tensor([[[[ 1.7220e-01,  4.6780e-02, -1.0000e+09],\n",
            "          [ 2.0395e-01,  5.5405e-02, -1.0000e+09],\n",
            "          [ 2.9331e-01,  7.9683e-02, -1.0000e+09]]]], device='cuda:0',\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "scaled_scores: tensor([[[[ 8.6099e-02,  2.3390e-02, -5.0000e+08],\n",
            "          [ 1.0197e-01,  2.7702e-02, -5.0000e+08],\n",
            "          [ 1.4666e-01,  3.9841e-02, -5.0000e+08]]]], device='cuda:0',\n",
            "       grad_fn=<DivBackward0>)\n",
            "softmax_scores: tensor([[[[0.5730, 0.5381, 0.0000],\n",
            "          [0.5762, 0.5349, 0.0000],\n",
            "          [0.5852, 0.5259, 0.0000]]]], device='cuda:0',\n",
            "       grad_fn=<NativeDropoutBackward0>)\n",
            "softmax_socres shape: torch.Size([1, 1, 3, 3])\n",
            "values: tensor([[[[0.0233],\n",
            "          [0.0063],\n",
            "          [0.0539]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "v_out shape torch.Size([1, 1, 3, 1])\n",
            "v_out: tensor([[[[0.0167],\n",
            "          [0.0168],\n",
            "          [0.0170]]]], device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
            "v_out shape before: torch.Size([1, 1, 3, 1])\n",
            "v_out shape: torch.Size([1, 3, 1])\n",
            "weight_out shape: torch.Size([1, 1])\n",
            "v_out reshaped: tensor([[[0.0167],\n",
            "         [0.0168],\n",
            "         [0.0170]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "output shape torch.Size([1, 3, 1])\n",
            "output: tensor([[[0.0164],\n",
            "         [0.0164],\n",
            "         [0.0166]]], device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_headed_attention_decoder_self():\n",
        "    num_heads = 8\n",
        "    d_attn = 4\n",
        "    d_x = 4\n",
        "    d_out = 4\n",
        "    d_mid = 2\n",
        "    length_x = 3\n",
        "    batch_size = 4\n",
        "    padding_mask = torch.tensor([[1, 1, 0], [1, 1, 0], [1, 0, 0], [1, 1, 1]], dtype=torch.int32).to(device)\n",
        "\n",
        "    multi_headed_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_x, d_out, d_mid, MaskStrategy['UNMASKED'], 0.1, True).to(device)\n",
        "    multi_headed_attention.enable_subsequent_mask()\n",
        "    x = torch.rand(batch_size, length_x, d_x).to(device)\n",
        "    output = multi_headed_attention(x, x, padding_mask)\n",
        "    print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, length_x, d_out)\n",
        "test_multi_headed_attention_decoder_self()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vn9s5ak_Yr1B",
        "outputId": "4766ad09-443a-4323-fc9a-6733b52c4a24"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "padding mask: torch.Size([4, 1, 3])\n",
            "mask tril tensor([[ True, False, False],\n",
            "        [ True,  True, False],\n",
            "        [ True,  True,  True]], device='cuda:0')\n",
            "merged mask: tensor([[[1, 0, 0],\n",
            "         [1, 1, 0],\n",
            "         [1, 1, 0]],\n",
            "\n",
            "        [[1, 0, 0],\n",
            "         [1, 1, 0],\n",
            "         [1, 1, 0]],\n",
            "\n",
            "        [[1, 0, 0],\n",
            "         [1, 0, 0],\n",
            "         [1, 0, 0]],\n",
            "\n",
            "        [[1, 0, 0],\n",
            "         [1, 1, 0],\n",
            "         [1, 1, 1]]], device='cuda:0', dtype=torch.int32)\n",
            "mask tensor([[[[1, 0, 0],\n",
            "          [1, 1, 0],\n",
            "          [1, 1, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 0, 0],\n",
            "          [1, 1, 0],\n",
            "          [1, 1, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 0, 0],\n",
            "          [1, 0, 0],\n",
            "          [1, 0, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 0, 0],\n",
            "          [1, 1, 0],\n",
            "          [1, 1, 1]]]], device='cuda:0', dtype=torch.int32)\n",
            "mask torch.Size([4, 1, 3, 3])\n",
            "queries: tensor([[[[0.7571, 0.8545, 0.8454, 0.7041],\n",
            "          [0.8567, 1.2186, 1.1153, 0.9453],\n",
            "          [1.5053, 1.9624, 1.9137, 0.8709]],\n",
            "\n",
            "         [[0.4776, 0.7881, 0.7576, 0.6827],\n",
            "          [0.7333, 0.9648, 1.1086, 1.0104],\n",
            "          [0.8271, 1.5986, 0.8418, 1.1153]],\n",
            "\n",
            "         [[1.0365, 0.4125, 1.0202, 0.8579],\n",
            "          [1.4189, 0.5324, 1.5862, 1.2243],\n",
            "          [1.2696, 0.8943, 1.2796, 1.4341]],\n",
            "\n",
            "         [[0.7561, 0.9143, 0.6717, 0.8266],\n",
            "          [0.9375, 1.1759, 1.1569, 0.9809],\n",
            "          [1.1402, 1.2233, 1.1121, 1.3426]],\n",
            "\n",
            "         [[0.4207, 0.2265, 0.9630, 0.5541],\n",
            "          [0.4779, 0.3635, 1.5585, 1.0554],\n",
            "          [0.9522, 0.3697, 1.1704, 0.3645]],\n",
            "\n",
            "         [[0.8763, 0.7020, 0.4713, 0.5887],\n",
            "          [1.3156, 0.8121, 0.4875, 0.7379],\n",
            "          [1.2281, 0.9775, 1.3011, 1.5724]],\n",
            "\n",
            "         [[1.1914, 0.7184, 1.2270, 0.9703],\n",
            "          [1.5773, 0.8564, 1.7085, 1.1907],\n",
            "          [2.0473, 1.3522, 1.4469, 1.4815]],\n",
            "\n",
            "         [[0.8107, 0.7912, 0.7428, 0.8412],\n",
            "          [1.1423, 1.0311, 1.1277, 1.3325],\n",
            "          [0.9348, 1.1171, 1.1676, 0.8974]]],\n",
            "\n",
            "\n",
            "        [[[1.1317, 1.6059, 1.5004, 1.0389],\n",
            "          [1.1520, 1.4302, 1.3893, 0.8224],\n",
            "          [0.6899, 0.8848, 0.8546, 0.4951]],\n",
            "\n",
            "         [[0.8451, 1.2311, 1.1311, 1.1425],\n",
            "          [0.6977, 1.2489, 0.8972, 0.9741],\n",
            "          [0.4248, 0.7448, 0.5290, 0.5856]],\n",
            "\n",
            "         [[1.5638, 0.7064, 1.6028, 1.3947],\n",
            "          [1.1921, 0.6613, 1.3454, 1.2457],\n",
            "          [0.7240, 0.4044, 0.7855, 0.7432]],\n",
            "\n",
            "         [[1.1454, 1.3193, 1.2000, 1.1434],\n",
            "          [0.9278, 1.1187, 1.0721, 1.1687],\n",
            "          [0.5716, 0.6681, 0.6271, 0.6812]],\n",
            "\n",
            "         [[0.6888, 0.4124, 1.5219, 0.9141],\n",
            "          [0.6685, 0.3171, 1.2999, 0.6326],\n",
            "          [0.4112, 0.1952, 0.7497, 0.3614]],\n",
            "\n",
            "         [[1.4687, 0.9407, 0.8608, 1.0992],\n",
            "          [1.1011, 0.8688, 0.7836, 1.0576],\n",
            "          [0.6762, 0.5135, 0.5056, 0.6577]],\n",
            "\n",
            "         [[1.8903, 1.1290, 1.8250, 1.4536],\n",
            "          [1.6867, 1.0167, 1.4290, 1.2102],\n",
            "          [1.0172, 0.6240, 0.8555, 0.7410]],\n",
            "\n",
            "         [[1.1364, 1.2159, 1.2895, 1.2812],\n",
            "          [1.0228, 0.9462, 1.0009, 1.0543],\n",
            "          [0.5856, 0.5830, 0.6152, 0.6076]]],\n",
            "\n",
            "\n",
            "        [[[0.8799, 0.8570, 0.8757, 0.5511],\n",
            "          [1.3444, 2.0410, 1.8611, 1.0891],\n",
            "          [0.4366, 0.7838, 0.7179, 0.3669]],\n",
            "\n",
            "         [[0.4560, 0.9854, 0.6950, 0.6983],\n",
            "          [1.0647, 1.5699, 1.3141, 1.4382],\n",
            "          [0.3357, 0.4440, 0.3054, 0.4053]],\n",
            "\n",
            "         [[0.7377, 0.4350, 1.1389, 0.9492],\n",
            "          [1.6125, 0.8685, 2.0951, 1.7782],\n",
            "          [0.5909, 0.3224, 0.3734, 0.4646]],\n",
            "\n",
            "         [[0.5249, 0.8118, 0.9339, 1.0349],\n",
            "          [1.1286, 1.4191, 1.8008, 1.4024],\n",
            "          [0.5175, 0.4429, 0.2866, 0.2921]],\n",
            "\n",
            "         [[0.4217, 0.1847, 1.1839, 0.5702],\n",
            "          [0.7893, 0.5057, 2.1045, 1.2574],\n",
            "          [0.3449, 0.1737, 0.2783, 0.1274]],\n",
            "\n",
            "         [[0.6334, 0.6844, 0.2998, 0.6228],\n",
            "          [1.6533, 0.9824, 0.8673, 1.3734],\n",
            "          [0.6016, 0.2925, 0.6035, 0.5980]],\n",
            "\n",
            "         [[1.1897, 0.6291, 0.9939, 0.7291],\n",
            "          [2.2402, 1.2218, 1.9749, 1.4603],\n",
            "          [0.7240, 0.5268, 0.5994, 0.6325]],\n",
            "\n",
            "         [[0.9617, 0.5254, 0.5872, 0.9466],\n",
            "          [1.4399, 1.2123, 1.5160, 1.6815],\n",
            "          [0.1853, 0.5343, 0.5419, 0.2323]]],\n",
            "\n",
            "\n",
            "        [[[0.4249, 0.8443, 0.7375, 0.3473],\n",
            "          [1.1019, 1.5713, 1.4827, 0.7141],\n",
            "          [0.7800, 1.1436, 1.0651, 0.3137]],\n",
            "\n",
            "         [[0.4050, 0.5079, 0.3941, 0.5073],\n",
            "          [0.7155, 1.2197, 0.7646, 0.9563],\n",
            "          [0.4895, 0.9345, 0.4151, 0.6606]],\n",
            "\n",
            "         [[0.5440, 0.3307, 0.6390, 0.5977],\n",
            "          [1.0523, 0.6889, 1.2078, 1.2039],\n",
            "          [0.4236, 0.4910, 0.8620, 0.8665]],\n",
            "\n",
            "         [[0.4060, 0.4271, 0.6035, 0.3592],\n",
            "          [0.8602, 0.9704, 1.0712, 1.0226],\n",
            "          [0.3643, 0.4942, 0.9509, 0.7440]],\n",
            "\n",
            "         [[0.3010, 0.2021, 0.6207, 0.3846],\n",
            "          [0.6900, 0.3299, 1.1598, 0.5332],\n",
            "          [0.4627, 0.2076, 0.9104, 0.3681]],\n",
            "\n",
            "         [[0.6288, 0.2523, 0.4296, 0.5812],\n",
            "          [1.0708, 0.7230, 0.8959, 1.1810],\n",
            "          [0.5506, 0.3782, 0.5138, 0.8752]],\n",
            "\n",
            "         [[0.7692, 0.4478, 0.6224, 0.5075],\n",
            "          [1.6260, 0.9995, 1.2338, 1.1143],\n",
            "          [1.0586, 0.5823, 0.5701, 0.5080]],\n",
            "\n",
            "         [[0.3560, 0.4407, 0.5898, 0.4785],\n",
            "          [0.8349, 0.8741, 1.0098, 0.8989],\n",
            "          [0.5830, 0.3533, 0.5918, 0.6333]]]], device='cuda:0',\n",
            "       grad_fn=<AddBackward0>)\n",
            "keys: tensor([[[[1.1525, 0.5653, 0.7513, 1.0194],\n",
            "          [1.2390, 0.7970, 1.0305, 1.4978],\n",
            "          [1.8320, 1.0361, 1.0636, 1.5071]],\n",
            "\n",
            "         [[0.6929, 0.1577, 0.7366, 1.1930],\n",
            "          [0.9373, 0.3738, 1.1119, 1.4059],\n",
            "          [1.0624, 0.2310, 1.5254, 1.5780]],\n",
            "\n",
            "         [[0.4769, 0.4645, 0.3534, 0.2993],\n",
            "          [0.5266, 0.5650, 0.3913, 0.6418],\n",
            "          [1.4121, 0.9076, 0.7536, 0.3646]],\n",
            "\n",
            "         [[1.0044, 0.9692, 0.5048, 0.4299],\n",
            "          [1.5552, 1.4204, 0.6960, 0.7750],\n",
            "          [1.6335, 1.3116, 1.1092, 0.6488]],\n",
            "\n",
            "         [[1.0481, 0.4693, 1.1383, 0.7802],\n",
            "          [1.1137, 0.6331, 1.5345, 1.1286],\n",
            "          [1.9024, 0.6278, 1.9713, 1.1400]],\n",
            "\n",
            "         [[1.0852, 0.6015, 0.2140, 0.7896],\n",
            "          [1.4756, 0.7012, 0.2477, 1.0484],\n",
            "          [1.7033, 0.8216, 0.8336, 1.1255]],\n",
            "\n",
            "         [[0.5559, 0.2996, 0.4252, 0.8369],\n",
            "          [0.9844, 0.3207, 0.6606, 0.9034],\n",
            "          [0.8579, 0.9546, 0.7217, 1.6628]],\n",
            "\n",
            "         [[0.6740, 0.4488, 1.2163, 0.7916],\n",
            "          [0.9345, 0.7145, 1.4339, 1.2364],\n",
            "          [0.9452, 0.9127, 2.0207, 0.9986]]],\n",
            "\n",
            "\n",
            "        [[[1.5933, 0.9916, 1.0880, 1.6701],\n",
            "          [1.4475, 0.8006, 1.0380, 1.3524],\n",
            "          [0.8783, 0.4983, 0.5992, 0.8173]],\n",
            "\n",
            "         [[1.1389, 0.3593, 1.3371, 1.6517],\n",
            "          [0.8522, 0.2558, 1.2217, 1.3759],\n",
            "          [0.5349, 0.1532, 0.7374, 0.8321]],\n",
            "\n",
            "         [[0.9019, 0.7458, 0.5403, 0.6266],\n",
            "          [0.8751, 0.6797, 0.5558, 0.4100],\n",
            "          [0.5545, 0.4166, 0.3350, 0.2501]],\n",
            "\n",
            "         [[1.6883, 1.5425, 0.8869, 0.8196],\n",
            "          [1.4984, 1.2337, 0.8472, 0.6144],\n",
            "          [0.8857, 0.7413, 0.5126, 0.3734]],\n",
            "\n",
            "         [[1.5147, 0.7215, 1.8248, 1.1969],\n",
            "          [1.4206, 0.5571, 1.6345, 1.0975],\n",
            "          [0.8673, 0.3408, 0.9837, 0.6395]],\n",
            "\n",
            "         [[1.7140, 0.8116, 0.4355, 1.1840],\n",
            "          [1.4571, 0.7309, 0.5358, 1.0098],\n",
            "          [0.8803, 0.4339, 0.3248, 0.6043]],\n",
            "\n",
            "         [[1.0102, 0.5620, 0.7151, 1.2500],\n",
            "          [0.8472, 0.5991, 0.6614, 1.2227],\n",
            "          [0.4995, 0.3730, 0.3881, 0.7436]],\n",
            "\n",
            "         [[1.0678, 0.8181, 1.7830, 1.2894],\n",
            "          [0.8290, 0.7698, 1.6295, 1.0021],\n",
            "          [0.5076, 0.4574, 0.9831, 0.5989]]],\n",
            "\n",
            "\n",
            "        [[[0.9541, 0.4278, 0.9565, 0.9118],\n",
            "          [1.5142, 1.1269, 1.4268, 1.9844],\n",
            "          [0.6786, 0.4804, 0.2170, 0.6120]],\n",
            "\n",
            "         [[0.3927, 0.1866, 0.8855, 0.8957],\n",
            "          [1.1408, 0.5656, 1.8279, 1.5562],\n",
            "          [0.5639, 0.1031, 0.5132, 0.6423]],\n",
            "\n",
            "         [[0.4330, 0.4295, 0.4100, 0.2477],\n",
            "          [1.0421, 0.8374, 0.6419, 0.8730],\n",
            "          [0.5802, 0.3449, 0.2269, 0.2117]],\n",
            "\n",
            "         [[1.2143, 0.8576, 0.6092, 0.3976],\n",
            "          [2.3048, 1.8071, 1.1783, 1.0932],\n",
            "          [0.4869, 0.5276, 0.3629, 0.2961]],\n",
            "\n",
            "         [[0.9138, 0.3283, 1.1735, 0.9606],\n",
            "          [1.5096, 0.7423, 2.2109, 1.5882],\n",
            "          [0.6986, 0.2947, 0.6823, 0.2811]],\n",
            "\n",
            "         [[0.9961, 0.5560, 0.4132, 0.7299],\n",
            "          [1.9780, 0.8292, 0.6914, 1.3329],\n",
            "          [0.6497, 0.2646, 0.2153, 0.4063]],\n",
            "\n",
            "         [[0.6942, 0.3625, 0.5666, 0.8290],\n",
            "          [1.4681, 0.7132, 1.0182, 1.3526],\n",
            "          [0.2667, 0.3342, 0.1864, 0.5659]],\n",
            "\n",
            "         [[0.4905, 0.6232, 1.1370, 0.7180],\n",
            "          [1.1420, 1.2013, 1.9161, 1.5709],\n",
            "          [0.4390, 0.2589, 0.7074, 0.4049]]],\n",
            "\n",
            "\n",
            "        [[[0.4616, 0.4624, 0.3786, 0.7034],\n",
            "          [1.2770, 0.8350, 0.9030, 1.2971],\n",
            "          [0.5793, 0.4860, 0.6906, 0.7867]],\n",
            "\n",
            "         [[0.4659, 0.2256, 0.6821, 0.4660],\n",
            "          [0.8402, 0.2888, 1.2860, 1.1614],\n",
            "          [0.3426, 0.2578, 1.0262, 0.4246]],\n",
            "\n",
            "         [[0.4814, 0.3097, 0.2130, 0.3562],\n",
            "          [1.0061, 0.6787, 0.5453, 0.4456],\n",
            "          [0.6953, 0.4217, 0.3897, 0.3185]],\n",
            "\n",
            "         [[0.7688, 0.6145, 0.4362, 0.4274],\n",
            "          [1.4596, 1.1439, 0.8872, 0.6440],\n",
            "          [1.1473, 0.6613, 0.6964, 0.4563]],\n",
            "\n",
            "         [[0.5025, 0.2621, 0.7591, 0.4670],\n",
            "          [1.3258, 0.5080, 1.5843, 0.9955],\n",
            "          [0.7001, 0.2148, 1.0671, 0.7612]],\n",
            "\n",
            "         [[0.6815, 0.2197, 0.2719, 0.4228],\n",
            "          [1.3819, 0.6086, 0.6200, 0.9096],\n",
            "          [0.8297, 0.2968, 0.5992, 0.5155]],\n",
            "\n",
            "         [[0.5112, 0.3057, 0.3306, 0.4576],\n",
            "          [0.8514, 0.6819, 0.6453, 1.1783],\n",
            "          [0.7130, 0.5346, 0.5428, 0.7300]],\n",
            "\n",
            "         [[0.4144, 0.4254, 0.6130, 0.5348],\n",
            "          [0.7776, 0.8013, 1.4975, 0.9301],\n",
            "          [0.3684, 0.6999, 0.8513, 0.5627]]]], device='cuda:0',\n",
            "       grad_fn=<AddBackward0>)\n",
            "values: tensor([[[[0.7152, 0.9848],\n",
            "          [1.0479, 1.2511],\n",
            "          [1.1167, 1.5589]],\n",
            "\n",
            "         [[0.4957, 0.6030],\n",
            "          [0.8720, 1.1114],\n",
            "          [0.5333, 1.1611]],\n",
            "\n",
            "         [[0.5664, 1.0078],\n",
            "          [1.0194, 1.5073],\n",
            "          [0.6463, 1.7179]],\n",
            "\n",
            "         [[1.0600, 0.8251],\n",
            "          [1.5713, 0.9703],\n",
            "          [1.6598, 1.5885]],\n",
            "\n",
            "         [[0.8740, 0.8636],\n",
            "          [1.1552, 1.0517],\n",
            "          [1.3910, 1.1983]],\n",
            "\n",
            "         [[0.8556, 0.4282],\n",
            "          [0.9728, 0.8037],\n",
            "          [1.3823, 0.8536]],\n",
            "\n",
            "         [[0.7551, 0.2786],\n",
            "          [1.1686, 0.3334],\n",
            "          [1.3327, 0.3095]],\n",
            "\n",
            "         [[1.0342, 1.1618],\n",
            "          [1.5670, 1.6724],\n",
            "          [1.3047, 1.6776]]],\n",
            "\n",
            "\n",
            "        [[[1.1495, 1.4295],\n",
            "          [1.0202, 1.3768],\n",
            "          [0.6037, 0.8082]],\n",
            "\n",
            "         [[0.8542, 1.1686],\n",
            "          [0.6110, 1.0761],\n",
            "          [0.3642, 0.6302]],\n",
            "\n",
            "         [[1.0154, 1.6972],\n",
            "          [0.7099, 1.5031],\n",
            "          [0.4280, 0.8968]],\n",
            "\n",
            "         [[1.7123, 1.2527],\n",
            "          [1.5272, 1.2326],\n",
            "          [0.9016, 0.7400]],\n",
            "\n",
            "         [[1.3198, 1.1598],\n",
            "          [1.2211, 1.1236],\n",
            "          [0.7227, 0.6518]],\n",
            "\n",
            "         [[1.2227, 0.8712],\n",
            "          [1.1086, 0.7571],\n",
            "          [0.6707, 0.4518]],\n",
            "\n",
            "         [[1.3035, 0.3576],\n",
            "          [1.1727, 0.3111],\n",
            "          [0.6968, 0.1819]],\n",
            "\n",
            "         [[1.6167, 1.8547],\n",
            "          [1.3318, 1.5243],\n",
            "          [0.7844, 0.9165]]],\n",
            "\n",
            "\n",
            "        [[[0.8156, 1.1618],\n",
            "          [1.4933, 1.7584],\n",
            "          [0.3400, 0.3988]],\n",
            "\n",
            "         [[0.4365, 0.9495],\n",
            "          [1.0914, 1.8792],\n",
            "          [0.2446, 0.2833]],\n",
            "\n",
            "         [[0.4669, 1.1495],\n",
            "          [1.2944, 2.2395],\n",
            "          [0.3254, 0.5585]],\n",
            "\n",
            "         [[1.2408, 0.9189],\n",
            "          [2.2598, 1.4811],\n",
            "          [0.4900, 0.4873]],\n",
            "\n",
            "         [[0.9770, 1.0007],\n",
            "          [1.6189, 1.3604],\n",
            "          [0.4064, 0.2663]],\n",
            "\n",
            "         [[0.7519, 0.5933],\n",
            "          [1.2447, 1.3428],\n",
            "          [0.5004, 0.2737]],\n",
            "\n",
            "         [[0.9290, 0.2534],\n",
            "          [1.7984, 0.3562],\n",
            "          [0.4065, 0.0929]],\n",
            "\n",
            "         [[1.0635, 1.0666],\n",
            "          [2.0215, 2.1909],\n",
            "          [0.4316, 0.6498]]],\n",
            "\n",
            "\n",
            "        [[[0.4835, 0.4970],\n",
            "          [0.9660, 1.2330],\n",
            "          [0.7089, 0.8731]],\n",
            "\n",
            "         [[0.3842, 0.6565],\n",
            "          [0.5747, 1.1313],\n",
            "          [0.3575, 1.0887]],\n",
            "\n",
            "         [[0.4816, 0.7716],\n",
            "          [0.6940, 1.4826],\n",
            "          [0.4274, 1.1145]],\n",
            "\n",
            "         [[0.7288, 0.4806],\n",
            "          [1.4488, 1.1824],\n",
            "          [1.0892, 0.8165]],\n",
            "\n",
            "         [[0.4945, 0.3253],\n",
            "          [1.1219, 0.9320],\n",
            "          [0.7759, 0.6112]],\n",
            "\n",
            "         [[0.3912, 0.5072],\n",
            "          [0.9996, 0.8258],\n",
            "          [0.5138, 0.7470]],\n",
            "\n",
            "         [[0.6200, 0.0793],\n",
            "          [1.1729, 0.2361],\n",
            "          [0.9401, 0.0956]],\n",
            "\n",
            "         [[0.6319, 0.7482],\n",
            "          [1.1970, 1.4313],\n",
            "          [0.7854, 0.8640]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "keys_transposed: tensor([[[[1.1525, 1.2390, 1.8320],\n",
            "          [0.5653, 0.7970, 1.0361],\n",
            "          [0.7513, 1.0305, 1.0636],\n",
            "          [1.0194, 1.4978, 1.5071]],\n",
            "\n",
            "         [[0.6929, 0.9373, 1.0624],\n",
            "          [0.1577, 0.3738, 0.2310],\n",
            "          [0.7366, 1.1119, 1.5254],\n",
            "          [1.1930, 1.4059, 1.5780]],\n",
            "\n",
            "         [[0.4769, 0.5266, 1.4121],\n",
            "          [0.4645, 0.5650, 0.9076],\n",
            "          [0.3534, 0.3913, 0.7536],\n",
            "          [0.2993, 0.6418, 0.3646]],\n",
            "\n",
            "         [[1.0044, 1.5552, 1.6335],\n",
            "          [0.9692, 1.4204, 1.3116],\n",
            "          [0.5048, 0.6960, 1.1092],\n",
            "          [0.4299, 0.7750, 0.6488]],\n",
            "\n",
            "         [[1.0481, 1.1137, 1.9024],\n",
            "          [0.4693, 0.6331, 0.6278],\n",
            "          [1.1383, 1.5345, 1.9713],\n",
            "          [0.7802, 1.1286, 1.1400]],\n",
            "\n",
            "         [[1.0852, 1.4756, 1.7033],\n",
            "          [0.6015, 0.7012, 0.8216],\n",
            "          [0.2140, 0.2477, 0.8336],\n",
            "          [0.7896, 1.0484, 1.1255]],\n",
            "\n",
            "         [[0.5559, 0.9844, 0.8579],\n",
            "          [0.2996, 0.3207, 0.9546],\n",
            "          [0.4252, 0.6606, 0.7217],\n",
            "          [0.8369, 0.9034, 1.6628]],\n",
            "\n",
            "         [[0.6740, 0.9345, 0.9452],\n",
            "          [0.4488, 0.7145, 0.9127],\n",
            "          [1.2163, 1.4339, 2.0207],\n",
            "          [0.7916, 1.2364, 0.9986]]],\n",
            "\n",
            "\n",
            "        [[[1.5933, 1.4475, 0.8783],\n",
            "          [0.9916, 0.8006, 0.4983],\n",
            "          [1.0880, 1.0380, 0.5992],\n",
            "          [1.6701, 1.3524, 0.8173]],\n",
            "\n",
            "         [[1.1389, 0.8522, 0.5349],\n",
            "          [0.3593, 0.2558, 0.1532],\n",
            "          [1.3371, 1.2217, 0.7374],\n",
            "          [1.6517, 1.3759, 0.8321]],\n",
            "\n",
            "         [[0.9019, 0.8751, 0.5545],\n",
            "          [0.7458, 0.6797, 0.4166],\n",
            "          [0.5403, 0.5558, 0.3350],\n",
            "          [0.6266, 0.4100, 0.2501]],\n",
            "\n",
            "         [[1.6883, 1.4984, 0.8857],\n",
            "          [1.5425, 1.2337, 0.7413],\n",
            "          [0.8869, 0.8472, 0.5126],\n",
            "          [0.8196, 0.6144, 0.3734]],\n",
            "\n",
            "         [[1.5147, 1.4206, 0.8673],\n",
            "          [0.7215, 0.5571, 0.3408],\n",
            "          [1.8248, 1.6345, 0.9837],\n",
            "          [1.1969, 1.0975, 0.6395]],\n",
            "\n",
            "         [[1.7140, 1.4571, 0.8803],\n",
            "          [0.8116, 0.7309, 0.4339],\n",
            "          [0.4355, 0.5358, 0.3248],\n",
            "          [1.1840, 1.0098, 0.6043]],\n",
            "\n",
            "         [[1.0102, 0.8472, 0.4995],\n",
            "          [0.5620, 0.5991, 0.3730],\n",
            "          [0.7151, 0.6614, 0.3881],\n",
            "          [1.2500, 1.2227, 0.7436]],\n",
            "\n",
            "         [[1.0678, 0.8290, 0.5076],\n",
            "          [0.8181, 0.7698, 0.4574],\n",
            "          [1.7830, 1.6295, 0.9831],\n",
            "          [1.2894, 1.0021, 0.5989]]],\n",
            "\n",
            "\n",
            "        [[[0.9541, 1.5142, 0.6786],\n",
            "          [0.4278, 1.1269, 0.4804],\n",
            "          [0.9565, 1.4268, 0.2170],\n",
            "          [0.9118, 1.9844, 0.6120]],\n",
            "\n",
            "         [[0.3927, 1.1408, 0.5639],\n",
            "          [0.1866, 0.5656, 0.1031],\n",
            "          [0.8855, 1.8279, 0.5132],\n",
            "          [0.8957, 1.5562, 0.6423]],\n",
            "\n",
            "         [[0.4330, 1.0421, 0.5802],\n",
            "          [0.4295, 0.8374, 0.3449],\n",
            "          [0.4100, 0.6419, 0.2269],\n",
            "          [0.2477, 0.8730, 0.2117]],\n",
            "\n",
            "         [[1.2143, 2.3048, 0.4869],\n",
            "          [0.8576, 1.8071, 0.5276],\n",
            "          [0.6092, 1.1783, 0.3629],\n",
            "          [0.3976, 1.0932, 0.2961]],\n",
            "\n",
            "         [[0.9138, 1.5096, 0.6986],\n",
            "          [0.3283, 0.7423, 0.2947],\n",
            "          [1.1735, 2.2109, 0.6823],\n",
            "          [0.9606, 1.5882, 0.2811]],\n",
            "\n",
            "         [[0.9961, 1.9780, 0.6497],\n",
            "          [0.5560, 0.8292, 0.2646],\n",
            "          [0.4132, 0.6914, 0.2153],\n",
            "          [0.7299, 1.3329, 0.4063]],\n",
            "\n",
            "         [[0.6942, 1.4681, 0.2667],\n",
            "          [0.3625, 0.7132, 0.3342],\n",
            "          [0.5666, 1.0182, 0.1864],\n",
            "          [0.8290, 1.3526, 0.5659]],\n",
            "\n",
            "         [[0.4905, 1.1420, 0.4390],\n",
            "          [0.6232, 1.2013, 0.2589],\n",
            "          [1.1370, 1.9161, 0.7074],\n",
            "          [0.7180, 1.5709, 0.4049]]],\n",
            "\n",
            "\n",
            "        [[[0.4616, 1.2770, 0.5793],\n",
            "          [0.4624, 0.8350, 0.4860],\n",
            "          [0.3786, 0.9030, 0.6906],\n",
            "          [0.7034, 1.2971, 0.7867]],\n",
            "\n",
            "         [[0.4659, 0.8402, 0.3426],\n",
            "          [0.2256, 0.2888, 0.2578],\n",
            "          [0.6821, 1.2860, 1.0262],\n",
            "          [0.4660, 1.1614, 0.4246]],\n",
            "\n",
            "         [[0.4814, 1.0061, 0.6953],\n",
            "          [0.3097, 0.6787, 0.4217],\n",
            "          [0.2130, 0.5453, 0.3897],\n",
            "          [0.3562, 0.4456, 0.3185]],\n",
            "\n",
            "         [[0.7688, 1.4596, 1.1473],\n",
            "          [0.6145, 1.1439, 0.6613],\n",
            "          [0.4362, 0.8872, 0.6964],\n",
            "          [0.4274, 0.6440, 0.4563]],\n",
            "\n",
            "         [[0.5025, 1.3258, 0.7001],\n",
            "          [0.2621, 0.5080, 0.2148],\n",
            "          [0.7591, 1.5843, 1.0671],\n",
            "          [0.4670, 0.9955, 0.7612]],\n",
            "\n",
            "         [[0.6815, 1.3819, 0.8297],\n",
            "          [0.2197, 0.6086, 0.2968],\n",
            "          [0.2719, 0.6200, 0.5992],\n",
            "          [0.4228, 0.9096, 0.5155]],\n",
            "\n",
            "         [[0.5112, 0.8514, 0.7130],\n",
            "          [0.3057, 0.6819, 0.5346],\n",
            "          [0.3306, 0.6453, 0.5428],\n",
            "          [0.4576, 1.1783, 0.7300]],\n",
            "\n",
            "         [[0.4144, 0.7776, 0.3684],\n",
            "          [0.4254, 0.8013, 0.6999],\n",
            "          [0.6130, 1.4975, 0.8513],\n",
            "          [0.5348, 0.9301, 0.5627]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "scores: tensor([[[[2.7086, 3.5449, 4.2327],\n",
            "          [3.4779, 4.5979, 5.4430],\n",
            "          [5.1699, 6.7057, 8.1390]],\n",
            "\n",
            "         [[1.8277, 2.5444, 2.9224],\n",
            "          [2.6823, 3.7011, 4.2874],\n",
            "          [2.7758, 3.8767, 4.2920]],\n",
            "\n",
            "         [[1.3032, 1.7286, 2.9196],\n",
            "          [1.8510, 2.4544, 4.1286],\n",
            "          [1.9023, 2.5950, 4.0917]],\n",
            "\n",
            "         [[2.3399, 3.5826, 3.7155],\n",
            "          [3.0870, 4.6938, 4.9934],\n",
            "          [3.4693, 5.3254, 5.5715]],\n",
            "\n",
            "         [[2.0757, 2.7150, 3.4724],\n",
            "          [3.2690, 4.3451, 5.4127],\n",
            "          [2.7881, 3.5018, 4.7662]],\n",
            "\n",
            "         [[1.9389, 2.5193, 3.1249],\n",
            "          [2.6031, 3.4052, 4.1451],\n",
            "          [3.4407, 4.4685, 5.7494]],\n",
            "\n",
            "         [[2.2112, 3.0902, 4.2068],\n",
            "          [2.8563, 4.0316, 5.3837],\n",
            "          [3.3983, 4.7431, 6.5549]],\n",
            "\n",
            "         [[2.4708, 3.4282, 3.8294],\n",
            "          [3.6589, 5.0688, 5.6300],\n",
            "          [3.2618, 4.4556, 5.1587]]],\n",
            "\n",
            "\n",
            "        [[[6.7630, 5.8862, 3.5424],\n",
            "          [6.1387, 5.3668, 3.2291],\n",
            "          [3.7331, 3.2635, 1.9635]],\n",
            "\n",
            "         [[4.8044, 3.9888, 2.4254],\n",
            "          [4.0520, 3.3503, 2.0366],\n",
            "          [2.4261, 2.0045, 1.2187]],\n",
            "\n",
            "         [[3.6771, 3.3113, 2.0472],\n",
            "          [3.0758, 2.7512, 1.6988],\n",
            "          [1.8447, 1.6497, 1.0189]],\n",
            "\n",
            "         [[5.9704, 5.0631, 3.0346],\n",
            "          [5.2008, 4.3967, 2.6371],\n",
            "          [3.1101, 2.6305, 1.5773]],\n",
            "\n",
            "         [[5.2121, 4.6990, 2.8196],\n",
            "          [4.3706, 3.9453, 2.3712],\n",
            "          [2.5642, 2.3148, 1.3917]],\n",
            "\n",
            "         [[4.9572, 4.3987, 2.6450],\n",
            "          [4.1858, 3.7271, 2.2399],\n",
            "          [2.5748, 2.2957, 1.3798]],\n",
            "\n",
            "         [[5.6661, 5.2624, 3.1545],\n",
            "          [4.8097, 4.4630, 2.6762],\n",
            "          [2.9162, 2.7076, 1.6239]],\n",
            "\n",
            "         [[6.1592, 5.2631, 3.1681],\n",
            "          [5.0102, 4.2637, 2.5674],\n",
            "          [2.9826, 2.5456, 1.5327]]],\n",
            "\n",
            "\n",
            "        [[[2.5462, 4.6411, 1.5361],\n",
            "          [4.9289, 9.1524, 2.9632],\n",
            "          [1.7731, 3.2968, 1.0531]],\n",
            "\n",
            "         [[1.6039, 3.4347, 1.1639],\n",
            "          [3.1630, 6.7429, 2.3603],\n",
            "          [0.8482, 1.8232, 0.6521]],\n",
            "\n",
            "         [[1.2083, 2.6927, 1.0373],\n",
            "          [2.3707, 5.3048, 2.0868],\n",
            "          [0.6625, 1.5309, 0.6370]],\n",
            "\n",
            "         [[2.3141, 4.9086, 1.3292],\n",
            "          [4.2421, 8.8204, 2.3669],\n",
            "          [1.2990, 2.6501, 0.6761]],\n",
            "\n",
            "         [[2.3830, 4.2968, 1.3170],\n",
            "          [4.5648, 8.2169, 2.4897],\n",
            "          [0.8212, 1.4674, 0.5179]],\n",
            "\n",
            "         [[1.5899, 2.8578, 0.9102],\n",
            "          [3.5538, 6.5152, 2.0789],\n",
            "          [1.4476, 2.6468, 0.8412]],\n",
            "\n",
            "         [[2.2215, 4.1934, 1.1255],\n",
            "          [4.3276, 8.1463, 2.2004],\n",
            "          [1.5575, 2.9045, 0.8389]],\n",
            "\n",
            "         [[2.1465, 4.3416, 1.3569],\n",
            "          [4.3930, 8.6470, 2.6994],\n",
            "          [1.2068, 2.2566, 0.6970]]],\n",
            "\n",
            "\n",
            "        [[[1.1101, 2.3640, 1.4390],\n",
            "          [2.2989, 4.9843, 2.9876],\n",
            "          [1.5127, 3.3196, 1.9899]],\n",
            "\n",
            "         [[0.8085, 1.5829, 0.8895],\n",
            "          [1.5757, 3.0473, 1.7502],\n",
            "          [1.0300, 1.9822, 1.1151]],\n",
            "\n",
            "         [[0.7133, 1.3866, 0.9570],\n",
            "          [1.4060, 2.7214, 1.8762],\n",
            "          [0.8482, 1.6156, 1.1134]],\n",
            "\n",
            "         [[0.9914, 1.8479, 1.3325],\n",
            "          [2.1620, 3.9745, 2.8414],\n",
            "          [1.3165, 2.4198, 1.7465]],\n",
            "\n",
            "         [[0.8550, 1.8680, 1.2092],\n",
            "          [1.5626, 3.4507, 2.1973],\n",
            "          [1.1499, 2.5277, 1.6202]],\n",
            "\n",
            "         [[0.8465, 1.8175, 1.1535],\n",
            "          [1.6315, 3.5495, 2.2486],\n",
            "          [0.9681, 2.1057, 1.3281]],\n",
            "\n",
            "         [[0.9681, 1.9598, 1.4961],\n",
            "          [2.0546, 4.1753, 3.1769],\n",
            "          [1.1401, 2.2649, 1.7464]],\n",
            "\n",
            "         [[0.9524, 1.9583, 1.2109],\n",
            "          [1.8175, 3.6979, 2.2847],\n",
            "          [1.0933, 2.2117, 1.3222]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "scores: torch.Size([4, 8, 3, 3])\n",
            "masks: torch.Size([4, 1, 3, 3])\n",
            "masked scores: tensor([[[[ 2.7086e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.4779e+00,  4.5979e+00, -1.0000e+09],\n",
            "          [ 5.1699e+00,  6.7057e+00, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.8277e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.6823e+00,  3.7011e+00, -1.0000e+09],\n",
            "          [ 2.7758e+00,  3.8767e+00, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.3032e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.8510e+00,  2.4544e+00, -1.0000e+09],\n",
            "          [ 1.9023e+00,  2.5950e+00, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.3399e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.0870e+00,  4.6938e+00, -1.0000e+09],\n",
            "          [ 3.4693e+00,  5.3254e+00, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.0757e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.2690e+00,  4.3451e+00, -1.0000e+09],\n",
            "          [ 2.7881e+00,  3.5018e+00, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.9389e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.6031e+00,  3.4052e+00, -1.0000e+09],\n",
            "          [ 3.4407e+00,  4.4685e+00, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.2112e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.8563e+00,  4.0316e+00, -1.0000e+09],\n",
            "          [ 3.3983e+00,  4.7431e+00, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.4708e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.6589e+00,  5.0688e+00, -1.0000e+09],\n",
            "          [ 3.2618e+00,  4.4556e+00, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 6.7630e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 6.1387e+00,  5.3668e+00, -1.0000e+09],\n",
            "          [ 3.7331e+00,  3.2635e+00, -1.0000e+09]],\n",
            "\n",
            "         [[ 4.8044e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.0520e+00,  3.3503e+00, -1.0000e+09],\n",
            "          [ 2.4261e+00,  2.0045e+00, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.6771e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.0758e+00,  2.7512e+00, -1.0000e+09],\n",
            "          [ 1.8447e+00,  1.6497e+00, -1.0000e+09]],\n",
            "\n",
            "         [[ 5.9704e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 5.2008e+00,  4.3967e+00, -1.0000e+09],\n",
            "          [ 3.1101e+00,  2.6305e+00, -1.0000e+09]],\n",
            "\n",
            "         [[ 5.2121e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.3706e+00,  3.9453e+00, -1.0000e+09],\n",
            "          [ 2.5642e+00,  2.3148e+00, -1.0000e+09]],\n",
            "\n",
            "         [[ 4.9572e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.1858e+00,  3.7271e+00, -1.0000e+09],\n",
            "          [ 2.5748e+00,  2.2957e+00, -1.0000e+09]],\n",
            "\n",
            "         [[ 5.6661e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.8097e+00,  4.4630e+00, -1.0000e+09],\n",
            "          [ 2.9162e+00,  2.7076e+00, -1.0000e+09]],\n",
            "\n",
            "         [[ 6.1592e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 5.0102e+00,  4.2637e+00, -1.0000e+09],\n",
            "          [ 2.9826e+00,  2.5456e+00, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 2.5462e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.9289e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.7731e+00, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.6039e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.1630e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 8.4820e-01, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.2083e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.3707e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 6.6245e-01, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.3141e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.2421e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.2990e+00, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.3830e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.5648e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 8.2122e-01, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.5899e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.5538e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.4476e+00, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.2215e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.3276e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.5575e+00, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.1465e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.3930e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.2068e+00, -1.0000e+09, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1101e+00, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.2989e+00,  4.9843e+00, -1.0000e+09],\n",
            "          [ 1.5127e+00,  3.3196e+00,  1.9899e+00]],\n",
            "\n",
            "         [[ 8.0851e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.5757e+00,  3.0473e+00, -1.0000e+09],\n",
            "          [ 1.0300e+00,  1.9822e+00,  1.1151e+00]],\n",
            "\n",
            "         [[ 7.1331e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.4060e+00,  2.7214e+00, -1.0000e+09],\n",
            "          [ 8.4824e-01,  1.6156e+00,  1.1134e+00]],\n",
            "\n",
            "         [[ 9.9135e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.1620e+00,  3.9745e+00, -1.0000e+09],\n",
            "          [ 1.3165e+00,  2.4198e+00,  1.7465e+00]],\n",
            "\n",
            "         [[ 8.5501e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.5626e+00,  3.4507e+00, -1.0000e+09],\n",
            "          [ 1.1499e+00,  2.5277e+00,  1.6202e+00]],\n",
            "\n",
            "         [[ 8.4648e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.6315e+00,  3.5495e+00, -1.0000e+09],\n",
            "          [ 9.6808e-01,  2.1057e+00,  1.3281e+00]],\n",
            "\n",
            "         [[ 9.6807e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.0546e+00,  4.1753e+00, -1.0000e+09],\n",
            "          [ 1.1401e+00,  2.2649e+00,  1.7464e+00]],\n",
            "\n",
            "         [[ 9.5245e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.8175e+00,  3.6979e+00, -1.0000e+09],\n",
            "          [ 1.0933e+00,  2.2117e+00,  1.3222e+00]]]], device='cuda:0',\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "scaled_scores: tensor([[[[ 1.3543e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 1.7389e+00,  2.2989e+00, -5.0000e+08],\n",
            "          [ 2.5849e+00,  3.3528e+00, -5.0000e+08]],\n",
            "\n",
            "         [[ 9.1386e-01, -5.0000e+08, -5.0000e+08],\n",
            "          [ 1.3411e+00,  1.8505e+00, -5.0000e+08],\n",
            "          [ 1.3879e+00,  1.9383e+00, -5.0000e+08]],\n",
            "\n",
            "         [[ 6.5160e-01, -5.0000e+08, -5.0000e+08],\n",
            "          [ 9.2548e-01,  1.2272e+00, -5.0000e+08],\n",
            "          [ 9.5116e-01,  1.2975e+00, -5.0000e+08]],\n",
            "\n",
            "         [[ 1.1699e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 1.5435e+00,  2.3469e+00, -5.0000e+08],\n",
            "          [ 1.7347e+00,  2.6627e+00, -5.0000e+08]],\n",
            "\n",
            "         [[ 1.0378e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 1.6345e+00,  2.1725e+00, -5.0000e+08],\n",
            "          [ 1.3941e+00,  1.7509e+00, -5.0000e+08]],\n",
            "\n",
            "         [[ 9.6947e-01, -5.0000e+08, -5.0000e+08],\n",
            "          [ 1.3016e+00,  1.7026e+00, -5.0000e+08],\n",
            "          [ 1.7204e+00,  2.2343e+00, -5.0000e+08]],\n",
            "\n",
            "         [[ 1.1056e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 1.4282e+00,  2.0158e+00, -5.0000e+08],\n",
            "          [ 1.6991e+00,  2.3716e+00, -5.0000e+08]],\n",
            "\n",
            "         [[ 1.2354e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 1.8295e+00,  2.5344e+00, -5.0000e+08],\n",
            "          [ 1.6309e+00,  2.2278e+00, -5.0000e+08]]],\n",
            "\n",
            "\n",
            "        [[[ 3.3815e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 3.0693e+00,  2.6834e+00, -5.0000e+08],\n",
            "          [ 1.8666e+00,  1.6318e+00, -5.0000e+08]],\n",
            "\n",
            "         [[ 2.4022e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 2.0260e+00,  1.6752e+00, -5.0000e+08],\n",
            "          [ 1.2130e+00,  1.0023e+00, -5.0000e+08]],\n",
            "\n",
            "         [[ 1.8385e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 1.5379e+00,  1.3756e+00, -5.0000e+08],\n",
            "          [ 9.2233e-01,  8.2486e-01, -5.0000e+08]],\n",
            "\n",
            "         [[ 2.9852e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 2.6004e+00,  2.1984e+00, -5.0000e+08],\n",
            "          [ 1.5550e+00,  1.3152e+00, -5.0000e+08]],\n",
            "\n",
            "         [[ 2.6060e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 2.1853e+00,  1.9726e+00, -5.0000e+08],\n",
            "          [ 1.2821e+00,  1.1574e+00, -5.0000e+08]],\n",
            "\n",
            "         [[ 2.4786e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 2.0929e+00,  1.8636e+00, -5.0000e+08],\n",
            "          [ 1.2874e+00,  1.1478e+00, -5.0000e+08]],\n",
            "\n",
            "         [[ 2.8330e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 2.4049e+00,  2.2315e+00, -5.0000e+08],\n",
            "          [ 1.4581e+00,  1.3538e+00, -5.0000e+08]],\n",
            "\n",
            "         [[ 3.0796e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 2.5051e+00,  2.1319e+00, -5.0000e+08],\n",
            "          [ 1.4913e+00,  1.2728e+00, -5.0000e+08]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2731e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 2.4645e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 8.8653e-01, -5.0000e+08, -5.0000e+08]],\n",
            "\n",
            "         [[ 8.0194e-01, -5.0000e+08, -5.0000e+08],\n",
            "          [ 1.5815e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 4.2410e-01, -5.0000e+08, -5.0000e+08]],\n",
            "\n",
            "         [[ 6.0414e-01, -5.0000e+08, -5.0000e+08],\n",
            "          [ 1.1853e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 3.3123e-01, -5.0000e+08, -5.0000e+08]],\n",
            "\n",
            "         [[ 1.1570e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 2.1211e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 6.4949e-01, -5.0000e+08, -5.0000e+08]],\n",
            "\n",
            "         [[ 1.1915e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 2.2824e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 4.1061e-01, -5.0000e+08, -5.0000e+08]],\n",
            "\n",
            "         [[ 7.9493e-01, -5.0000e+08, -5.0000e+08],\n",
            "          [ 1.7769e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 7.2382e-01, -5.0000e+08, -5.0000e+08]],\n",
            "\n",
            "         [[ 1.1107e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 2.1638e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 7.7876e-01, -5.0000e+08, -5.0000e+08]],\n",
            "\n",
            "         [[ 1.0733e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 2.1965e+00, -5.0000e+08, -5.0000e+08],\n",
            "          [ 6.0339e-01, -5.0000e+08, -5.0000e+08]]],\n",
            "\n",
            "\n",
            "        [[[ 5.5503e-01, -5.0000e+08, -5.0000e+08],\n",
            "          [ 1.1494e+00,  2.4921e+00, -5.0000e+08],\n",
            "          [ 7.5636e-01,  1.6598e+00,  9.9494e-01]],\n",
            "\n",
            "         [[ 4.0426e-01, -5.0000e+08, -5.0000e+08],\n",
            "          [ 7.8787e-01,  1.5236e+00, -5.0000e+08],\n",
            "          [ 5.1498e-01,  9.9112e-01,  5.5756e-01]],\n",
            "\n",
            "         [[ 3.5665e-01, -5.0000e+08, -5.0000e+08],\n",
            "          [ 7.0302e-01,  1.3607e+00, -5.0000e+08],\n",
            "          [ 4.2412e-01,  8.0781e-01,  5.5672e-01]],\n",
            "\n",
            "         [[ 4.9568e-01, -5.0000e+08, -5.0000e+08],\n",
            "          [ 1.0810e+00,  1.9873e+00, -5.0000e+08],\n",
            "          [ 6.5826e-01,  1.2099e+00,  8.7327e-01]],\n",
            "\n",
            "         [[ 4.2750e-01, -5.0000e+08, -5.0000e+08],\n",
            "          [ 7.8130e-01,  1.7253e+00, -5.0000e+08],\n",
            "          [ 5.7495e-01,  1.2638e+00,  8.1008e-01]],\n",
            "\n",
            "         [[ 4.2324e-01, -5.0000e+08, -5.0000e+08],\n",
            "          [ 8.1577e-01,  1.7747e+00, -5.0000e+08],\n",
            "          [ 4.8404e-01,  1.0529e+00,  6.6404e-01]],\n",
            "\n",
            "         [[ 4.8403e-01, -5.0000e+08, -5.0000e+08],\n",
            "          [ 1.0273e+00,  2.0876e+00, -5.0000e+08],\n",
            "          [ 5.7004e-01,  1.1324e+00,  8.7319e-01]],\n",
            "\n",
            "         [[ 4.7622e-01, -5.0000e+08, -5.0000e+08],\n",
            "          [ 9.0877e-01,  1.8489e+00, -5.0000e+08],\n",
            "          [ 5.4667e-01,  1.1059e+00,  6.6109e-01]]]], device='cuda:0',\n",
            "       grad_fn=<DivBackward0>)\n",
            "softmax_scores: tensor([[[[1.1111, 0.0000, 0.0000],\n",
            "          [0.4039, 0.7072, 0.0000],\n",
            "          [0.3522, 0.7590, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [0.4170, 0.6941, 0.0000],\n",
            "          [0.4064, 0.7047, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [0.4724, 0.6387, 0.0000],\n",
            "          [0.4603, 0.6508, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [0.3437, 0.7674, 0.0000],\n",
            "          [0.3148, 0.7963, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [0.4096, 0.7015, 0.0000],\n",
            "          [0.4575, 0.6536, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [0.4456, 0.6655, 0.0000],\n",
            "          [0.4159, 0.6952, 0.0000]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000],\n",
            "          [0.3969, 0.7142, 0.0000],\n",
            "          [0.3755, 0.7356, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [0.3675, 0.7436, 0.0000],\n",
            "          [0.3945, 0.7166, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.0000, 0.0000, 0.0000],\n",
            "          [0.6615, 0.4497, 0.0000],\n",
            "          [0.6205, 0.4906, 0.0000]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.4591, 0.0000],\n",
            "          [0.6139, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [0.6005, 0.5106, 0.0000],\n",
            "          [0.5826, 0.5285, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [0.0000, 0.4454, 0.0000],\n",
            "          [0.6218, 0.4893, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [0.6144, 0.4967, 0.0000],\n",
            "          [0.5901, 0.5210, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [0.6190, 0.4921, 0.0000],\n",
            "          [0.5943, 0.5169, 0.0000]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000],\n",
            "          [0.6036, 0.5075, 0.0000],\n",
            "          [0.5845, 0.5266, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [0.6580, 0.4531, 0.0000],\n",
            "          [0.6160, 0.4951, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[1.1111, 0.0000, 0.0000],\n",
            "          [1.1111, 0.0000, 0.0000],\n",
            "          [1.1111, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [1.1111, 0.0000, 0.0000],\n",
            "          [1.1111, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [1.1111, 0.0000, 0.0000],\n",
            "          [1.1111, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [1.1111, 0.0000, 0.0000],\n",
            "          [1.1111, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000],\n",
            "          [1.1111, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [1.1111, 0.0000, 0.0000],\n",
            "          [1.1111, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [1.1111, 0.0000, 0.0000],\n",
            "          [1.1111, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [1.1111, 0.0000, 0.0000],\n",
            "          [1.1111, 0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[1.1111, 0.0000, 0.0000],\n",
            "          [0.2301, 0.8810, 0.0000],\n",
            "          [0.2345, 0.5788, 0.2977]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [0.3599, 0.0000, 0.0000],\n",
            "          [0.3041, 0.4896, 0.3174]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [0.3792, 0.7319, 0.0000],\n",
            "          [0.3078, 0.4518, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [0.3197, 0.7914, 0.0000],\n",
            "          [0.2795, 0.4852, 0.0000]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [0.3112, 0.7999, 0.0000],\n",
            "          [0.0000, 0.5199, 0.3302]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [0.0000, 0.8032, 0.0000],\n",
            "          [0.2803, 0.4951, 0.3356]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [0.2858, 0.8253, 0.0000],\n",
            "          [0.2704, 0.4745, 0.3662]],\n",
            "\n",
            "         [[1.1111, 0.0000, 0.0000],\n",
            "          [0.3121, 0.7990, 0.0000],\n",
            "          [0.2871, 0.5022, 0.0000]]]], device='cuda:0',\n",
            "       grad_fn=<NativeDropoutBackward0>)\n",
            "softmax_socres shape: torch.Size([4, 8, 3, 3])\n",
            "values: tensor([[[[0.7152, 0.9848],\n",
            "          [1.0479, 1.2511],\n",
            "          [1.1167, 1.5589]],\n",
            "\n",
            "         [[0.4957, 0.6030],\n",
            "          [0.8720, 1.1114],\n",
            "          [0.5333, 1.1611]],\n",
            "\n",
            "         [[0.5664, 1.0078],\n",
            "          [1.0194, 1.5073],\n",
            "          [0.6463, 1.7179]],\n",
            "\n",
            "         [[1.0600, 0.8251],\n",
            "          [1.5713, 0.9703],\n",
            "          [1.6598, 1.5885]],\n",
            "\n",
            "         [[0.8740, 0.8636],\n",
            "          [1.1552, 1.0517],\n",
            "          [1.3910, 1.1983]],\n",
            "\n",
            "         [[0.8556, 0.4282],\n",
            "          [0.9728, 0.8037],\n",
            "          [1.3823, 0.8536]],\n",
            "\n",
            "         [[0.7551, 0.2786],\n",
            "          [1.1686, 0.3334],\n",
            "          [1.3327, 0.3095]],\n",
            "\n",
            "         [[1.0342, 1.1618],\n",
            "          [1.5670, 1.6724],\n",
            "          [1.3047, 1.6776]]],\n",
            "\n",
            "\n",
            "        [[[1.1495, 1.4295],\n",
            "          [1.0202, 1.3768],\n",
            "          [0.6037, 0.8082]],\n",
            "\n",
            "         [[0.8542, 1.1686],\n",
            "          [0.6110, 1.0761],\n",
            "          [0.3642, 0.6302]],\n",
            "\n",
            "         [[1.0154, 1.6972],\n",
            "          [0.7099, 1.5031],\n",
            "          [0.4280, 0.8968]],\n",
            "\n",
            "         [[1.7123, 1.2527],\n",
            "          [1.5272, 1.2326],\n",
            "          [0.9016, 0.7400]],\n",
            "\n",
            "         [[1.3198, 1.1598],\n",
            "          [1.2211, 1.1236],\n",
            "          [0.7227, 0.6518]],\n",
            "\n",
            "         [[1.2227, 0.8712],\n",
            "          [1.1086, 0.7571],\n",
            "          [0.6707, 0.4518]],\n",
            "\n",
            "         [[1.3035, 0.3576],\n",
            "          [1.1727, 0.3111],\n",
            "          [0.6968, 0.1819]],\n",
            "\n",
            "         [[1.6167, 1.8547],\n",
            "          [1.3318, 1.5243],\n",
            "          [0.7844, 0.9165]]],\n",
            "\n",
            "\n",
            "        [[[0.8156, 1.1618],\n",
            "          [1.4933, 1.7584],\n",
            "          [0.3400, 0.3988]],\n",
            "\n",
            "         [[0.4365, 0.9495],\n",
            "          [1.0914, 1.8792],\n",
            "          [0.2446, 0.2833]],\n",
            "\n",
            "         [[0.4669, 1.1495],\n",
            "          [1.2944, 2.2395],\n",
            "          [0.3254, 0.5585]],\n",
            "\n",
            "         [[1.2408, 0.9189],\n",
            "          [2.2598, 1.4811],\n",
            "          [0.4900, 0.4873]],\n",
            "\n",
            "         [[0.9770, 1.0007],\n",
            "          [1.6189, 1.3604],\n",
            "          [0.4064, 0.2663]],\n",
            "\n",
            "         [[0.7519, 0.5933],\n",
            "          [1.2447, 1.3428],\n",
            "          [0.5004, 0.2737]],\n",
            "\n",
            "         [[0.9290, 0.2534],\n",
            "          [1.7984, 0.3562],\n",
            "          [0.4065, 0.0929]],\n",
            "\n",
            "         [[1.0635, 1.0666],\n",
            "          [2.0215, 2.1909],\n",
            "          [0.4316, 0.6498]]],\n",
            "\n",
            "\n",
            "        [[[0.4835, 0.4970],\n",
            "          [0.9660, 1.2330],\n",
            "          [0.7089, 0.8731]],\n",
            "\n",
            "         [[0.3842, 0.6565],\n",
            "          [0.5747, 1.1313],\n",
            "          [0.3575, 1.0887]],\n",
            "\n",
            "         [[0.4816, 0.7716],\n",
            "          [0.6940, 1.4826],\n",
            "          [0.4274, 1.1145]],\n",
            "\n",
            "         [[0.7288, 0.4806],\n",
            "          [1.4488, 1.1824],\n",
            "          [1.0892, 0.8165]],\n",
            "\n",
            "         [[0.4945, 0.3253],\n",
            "          [1.1219, 0.9320],\n",
            "          [0.7759, 0.6112]],\n",
            "\n",
            "         [[0.3912, 0.5072],\n",
            "          [0.9996, 0.8258],\n",
            "          [0.5138, 0.7470]],\n",
            "\n",
            "         [[0.6200, 0.0793],\n",
            "          [1.1729, 0.2361],\n",
            "          [0.9401, 0.0956]],\n",
            "\n",
            "         [[0.6319, 0.7482],\n",
            "          [1.1970, 1.4313],\n",
            "          [0.7854, 0.8640]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "v_out shape torch.Size([4, 8, 3, 2])\n",
            "v_out: tensor([[[[0.7946, 1.0942],\n",
            "          [1.0299, 1.2825],\n",
            "          [1.0472, 1.2963]],\n",
            "\n",
            "         [[0.5507, 0.6700],\n",
            "          [0.8119, 1.0228],\n",
            "          [0.8159, 1.0282]],\n",
            "\n",
            "         [[0.6293, 1.1197],\n",
            "          [0.9187, 1.4388],\n",
            "          [0.9241, 1.4449]],\n",
            "\n",
            "         [[1.1778, 0.9168],\n",
            "          [1.5702, 1.0282],\n",
            "          [1.5849, 1.0324]],\n",
            "\n",
            "         [[0.9712, 0.9595],\n",
            "          [1.1684, 1.0915],\n",
            "          [1.1549, 1.0825]],\n",
            "\n",
            "         [[0.9507, 0.4757],\n",
            "          [1.0287, 0.7256],\n",
            "          [1.0322, 0.7368]],\n",
            "\n",
            "         [[0.0000, 0.0000],\n",
            "          [1.1344, 0.3487],\n",
            "          [1.1432, 0.3499]],\n",
            "\n",
            "         [[1.1491, 1.2909],\n",
            "          [1.5454, 1.6706],\n",
            "          [1.5309, 1.6568]]],\n",
            "\n",
            "\n",
            "        [[[0.0000, 0.0000],\n",
            "          [1.2191, 1.5646],\n",
            "          [1.2138, 1.5625]],\n",
            "\n",
            "         [[0.0000, 0.0000],\n",
            "          [0.2805, 0.4940],\n",
            "          [0.5244, 0.7174]],\n",
            "\n",
            "         [[1.1282, 1.8858],\n",
            "          [0.9722, 1.7867],\n",
            "          [0.9667, 1.7832]],\n",
            "\n",
            "         [[1.9026, 1.3919],\n",
            "          [0.6801, 0.5490],\n",
            "          [1.8120, 1.3821]],\n",
            "\n",
            "         [[1.4664, 1.2887],\n",
            "          [1.4174, 1.2707],\n",
            "          [1.4150, 1.2698]],\n",
            "\n",
            "         [[1.3586, 0.9680],\n",
            "          [1.3024, 0.9118],\n",
            "          [1.2996, 0.9090]],\n",
            "\n",
            "         [[0.0000, 0.0000],\n",
            "          [1.3819, 0.3737],\n",
            "          [1.3794, 0.3728]],\n",
            "\n",
            "         [[1.7963, 2.0608],\n",
            "          [1.6672, 1.9111],\n",
            "          [1.6553, 1.8972]]],\n",
            "\n",
            "\n",
            "        [[[0.9062, 1.2909],\n",
            "          [0.9062, 1.2909],\n",
            "          [0.9062, 1.2909]],\n",
            "\n",
            "         [[0.4850, 1.0550],\n",
            "          [0.4850, 1.0550],\n",
            "          [0.4850, 1.0550]],\n",
            "\n",
            "         [[0.5188, 1.2772],\n",
            "          [0.5188, 1.2772],\n",
            "          [0.5188, 1.2772]],\n",
            "\n",
            "         [[1.3787, 1.0210],\n",
            "          [1.3787, 1.0210],\n",
            "          [1.3787, 1.0210]],\n",
            "\n",
            "         [[1.0856, 1.1119],\n",
            "          [0.0000, 0.0000],\n",
            "          [1.0856, 1.1119]],\n",
            "\n",
            "         [[0.8355, 0.6592],\n",
            "          [0.8355, 0.6592],\n",
            "          [0.8355, 0.6592]],\n",
            "\n",
            "         [[1.0323, 0.2815],\n",
            "          [1.0323, 0.2815],\n",
            "          [1.0323, 0.2815]],\n",
            "\n",
            "         [[1.1816, 1.1851],\n",
            "          [1.1816, 1.1851],\n",
            "          [1.1816, 1.1851]]],\n",
            "\n",
            "\n",
            "        [[[0.5372, 0.5523],\n",
            "          [0.9623, 1.2006],\n",
            "          [0.8836, 1.0902]],\n",
            "\n",
            "         [[0.4269, 0.7294],\n",
            "          [0.1383, 0.2363],\n",
            "          [0.5117, 1.0991]],\n",
            "\n",
            "         [[0.5351, 0.8573],\n",
            "          [0.6906, 1.3777],\n",
            "          [0.4618, 0.9073]],\n",
            "\n",
            "         [[0.8097, 0.5340],\n",
            "          [1.3796, 1.0894],\n",
            "          [0.9066, 0.7079]],\n",
            "\n",
            "         [[0.5494, 0.3614],\n",
            "          [1.0513, 0.8467],\n",
            "          [0.8394, 0.6863]],\n",
            "\n",
            "         [[0.4347, 0.5635],\n",
            "          [0.8029, 0.6633],\n",
            "          [0.7771, 0.8017]],\n",
            "\n",
            "         [[0.6889, 0.0881],\n",
            "          [1.1452, 0.2176],\n",
            "          [1.0684, 0.1685]],\n",
            "\n",
            "         [[0.7021, 0.8314],\n",
            "          [1.1536, 1.3772],\n",
            "          [0.7825, 0.9335]]]], device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
            "v_out shape before: torch.Size([4, 8, 3, 2])\n",
            "v_out shape: torch.Size([4, 3, 16])\n",
            "weight_out shape: torch.Size([16, 4])\n",
            "v_out reshaped: tensor([[[0.7946, 1.0942, 1.0299, 1.2825, 1.0472, 1.2963, 0.5507, 0.6700,\n",
            "          0.8119, 1.0228, 0.8159, 1.0282, 0.6293, 1.1197, 0.9187, 1.4388],\n",
            "         [0.9241, 1.4449, 1.1778, 0.9168, 1.5702, 1.0282, 1.5849, 1.0324,\n",
            "          0.9712, 0.9595, 1.1684, 1.0915, 1.1549, 1.0825, 0.9507, 0.4757],\n",
            "         [1.0287, 0.7256, 1.0322, 0.7368, 0.0000, 0.0000, 1.1344, 0.3487,\n",
            "          1.1432, 0.3499, 1.1491, 1.2909, 1.5454, 1.6706, 1.5309, 1.6568]],\n",
            "\n",
            "        [[0.0000, 0.0000, 1.2191, 1.5646, 1.2138, 1.5625, 0.0000, 0.0000,\n",
            "          0.2805, 0.4940, 0.5244, 0.7174, 1.1282, 1.8858, 0.9722, 1.7867],\n",
            "         [0.9667, 1.7832, 1.9026, 1.3919, 0.6801, 0.5490, 1.8120, 1.3821,\n",
            "          1.4664, 1.2887, 1.4174, 1.2707, 1.4150, 1.2698, 1.3586, 0.9680],\n",
            "         [1.3024, 0.9118, 1.2996, 0.9090, 0.0000, 0.0000, 1.3819, 0.3737,\n",
            "          1.3794, 0.3728, 1.7963, 2.0608, 1.6672, 1.9111, 1.6553, 1.8972]],\n",
            "\n",
            "        [[0.9062, 1.2909, 0.9062, 1.2909, 0.9062, 1.2909, 0.4850, 1.0550,\n",
            "          0.4850, 1.0550, 0.4850, 1.0550, 0.5188, 1.2772, 0.5188, 1.2772],\n",
            "         [0.5188, 1.2772, 1.3787, 1.0210, 1.3787, 1.0210, 1.3787, 1.0210,\n",
            "          1.0856, 1.1119, 0.0000, 0.0000, 1.0856, 1.1119, 0.8355, 0.6592],\n",
            "         [0.8355, 0.6592, 0.8355, 0.6592, 1.0323, 0.2815, 1.0323, 0.2815,\n",
            "          1.0323, 0.2815, 1.1816, 1.1851, 1.1816, 1.1851, 1.1816, 1.1851]],\n",
            "\n",
            "        [[0.5372, 0.5523, 0.9623, 1.2006, 0.8836, 1.0902, 0.4269, 0.7294,\n",
            "          0.1383, 0.2363, 0.5117, 1.0991, 0.5351, 0.8573, 0.6906, 1.3777],\n",
            "         [0.4618, 0.9073, 0.8097, 0.5340, 1.3796, 1.0894, 0.9066, 0.7079,\n",
            "          0.5494, 0.3614, 1.0513, 0.8467, 0.8394, 0.6863, 0.4347, 0.5635],\n",
            "         [0.8029, 0.6633, 0.7771, 0.8017, 0.6889, 0.0881, 1.1452, 0.2176,\n",
            "          1.0684, 0.1685, 0.7021, 0.8314, 1.1536, 1.3772, 0.7825, 0.9335]]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "output shape torch.Size([4, 3, 4])\n",
            "output: tensor([[[ 7.0446,  6.9045,  8.5649,  7.4094],\n",
            "         [ 7.6841,  7.0774, 10.0812,  7.9157],\n",
            "         [ 6.3608,  5.8980,  9.1244,  8.3133]],\n",
            "\n",
            "        [[ 6.3278,  6.1016,  7.3412,  7.0152],\n",
            "         [ 8.8475,  8.7930, 11.8639, 10.3288],\n",
            "         [ 7.7382,  7.2657, 11.0501, 10.1387]],\n",
            "\n",
            "        [[ 6.6097,  6.7584,  7.9197,  6.8207],\n",
            "         [ 6.9380,  6.5421,  8.9090,  7.0794],\n",
            "         [ 5.9688,  5.2494,  8.4694,  7.0818]],\n",
            "\n",
            "        [[ 5.2984,  5.0194,  6.4664,  5.8392],\n",
            "         [ 5.3606,  4.6680,  6.7041,  5.1269],\n",
            "         [ 5.4978,  4.6541,  7.5833,  6.3734]]], device='cuda:0',\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, feature_length):\n",
        "        super().__init__()\n",
        "        self.scale = nn.Parameter(torch.ones(feature_length))\n",
        "        self.offset = nn.Parameter(torch.zeros(feature_length))\n",
        "\n",
        "    def forward(self, activations):\n",
        "        mean = torch.mean(activations, -1, keepdim=True)\n",
        "        #print(\"mean:\", mean)\n",
        "        #print(\"activations - mean\", activations - mean)\n",
        "        variance = torch.var(activations, -1, keepdim=True, unbiased=False)\n",
        "        normalized_activations = (activations - mean) / torch.sqrt(variance + 1e-6)\n",
        "        return (normalized_activations * self.scale) + self.offset"
      ],
      "metadata": {
        "id": "-Si4-i6PTlFu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_layer_norm():\n",
        "    feature_length = 4\n",
        "    length_x = 3\n",
        "    batch_size = 5\n",
        "    layer_norm = LayerNorm(feature_length)\n",
        "\n",
        "    activations = torch.rand(batch_size, length_x, feature_length)\n",
        "\n",
        "    print(\"activations:\", activations)\n",
        "    print(\"layer_normed:\", layer_norm(activations))\n",
        "    assert layer_norm(activations).shape == activations.shape\n",
        "\n",
        "test_layer_norm()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyzgWHqrWXL4",
        "outputId": "5aa7af91-d7ab-43c8-f6bb-890c13cd1cee"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "activations: tensor([[[0.9585, 0.8650, 0.0683, 0.0543],\n",
            "         [0.5648, 0.4272, 0.2048, 0.5931],\n",
            "         [0.2845, 0.4615, 0.7539, 0.5990]],\n",
            "\n",
            "        [[0.1542, 0.8416, 0.1384, 0.8494],\n",
            "         [0.0991, 0.7173, 0.6886, 0.2655],\n",
            "         [0.8548, 0.9399, 0.1831, 0.6274]],\n",
            "\n",
            "        [[0.1678, 0.5979, 0.4609, 0.9057],\n",
            "         [0.7310, 0.5347, 0.3886, 0.9706],\n",
            "         [0.3451, 0.1739, 0.4810, 0.4873]],\n",
            "\n",
            "        [[0.2569, 0.6881, 0.7672, 0.9335],\n",
            "         [0.6350, 0.1567, 0.7177, 0.5660],\n",
            "         [0.8676, 0.5293, 0.5051, 0.2700]],\n",
            "\n",
            "        [[0.7937, 0.1208, 0.8246, 0.1129],\n",
            "         [0.9325, 0.3642, 0.3760, 0.2328],\n",
            "         [0.2098, 0.6366, 0.3749, 0.1389]]])\n",
            "layer_normed: tensor([[[ 1.1065,  0.8873, -0.9804, -1.0134],\n",
            "         [ 0.7645, -0.1323, -1.5806,  0.9485],\n",
            "         [-1.3885, -0.3652,  1.3244,  0.4293]],\n",
            "\n",
            "        [[-0.9773,  0.9887, -1.0224,  1.0110],\n",
            "         [-1.2861,  1.0284,  0.9210, -0.6633],\n",
            "         [ 0.6936,  0.9833, -1.5954, -0.0815]],\n",
            "\n",
            "        [[-1.3766,  0.2443, -0.2718,  1.4041],\n",
            "         [ 0.3424, -0.5563, -1.2254,  1.4393],\n",
            "         [-0.2093, -1.5510,  0.8555,  0.9047]],\n",
            "\n",
            "        [[-1.6195,  0.1067,  0.4235,  1.0892],\n",
            "         [ 0.5382, -1.6776,  0.9210,  0.2185],\n",
            "         [ 1.5237, -0.0642, -0.1778, -1.2817]],\n",
            "\n",
            "        [[ 0.9548, -0.9881,  1.0442, -1.0109],\n",
            "         [ 1.6939, -0.4165, -0.3728, -0.9046],\n",
            "         [-0.6807,  1.5491,  0.1822, -1.0506]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, hiddenLayerWidth, d_e, p_dropout):\n",
        "        super().__init__()\n",
        "        self.mlp1 = nn.Parameter(torch.rand(d_e, hiddenLayerWidth))\n",
        "        self.mlp2 = nn.Parameter(torch.rand(hiddenLayerWidth, d_e))\n",
        "        self.mlp1_bias = nn.Parameter(torch.zeros(hiddenLayerWidth))\n",
        "        self.mlp2_bias = nn.Parameter(torch.zeros(d_e))\n",
        "        self.dropout = nn.Dropout(p_dropout)\n",
        "\n",
        "    def forward(self, activations):\n",
        "        activations = torch.matmul(activations, self.mlp1) + self.mlp1_bias\n",
        "        activations = activations.relu()\n",
        "        activations = torch.matmul(activations, self.mlp2) + self.mlp2_bias\n",
        "        activations = self.dropout(activations)\n",
        "        return activations\n"
      ],
      "metadata": {
        "id": "X7rAEAkFoNI5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_feed_forward():\n",
        "    hiddenLayerWidth = 3\n",
        "    d_e = 4\n",
        "    feed_forward = FeedForward(hiddenLayerWidth, d_e, 0.1)\n",
        "    activations = torch.rand(10, 5, d_e)\n",
        "\n",
        "    print(\"activations:\", activations)\n",
        "    output = feed_forward(activations)\n",
        "    print(\"feed forward:\", output)\n",
        "    assert output.shape == activations.shape\n",
        "\n",
        "test_feed_forward()"
      ],
      "metadata": {
        "id": "mLQj6GjmUFN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e5dfa41-1ae8-4853-fbe2-d5f66f6563f9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "activations: tensor([[[0.7162, 0.8391, 0.7896, 0.7189],\n",
            "         [0.4911, 0.8424, 0.8687, 0.0647],\n",
            "         [0.5847, 0.6227, 0.4764, 0.1564],\n",
            "         [0.2567, 0.7449, 0.0281, 0.6179],\n",
            "         [0.3166, 0.7580, 0.0795, 0.9892]],\n",
            "\n",
            "        [[0.3658, 0.3713, 0.9095, 0.2346],\n",
            "         [0.4220, 0.1864, 0.4479, 0.7047],\n",
            "         [0.4035, 0.5706, 0.9836, 0.7179],\n",
            "         [0.6039, 0.7702, 0.2363, 0.8699],\n",
            "         [0.3465, 0.5148, 0.4237, 0.1254]],\n",
            "\n",
            "        [[0.5822, 0.1521, 0.6643, 0.8174],\n",
            "         [0.0329, 0.2912, 0.9359, 0.8245],\n",
            "         [0.2528, 0.2244, 0.9205, 0.2973],\n",
            "         [0.2273, 0.3442, 0.5416, 0.2200],\n",
            "         [0.6495, 0.2182, 0.4325, 0.9734]],\n",
            "\n",
            "        [[0.3090, 0.1305, 0.4369, 0.2272],\n",
            "         [0.3768, 0.0799, 0.2608, 0.3879],\n",
            "         [0.5184, 0.3851, 0.2945, 0.7441],\n",
            "         [0.4496, 0.0372, 0.6992, 0.9133],\n",
            "         [0.5403, 0.3095, 0.7622, 0.7765]],\n",
            "\n",
            "        [[0.7340, 0.5315, 0.3123, 0.1216],\n",
            "         [0.5905, 0.7827, 0.5246, 0.2818],\n",
            "         [0.3251, 0.9486, 0.7465, 0.8361],\n",
            "         [0.2152, 0.9253, 0.1532, 0.9265],\n",
            "         [0.2236, 0.7171, 0.8762, 0.0173]],\n",
            "\n",
            "        [[0.8524, 0.3550, 0.3433, 0.9243],\n",
            "         [0.8896, 0.5362, 0.7230, 0.5878],\n",
            "         [0.9095, 0.1898, 0.3825, 0.5456],\n",
            "         [0.3005, 0.6267, 0.9540, 0.6107],\n",
            "         [0.5523, 0.0066, 0.2676, 0.8121]],\n",
            "\n",
            "        [[0.9950, 0.5809, 0.3101, 0.2751],\n",
            "         [0.7424, 0.8780, 0.5843, 0.2509],\n",
            "         [0.9813, 0.0220, 0.9977, 0.5921],\n",
            "         [0.8650, 0.0410, 0.4478, 0.3561],\n",
            "         [0.9486, 0.0837, 0.5711, 0.3485]],\n",
            "\n",
            "        [[0.4403, 0.4605, 0.8871, 0.5350],\n",
            "         [0.1355, 0.8206, 0.3544, 0.7126],\n",
            "         [0.6834, 0.8336, 0.2763, 0.3298],\n",
            "         [0.4444, 0.8771, 0.9834, 0.1246],\n",
            "         [0.5706, 0.4069, 0.6391, 0.0773]],\n",
            "\n",
            "        [[0.1389, 0.9018, 0.4024, 0.0963],\n",
            "         [0.7001, 0.1936, 0.8411, 0.3009],\n",
            "         [0.3786, 0.3271, 0.6998, 0.0040],\n",
            "         [0.6517, 0.6713, 0.5993, 0.2971],\n",
            "         [0.9258, 0.1954, 0.3357, 0.1849]],\n",
            "\n",
            "        [[0.7486, 0.9722, 0.1511, 0.8022],\n",
            "         [0.8970, 0.7180, 0.6911, 0.3254],\n",
            "         [0.2980, 0.8007, 0.2312, 0.7990],\n",
            "         [0.9744, 0.0737, 0.7119, 0.0933],\n",
            "         [0.4882, 0.1985, 0.4847, 0.8580]]])\n",
            "feed forward: tensor([[[3.4194, 2.5743, 2.5988, 1.7923],\n",
            "         [2.8555, 2.1619, 2.1881, 1.4603],\n",
            "         [2.3472, 1.7681, 1.7770, 1.2803],\n",
            "         [1.5843, 1.1892, 1.1950, 0.8654],\n",
            "         [1.9157, 1.4348, 0.0000, 1.0266]],\n",
            "\n",
            "        [[2.2165, 0.0000, 1.7073, 1.0750],\n",
            "         [1.7578, 1.3173, 1.3321, 0.9077],\n",
            "         [2.7950, 2.1107, 2.1484, 1.3534],\n",
            "         [2.5625, 1.9202, 1.9279, 1.4099],\n",
            "         [1.7360, 1.3109, 1.3227, 0.9140]],\n",
            "\n",
            "        [[2.2954, 0.0000, 1.7409, 1.1818],\n",
            "         [1.8200, 0.0000, 0.0000, 0.7562],\n",
            "         [1.8908, 1.4327, 0.0000, 0.8712],\n",
            "         [1.5100, 1.1423, 1.1613, 0.7398],\n",
            "         [2.2818, 1.7049, 0.0000, 1.2262]],\n",
            "\n",
            "        [[1.2886, 0.9703, 0.9822, 0.6590],\n",
            "         [1.2092, 0.9044, 0.9093, 0.6572],\n",
            "         [1.9898, 1.4892, 0.0000, 1.0804],\n",
            "         [2.0137, 1.5101, 1.5347, 0.9910],\n",
            "         [2.4999, 1.8793, 1.9044, 1.2652]],\n",
            "\n",
            "        [[2.2965, 1.7240, 1.7222, 1.3190],\n",
            "         [2.6480, 1.9955, 2.0083, 1.4266],\n",
            "         [2.8631, 2.1620, 2.1964, 1.4130],\n",
            "         [1.9910, 1.4968, 1.5126, 0.0000],\n",
            "         [2.2369, 1.7006, 1.7324, 1.0728]],\n",
            "\n",
            "        [[2.6664, 1.9910, 1.9946, 1.4954],\n",
            "         [3.2425, 2.4353, 2.4498, 1.7547],\n",
            "         [2.4667, 1.8425, 1.8412, 1.4126],\n",
            "         [2.6016, 1.9682, 2.0066, 1.2391],\n",
            "         [1.6125, 1.1999, 1.2047, 0.8877]],\n",
            "\n",
            "        [[2.8668, 2.1474, 2.1409, 1.6728],\n",
            "         [3.0777, 2.3184, 2.3300, 1.6790],\n",
            "         [0.0000, 2.3528, 2.3721, 1.6638],\n",
            "         [2.2176, 1.6574, 1.6567, 1.2668],\n",
            "         [2.5522, 1.9102, 1.9122, 1.4401]],\n",
            "\n",
            "        [[2.5452, 1.9212, 1.9518, 1.2559],\n",
            "         [1.8795, 1.4187, 1.4411, 0.9288],\n",
            "         [2.5977, 1.9519, 1.9540, 1.4656],\n",
            "         [2.9724, 2.2524, 0.0000, 1.4874],\n",
            "         [2.2330, 1.6838, 1.6969, 1.1886]],\n",
            "\n",
            "        [[1.7811, 1.3523, 1.3705, 0.8990],\n",
            "         [2.5415, 1.9127, 0.0000, 1.3308],\n",
            "         [1.8489, 1.3992, 1.4181, 0.9338],\n",
            "         [0.0000, 2.0488, 2.0623, 1.4631],\n",
            "         [2.2988, 0.0000, 1.7116, 1.3538]],\n",
            "\n",
            "        [[2.9177, 0.0000, 2.1877, 1.6510],\n",
            "         [3.3182, 2.4960, 2.5075, 1.8163],\n",
            "         [2.0310, 1.5267, 1.5413, 1.0644],\n",
            "         [2.6449, 1.9843, 1.9882, 1.4808],\n",
            "         [1.9925, 1.4920, 1.5081, 1.0325]]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.multi_head_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], p_dropout, verbose)\n",
        "        self.layer_norm1 = LayerNorm(d_z)\n",
        "        self.feed_forward = FeedForward(d_mlp, d_z, p_dropout)\n",
        "        self.layer_norm2 = LayerNorm(d_z)\n",
        "\n",
        "    def forward(self, z, padding_mask):\n",
        "        z = self.layer_norm1(z)\n",
        "        z = z + self.multi_head_attention(z, z, padding_mask)\n",
        "        z = self.layer_norm2(z)\n",
        "        z = z + self.feed_forward(z)\n",
        "        return z"
      ],
      "metadata": {
        "id": "ikM15oD-qghT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        for i in range(num_layers):\n",
        "            encoder_layer = EncoderLayer(num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose)\n",
        "            self.layers.append(encoder_layer)\n",
        "        self.layers = nn.ModuleList(self.layers)\n",
        "        self.final_norm = LayerNorm(d_z)\n",
        "\n",
        "    def forward(self, z, padding_mask):\n",
        "        for layer in self.layers:\n",
        "            z = layer(z, padding_mask)\n",
        "        return self.final_norm(z)"
      ],
      "metadata": {
        "id": "zKGc6Xwr46Vh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.multi_head_self_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['MASKED'], p_dropout, verbose)\n",
        "        self.layer_norm1 = LayerNorm(d_x)\n",
        "        self.multi_head_global_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], p_dropout, verbose)\n",
        "        self.layer_norm2 = LayerNorm(d_x)\n",
        "        self.feed_forward = FeedForward(d_mlp, d_x, p_dropout)\n",
        "        self.layer_norm3 = LayerNorm(d_x)\n",
        "\n",
        "    def forward(self, z, x, src_mask, tgt_mask):\n",
        "        x = self.layer_norm1(x)\n",
        "        x = x + self.multi_head_self_attention(x, x, tgt_mask)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = x + self.multi_head_global_attention(z, x, src_mask)\n",
        "        x = self.layer_norm3(x)\n",
        "        x = x + self.feed_forward(x)\n",
        "        return x\n",
        "\n",
        "    def disable_subsequent_mask(self):\n",
        "        self.multi_head_self_attention.disable_subsequent_mask()"
      ],
      "metadata": {
        "id": "GSqElGm56xaK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        for i in range(num_layers):\n",
        "            decoder_layer = DecoderLayer(num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose)\n",
        "            self.layers.append(decoder_layer)\n",
        "        self.layers = nn.ModuleList(self.layers)\n",
        "        self.final_norm = LayerNorm(d_x)\n",
        "\n",
        "    def forward(self, z, x, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(z, x, src_mask, tgt_mask)\n",
        "        return self.final_norm(x)\n",
        "\n",
        "    def disable_subsequent_mask(self):\n",
        "        for layer in self.layers:\n",
        "            layer.multi_head_self_attention.disable_subsequent_mask()"
      ],
      "metadata": {
        "id": "LGX007WN8Bw6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoderTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, d_e, vocab_size, max_sequence_length, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.src_embedding = Embedding(vocab_size, d_e)\n",
        "        self.tgt_embedding = Embedding(vocab_size, d_e)\n",
        "        self.unembedding = Unembedding(vocab_size, d_e)\n",
        "        self.embedding_dropout = nn.Dropout(p_dropout)\n",
        "        self.positionalEmbedding = PositionalEmbedding(d_e, max_sequence_length)\n",
        "        self.encoder = Encoder(num_encoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose)\n",
        "        self.decoder = Decoder(num_decoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose)\n",
        "\n",
        "    def forward(self, z, x, src_mask, tgt_mask):\n",
        "        z = self.src_embedding(z) + self.positionalEmbedding(z)\n",
        "        z = self.embedding_dropout(z)\n",
        "        z = self.encoder(z, src_mask)\n",
        "        x = self.tgt_embedding(x) + self.positionalEmbedding(x)\n",
        "        x = self.decoder(z, x, src_mask, tgt_mask)\n",
        "        #print(\"x after decoder:\", x.shape)\n",
        "        x = self.unembedding(x)\n",
        "        #print(\"x after unembedding:\", x.shape)\n",
        "        return x\n",
        "\n",
        "    def disable_subsequent_mask(self):\n",
        "        self.decoder.disable_subsequent_mask()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ixXJDrPF8RU9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enRawName = \"drive/MyDrive/colab data/multi30kEnTrain.txt\"\n",
        "deRawName = \"drive/MyDrive/colab data/multi30kDeTrain.txt\"\n",
        "en30kVal = \"drive/MyDrive/colab data/multi30kEnVal.txt\"\n",
        "de30kVal = \"drive/MyDrive/colab data/multi30kDeVal.txt\"\n",
        "englishCleanName = \"data/english_tokens.pkl\"\n",
        "germanCleanName = \"data/german_tokens.pkl\"\n",
        "englishSortedName = \"data/englishSorted.pkl\"\n",
        "germanSortedName = \"data/germanSorted.pkl\"\n",
        "\n",
        "truncEn = \"drive/MyDrive/colab data/truncEn.pkl\"\n",
        "truncDe = \"drive/MyDrive/colab data/truncDe.pkl\"\n",
        "\n",
        "enTokenizerName = \"drive/MyDrive/colab data/enTokenizer.pkl\"\n",
        "deTokenizerName = \"drive/MyDrive/colab data/deTokenizer.pkl\"\n",
        "pairsName = \"drive/MyDrive/colab data/pairs.pkl\"\n",
        "folder = \"drive/MyDrive/colab data/\"\n",
        "\n",
        "enTrainingFileName = folder + \"enTraining\"\n",
        "deTrainingFileName = folder + \"deTraining\"\n",
        "enTestFileName = folder + \"enTest\"\n",
        "deTestFileName = folder + \"deTest\"\n",
        "enValFileName = folder + \"enValidation\"\n",
        "deValFileName = folder + \"deValidation\"\n",
        "\n",
        "enCombinedFileName = folder + \"enCombined\"\n",
        "deCombinedFileName = folder + \"deCombined\""
      ],
      "metadata": {
        "id": "NqAcQrmPg4y0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc6Y11QfhEAv",
        "outputId": "5d49f0e2-dcbb-436a-e499-bfd247885a8a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class SentenceDataset(Dataset):\n",
        "\n",
        "#     TOKENIZER_SUFFIX = \"_tokenizer\"\n",
        "#     BOS_TOKEN = \"[SOS]\"\n",
        "#     EOS_TOKEN = \"[EOS]\"\n",
        "#     PAD_TOKEN = \"[PAD]\"\n",
        "#     UNK_TOKEN = \"[UNK]\"\n",
        "\n",
        "#     def __init__(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, sequence_start_index, sequence_end_index):\n",
        "#         src_sequences = self.to_sequences(self.load_doc(src_filename), sequence_start_index, sequence_end_index)\n",
        "#         tgt_sequences = self.to_sequences(self.load_doc(tgt_filename), sequence_start_index, sequence_end_index)\n",
        "#         src_sequences = [self.add_special_tokens(sequence) for sequence in src_sequences]\n",
        "#         tgt_sequences = [self.add_special_tokens(sequence) for sequence in tgt_sequences]\n",
        "#         self.src_tokenizer, self.tgt_tokenizer = self.setup_tokenizers(src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, src_filename + SentenceDataset.TOKENIZER_SUFFIX, tgt_filename + SentenceDataset.TOKENIZER_SUFFIX)\n",
        "#         # src_tokenized = self.src_tokenizer.encode_batch(src_sequences)\n",
        "#         # tgt_tokenized = self.tgt_tokenizer.encode_batch(tgt_sequences)\n",
        "#         # src_tensors = [torch.IntTensor(sequence.ids) for sequence in src_tokenized]\n",
        "#         # tgt_tensor = [torch.IntTensor(sequence.ids) for sequence in tgt_tokenized]\n",
        "#         self.pairs = self.pair_sequences(src_sequences, tgt_sequences)\n",
        "#         #print(\"pairs\", self.pairs)\n",
        "\n",
        "#     # load doc into memory\n",
        "#     def load_doc(self, filename):\n",
        "#         # open the file as read only\n",
        "#         file = open(filename, mode='rt')\n",
        "#         # read all text\n",
        "#         text = file.read()\n",
        "#         # close the file\n",
        "#         file.close()\n",
        "#         return text\n",
        "\n",
        "#     def add_special_tokens(self, sequence):\n",
        "#         sequence = self.BOS_TOKEN + \" \" + sequence + \" \" + self.EOS_TOKEN\n",
        "#         return sequence\n",
        "\n",
        "#     def pair_sequences(self, src_sequences, tgt_sequences):\n",
        "#         paired_sequences = list(zip(src_sequences, tgt_sequences))\n",
        "#         sorted_pairs = sorted(paired_sequences, key=lambda x: len(x[0]))\n",
        "#         return sorted_pairs\n",
        "\n",
        "#     # split a loaded document into sequences\n",
        "#     def to_sequences(self, doc, sequence_start_index, sequence_end_index):\n",
        "#         sequences = doc.strip().split('\\n')\n",
        "#         return sequences[sequence_start_index:sequence_end_index]\n",
        "\n",
        "#     def setup_tokenizers(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, src_tokenizer_name, tgt_tokenizer_name):\n",
        "#         print(\"creating tokenizer for \" + src_filename)\n",
        "#         src_tokenizer = Tokenizer(BPE(unk_token=SentenceDataset.UNK_TOKEN))\n",
        "#         src_tokenizer.pre_tokenizer = Whitespace()\n",
        "#         # src_tokenizer.post_processor = TemplateProcessing(\n",
        "#         #     single=\"[BOS] $A [EOS]\",\n",
        "#         #     special_tokens=[(\"[BOS]\", 0), (\"[EOS]\", 1)],\n",
        "#         # )\n",
        "#         trainer = BpeTrainer(vocab_size = src_vocab_size, special_tokens=[SentenceDataset.BOS_TOKEN, SentenceDataset.EOS_TOKEN, SentenceDataset.PAD_TOKEN, SentenceDataset.UNK_TOKEN])\n",
        "#         src_tokenizer.train([src_filename], trainer=trainer)\n",
        "#         pickle.dump(src_tokenizer, open(src_tokenizer_name, \"wb\"))\n",
        "\n",
        "#         print(\"creating tokenizer for \" + tgt_filename)\n",
        "#         tgt_tokenizer = Tokenizer(BPE(unk_token=SentenceDataset.UNK_TOKEN))\n",
        "#         tgt_tokenizer.pre_tokenizer = Whitespace()\n",
        "#         trainer = BpeTrainer(vocab_size = tgt_vocab_size, special_tokens=[SentenceDataset.BOS_TOKEN, SentenceDataset.EOS_TOKEN, SentenceDataset.PAD_TOKEN, SentenceDataset.UNK_TOKEN])\n",
        "#         tgt_tokenizer.train([tgt_filename], trainer=trainer)\n",
        "#         pickle.dump(tgt_tokenizer, open(tgt_tokenizer_name, \"wb\"))\n",
        "#         return src_tokenizer, tgt_tokenizer\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.pairs)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         src_seq, tgt_seq = self.pairs[index]\n",
        "#         return src_seq, tgt_seq\n"
      ],
      "metadata": {
        "id": "PkCWRs4-khoT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SequencePairDataset(Dataset):\n",
        "    BOS_TOKEN = \"[SOS]\"\n",
        "    EOS_TOKEN = \"[EOS]\"\n",
        "    PAD_TOKEN = \"[PAD]\"\n",
        "    UNK_TOKEN = \"[UNK]\"\n",
        "    PAD_ID = 2\n",
        "\n",
        "    def __init__(self, src_text, tgt_text, start_index, end_index):\n",
        "        src_sequences = self.to_sequences(src_text, start_index, end_index)\n",
        "        #tgt_sequences = self.to_sequences(tgt_text, start_index, end_index)\n",
        "        tgt_sequences = self.to_sequences(src_text, start_index, end_index)\n",
        "        #src_sequences = [self.add_special_tokens(sequence) for sequence in src_sequences]\n",
        "        #tgt_sequences = [self.add_special_tokens(sequence) for sequence in tgt_sequences]\n",
        "        self.pairs = self.pair_sequences(src_sequences, tgt_sequences)\n",
        "\n",
        "    def pair_sequences(self, src_sequences, tgt_sequences):\n",
        "        paired_sequences = list(zip(src_sequences, tgt_sequences))\n",
        "        sorted_pairs = sorted(paired_sequences, key=lambda x: len(x[0]))\n",
        "        return sorted_pairs\n",
        "\n",
        "    # split a loaded document into sequences\n",
        "    def to_sequences(self, doc, sequence_start_index, sequence_end_index):\n",
        "        sequences = doc.strip().split('\\n')\n",
        "        return sequences[sequence_start_index : sequence_end_index]\n",
        "\n",
        "    def add_special_tokens(self, sequence):\n",
        "        sequence = self.BOS_TOKEN + \" \" + sequence + \" \" + self.EOS_TOKEN\n",
        "        return sequence\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        src_seq, tgt_seq = self.pairs[index]\n",
        "        return src_seq, tgt_seq"
      ],
      "metadata": {
        "id": "vVeybRtnKtJX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainAndValidationSequenceDatasets():\n",
        "    def __init__(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, train_start_index, train_end_index, val_start_index, val_end_index):\n",
        "        src_text = self.load_doc(src_filename)\n",
        "        tgt_text = self.load_doc(tgt_filename)\n",
        "        self.train_dataset = SequencePairDataset(src_text, tgt_text, train_start_index, train_end_index)\n",
        "        self.val_dataset = SequencePairDataset(src_text, tgt_text, val_start_index, val_end_index)\n",
        "\n",
        "        # load doc into memory\n",
        "    def load_doc(self, filename):\n",
        "        # open the file as read only\n",
        "        file = open(filename, mode='rt')\n",
        "        # read all text\n",
        "        text = file.read()\n",
        "        # close the file\n",
        "        file.close()\n",
        "        return text"
      ],
      "metadata": {
        "id": "IJciEqbpJajo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "class PadCollate:\n",
        "    TOKENIZER_SUFFIX = \"_tokenizer\"\n",
        "\n",
        "    def __init__(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size):\n",
        "        self.src_tokenizer, self.tgt_tokenizer = self.setup_tokenizers(src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, src_filename + self.TOKENIZER_SUFFIX, tgt_filename + self.TOKENIZER_SUFFIX)\n",
        "\n",
        "    def setup_tokenizers(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, src_tokenizer_name, tgt_tokenizer_name):\n",
        "        print(\"creating tokenizer for \" + src_filename)\n",
        "        src_tokenizer = Tokenizer(BPE(unk_token=SequencePairDataset.UNK_TOKEN))\n",
        "        src_tokenizer.pre_tokenizer = Whitespace()\n",
        "        # src_tokenizer.post_processor = TemplateProcessing(\n",
        "        #     single=\"[BOS] $A [EOS]\",\n",
        "        #     special_tokens=[(\"[BOS]\", 0), (\"[EOS]\", 1)],\n",
        "        # )\n",
        "        trainer = BpeTrainer(vocab_size = src_vocab_size, special_tokens=[SequencePairDataset.BOS_TOKEN, SequencePairDataset.EOS_TOKEN, SequencePairDataset.PAD_TOKEN, SequencePairDataset.UNK_TOKEN])\n",
        "        src_tokenizer.train([src_filename], trainer=trainer)\n",
        "        pickle.dump(src_tokenizer, open(src_tokenizer_name, \"wb\"))\n",
        "\n",
        "        print(\"creating tokenizer for \" + tgt_filename)\n",
        "        tgt_tokenizer = Tokenizer(BPE(unk_token=SequencePairDataset.UNK_TOKEN))\n",
        "        tgt_tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = BpeTrainer(vocab_size = tgt_vocab_size, special_tokens=[SequencePairDataset.BOS_TOKEN, SequencePairDataset.EOS_TOKEN, SequencePairDataset.PAD_TOKEN, SequencePairDataset.UNK_TOKEN])\n",
        "        tgt_tokenizer.train([tgt_filename], trainer=trainer)\n",
        "        tgt_tokenizer = copy.deepcopy(src_tokenizer)\n",
        "        tgt_tokenizer.post_processor = TemplateProcessing(\n",
        "            single=\"[BOS] $A [EOS]\",\n",
        "            special_tokens=[(\"[BOS]\", 0), (\"[EOS]\", 1)],\n",
        "        )\n",
        "        pickle.dump(tgt_tokenizer, open(tgt_tokenizer_name, \"wb\"))\n",
        "        return src_tokenizer, tgt_tokenizer\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # max_len_src = max([len(pair[0].split()) for pair in batch])\n",
        "        # max_len_tgt = max([len(pair[1].split()) for pair in batch])\n",
        "\n",
        "        #tgt_sequence_lengths\n",
        "\n",
        "        self.src_tokenizer.no_padding()\n",
        "        self.tgt_tokenizer.no_padding()\n",
        "\n",
        "        self.src_tokenizer.no_truncation()\n",
        "        self.tgt_tokenizer.no_truncation()\n",
        "\n",
        "        src_tokenized = self.src_tokenizer.encode_batch([pair[0] for pair in batch])\n",
        "        tgt_tokenized = self.tgt_tokenizer.encode_batch([pair[1] for pair in batch])\n",
        "\n",
        "        max_len_src = max([len(sequence) for sequence in src_tokenized])\n",
        "        max_len_tgt = max([len(sequence) for sequence in tgt_tokenized])\n",
        "\n",
        "        # print(\"max len src:\", max_len_src)\n",
        "        # print(\"max len tgt:\", max_len_tgt)\n",
        "\n",
        "        self.src_tokenizer.enable_padding(pad_id = SequencePairDataset.PAD_ID, pad_token = SequencePairDataset.PAD_TOKEN)\n",
        "        self.src_tokenizer.enable_truncation(max_length=max_len_src)\n",
        "        self.tgt_tokenizer.enable_padding(pad_id = SequencePairDataset.PAD_ID, pad_token = SequencePairDataset.PAD_TOKEN)\n",
        "        self.tgt_tokenizer.enable_truncation(max_length=max_len_tgt)\n",
        "\n",
        "        # print(\"src batch:\", [pair[0] for pair in batch])\n",
        "        # print(\"tgt batch:\", [pair[1] for pair in batch])\n",
        "\n",
        "        src_tokenized = self.src_tokenizer.encode_batch([pair[0] for pair in batch])\n",
        "        tgt_tokenized = self.tgt_tokenizer.encode_batch([pair[1] for pair in batch])\n",
        "        # src_tokenized = [sequence.ids for sequence in src_tokenized]\n",
        "        # tgt_tokenized = [sequence.ids for sequence in tgt_tokenized]\n",
        "        # src_tensors = torch.IntTensor(src_tokenized)\n",
        "        # tgt_tensor = torch.IntTensor(tgt_tokenized)\n",
        "\n",
        "        return src_tokenized, tgt_tokenized"
      ],
      "metadata": {
        "id": "P1wzP-DDLZ1X"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_encoder_layers = 4\n",
        "num_decoder_layers = 4\n",
        "num_heads = 8\n",
        "d_attn = 32\n",
        "d_x = 256\n",
        "d_z = 256\n",
        "d_out = 256\n",
        "d_mid = 256\n",
        "d_mlp = 512\n",
        "d_e = 256\n",
        "vocab_size = 10000\n",
        "max_sequence_length = 100\n",
        "p_dropout = 0.1"
      ],
      "metadata": {
        "id": "vFPxq_hkffx3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_validation_sequence_datasets = TrainAndValidationSequenceDatasets(enRawName, enRawName, vocab_size, vocab_size, 0, 20000, 20000, 25000)\n",
        "train_dataset = train_and_validation_sequence_datasets.train_dataset\n",
        "val_dataset = train_and_validation_sequence_datasets.val_dataset"
      ],
      "metadata": {
        "id": "uDnN_IgsNIJD"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.__getitem__(0))"
      ],
      "metadata": {
        "id": "L3WwzUFdRbk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ca1adf-e729-4f77-91f6-d1ee714a7485"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('A man on the sea.', 'A man on the sea.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pad_collate = PadCollate(enRawName, enRawName, vocab_size, vocab_size)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, collate_fn = pad_collate)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, collate_fn = pad_collate)"
      ],
      "metadata": {
        "id": "zLayKBKBLAYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49be2ea4-39fb-49b4-c6f6-30bb5230867f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating tokenizer for drive/MyDrive/colab data/multi30kEnTrain.txt\n",
            "creating tokenizer for drive/MyDrive/colab data/multi30kEnTrain.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for src, tgt in train_dataloader:\n",
        "    print(src[0].ids, tgt[0].ids)\n",
        "    # print(\"decoded\", sequenceDataset.src_tokenizer.decode_batch([sequence.ids for sequence in src]))\n",
        "    # print(\"tgt\", tgt)\n",
        "\n",
        "    # print(\"mask:\", src[0].attention_mask)\n",
        "    break"
      ],
      "metadata": {
        "id": "7mnqolodQCYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa8657b-8b97-4306-e07b-34baa596c41f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30, 93, 89, 94, 1602, 15, 2] [0, 30, 93, 89, 94, 1602, 15, 1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(x, tokenizer):\n",
        "    x = torch.softmax(x, -1)\n",
        "    #print(\"x softmax:\", x)\n",
        "    x = torch.argmax(x, dim=-1)\n",
        "    x = x.tolist()\n",
        "    print(\"argmax x:\", x)\n",
        "    return tokenizer.decode(x)"
      ],
      "metadata": {
        "id": "ERDbLlQVzJUx"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_decode(tokenizer):\n",
        "    x = torch.tensor([[0, 5], [10, 20]], dtype=torch.float32)\n",
        "    words = decode(x, tokenizer)\n",
        "    print(words)\n",
        "\n",
        "test_decode(pad_collate.tgt_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSetg38X8TAo",
        "outputId": "eee6f25f-a71f-4020-c6f7-a01a7d3719ae"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax x: [1, 1]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(25)\n",
        "encoder_decoder_transformer = EncoderDecoderTransformer(num_encoder_layers, num_decoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, d_e, vocab_size, max_sequence_length, 0.1, False).to(device)\n",
        "opt = optim.Adam(encoder_decoder_transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "loss_function = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "epochs = 1000\n",
        "state_dict_filename = folder + \"encoder_decoder_transformer_state_dict_\" + datetime.today().strftime('%Y-%m-%d %H')\n",
        "# Large models need this to actually train\n",
        "for p in encoder_decoder_transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "#labelSmoothing = LabelSmoothing(2000, PADDING_IDX, 0.1)\n",
        "training_step = 0\n",
        "validation_step = 0\n",
        "best_val_loss = 100\n",
        "num_fails = 0\n",
        "for i in range(epochs):\n",
        "    epoch_time_start = time.time()\n",
        "    dataloader_iter = iter(train_dataloader)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    for src_batch, tgt_batch in dataloader_iter:\n",
        "        # print(\"x:\", sequence_x)\n",
        "        # print(\"z:\", sequence_z)\n",
        "        # sequence_x, sequence_z = sequenceDataset.__getitem__(i)\n",
        "        src_tokens = torch.IntTensor([sequence.ids for sequence in src_batch]).to(device)\n",
        "        encoder_input = src_tokens\n",
        "        train_tgt_tokens = torch.IntTensor([sequence.ids for sequence in tgt_batch]).to(device)\n",
        "        decoder_input = train_tgt_tokens[:, :-1]\n",
        "        decoder_desired_output_train = train_tgt_tokens[:, 1:]\n",
        "        src_masks = torch.IntTensor([sequence.attention_mask for sequence in src_batch]).to(device)\n",
        "        tgt_masks = torch.IntTensor([sequence.attention_mask for sequence in tgt_batch])[:, :-1].to(device)\n",
        "        # print(\"src masks\", src_masks)\n",
        "        # print(\"tgt masks\", tgt_masks)\n",
        "        train_output = encoder_decoder_transformer(encoder_input, decoder_input, src_masks, tgt_masks)\n",
        "        #print(output.shape)\n",
        "        # print(\"output\", output.shape)\n",
        "        output_transpose = train_output.transpose(-1, -2) # output needs to be N, C, other dimension for torch cross entropy\n",
        "        loss = loss_function(output_transpose, decoder_desired_output_train.long())\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        train_losses.append(loss.item())\n",
        "        if (training_step % 20 == 0):\n",
        "            print(\"Completed training step\", training_step)\n",
        "        training_step += 1\n",
        "\n",
        "    for src_batch, tgt_batch in val_dataloader:\n",
        "        src_tokens = torch.IntTensor([sequence.ids for sequence in src_batch]).to(device)\n",
        "        encoder_input = src_tokens\n",
        "        val_tgt_tokens = torch.IntTensor([sequence.ids for sequence in tgt_batch]).to(device)\n",
        "        decoder_input = val_tgt_tokens[:, :-1]\n",
        "        decoder_desired_output_val = val_tgt_tokens[:, 1:]\n",
        "        src_masks = torch.IntTensor([sequence.attention_mask for sequence in src_batch]).to(device)\n",
        "        tgt_masks = torch.IntTensor([sequence.attention_mask for sequence in tgt_batch])[:, :-1].to(device)\n",
        "        val_output = encoder_decoder_transformer(encoder_input, decoder_input, src_masks, tgt_masks)\n",
        "        output_transpose = val_output.transpose(-1, -2) # output needs to be N, C, other dimension for torch cross entropy\n",
        "        loss = loss_function(output_transpose, decoder_desired_output_val.long())\n",
        "        val_losses.append(loss.item())\n",
        "        if (validation_step % 20 == 0):\n",
        "            print(\"Completed validation step\", validation_step)\n",
        "        validation_step += 1\n",
        "\n",
        "    print(\"epoch\", i, \"took\", time.time() - epoch_time_start)\n",
        "    print(\"avg training loss:\", sum(train_losses) / len(train_losses))\n",
        "    avg_val_loss = sum(val_losses)/ len(val_losses)\n",
        "    print(\"avg validation loss:\", avg_val_loss)\n",
        "    expected_train_output = pad_collate.tgt_tokenizer.decode(decoder_desired_output_train[0].tolist())\n",
        "    print(\"expected train output\", expected_train_output)\n",
        "    decoded_output = decode(train_output[0], pad_collate.tgt_tokenizer)\n",
        "    print(\"decoded train output:\", decoded_output)\n",
        "    expected_val_output = pad_collate.tgt_tokenizer.decode(decoder_desired_output_val[0].tolist())\n",
        "    print(\"expected validation output\", expected_val_output)\n",
        "    decoded_output = decode(val_output[0], pad_collate.tgt_tokenizer)\n",
        "    print(\"decoded validation output:\", decoded_output)\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(encoder_decoder_transformer.state_dict(), state_dict_filename)\n",
        "        print(\"Saved model state dict to\", state_dict_filename)\n",
        "        num_fails = 0\n",
        "    else:\n",
        "        print(\"Average validation loss did not decrease from \", best_val_loss)\n",
        "        num_fails += 1\n",
        "        print(\"Failed to decrease the average validation loss\", num_fails, \"times.\")\n",
        "        if num_fails >= 2:\n",
        "            print(\"Stopping training\")\n",
        "            break\n",
        "    print()\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCHMIluxq6Su",
        "outputId": "cffa43d6-ccba-415b-d123-d484c745327d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed training step 0\n",
            "Completed training step 20\n",
            "Completed training step 40\n",
            "Completed training step 60\n",
            "Completed training step 80\n",
            "Completed training step 100\n",
            "Completed training step 120\n",
            "Completed training step 140\n",
            "Completed training step 160\n",
            "Completed training step 180\n",
            "Completed training step 200\n",
            "Completed training step 220\n",
            "Completed training step 240\n",
            "Completed training step 260\n",
            "Completed training step 280\n",
            "Completed training step 300\n",
            "Completed training step 320\n",
            "Completed training step 340\n",
            "Completed training step 360\n",
            "Completed training step 380\n",
            "Completed training step 400\n",
            "Completed training step 420\n",
            "Completed training step 440\n",
            "Completed training step 460\n",
            "Completed training step 480\n",
            "Completed training step 500\n",
            "Completed training step 520\n",
            "Completed training step 540\n",
            "Completed training step 560\n",
            "Completed training step 580\n",
            "Completed training step 600\n",
            "Completed training step 620\n",
            "Completed validation step 0\n",
            "Completed validation step 20\n",
            "Completed validation step 40\n",
            "Completed validation step 60\n",
            "Completed validation step 80\n",
            "Completed validation step 100\n",
            "Completed validation step 120\n",
            "Completed validation step 140\n",
            "epoch 0 took 55.18796944618225\n",
            "avg training loss: 5.014542189788818\n",
            "avg validation loss: 4.390145809027799\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 57, 57, 109, 96, 57, 57, 127, 160, 96, 57, 160, 96, 57, 96, 96, 96, 57, 93, 109, 57, 57, 109, 96, 83, 57, 104, 89, 94, 15, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with a a with and a a woman shirt and a shirt and a and and and a man with a a with and in a of on the . .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [235, 93, 156, 57, 156, 83, 57, 104, 57, 160, 96, 96, 57, 13, 57, 127, 13, 109, 93, 96, 57, 93, 83, 57, 96, 96, 57, 104, 57, 216, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: The man wearing a wearing in a of a shirt and and a , a woman , with man and a man in a and and a of a street .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 640\n",
            "Completed training step 660\n",
            "Completed training step 680\n",
            "Completed training step 700\n",
            "Completed training step 720\n",
            "Completed training step 740\n",
            "Completed training step 760\n",
            "Completed training step 780\n",
            "Completed training step 800\n",
            "Completed training step 820\n",
            "Completed training step 840\n",
            "Completed training step 860\n",
            "Completed training step 880\n",
            "Completed training step 900\n",
            "Completed training step 920\n",
            "Completed training step 940\n",
            "Completed training step 960\n",
            "Completed training step 980\n",
            "Completed training step 1000\n",
            "Completed training step 1020\n",
            "Completed training step 1040\n",
            "Completed training step 1060\n",
            "Completed training step 1080\n",
            "Completed training step 1100\n",
            "Completed training step 1120\n",
            "Completed training step 1140\n",
            "Completed training step 1160\n",
            "Completed training step 1180\n",
            "Completed training step 1200\n",
            "Completed training step 1220\n",
            "Completed training step 1240\n",
            "Completed validation step 160\n",
            "Completed validation step 180\n",
            "Completed validation step 200\n",
            "Completed validation step 220\n",
            "Completed validation step 240\n",
            "Completed validation step 260\n",
            "Completed validation step 280\n",
            "Completed validation step 300\n",
            "epoch 1 took 53.73097825050354\n",
            "avg training loss: 3.4719755241394044\n",
            "avg validation loss: 3.483037483919958\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 57, 109, 93, 96, 156, 57, 191, 160, 13, 180, 109, 109, 172, 160, 101, 89, 57, 291, 94, 94, 242, 205, 156, 83, 94, 101, 184, 84, 214, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with a with man and wearing a blue shirt , black with with his shirt is on a large the the water standing wearing in the is while an group .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 156, 156, 83, 104, 104, 84, 385, 13, 96, 205, 236, 57, 216, 184, 303, 96, 96, 57, 93, 96, 57, 96, 57, 230, 104, 57, 216, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing wearing wearing in of of an orange , and standing holding a street while two and and a man and a and a front of a street .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 1260\n",
            "Completed training step 1280\n",
            "Completed training step 1300\n",
            "Completed training step 1320\n",
            "Completed training step 1340\n",
            "Completed training step 1360\n",
            "Completed training step 1380\n",
            "Completed training step 1400\n",
            "Completed training step 1420\n",
            "Completed training step 1440\n",
            "Completed training step 1460\n",
            "Completed training step 1480\n",
            "Completed training step 1500\n",
            "Completed training step 1520\n",
            "Completed training step 1540\n",
            "Completed training step 1560\n",
            "Completed training step 1580\n",
            "Completed training step 1600\n",
            "Completed training step 1620\n",
            "Completed training step 1640\n",
            "Completed training step 1660\n",
            "Completed training step 1680\n",
            "Completed training step 1700\n",
            "Completed training step 1720\n",
            "Completed training step 1740\n",
            "Completed training step 1760\n",
            "Completed training step 1780\n",
            "Completed training step 1800\n",
            "Completed training step 1820\n",
            "Completed training step 1840\n",
            "Completed training step 1860\n",
            "Completed validation step 320\n",
            "Completed validation step 340\n",
            "Completed validation step 360\n",
            "Completed validation step 380\n",
            "Completed validation step 400\n",
            "Completed validation step 420\n",
            "Completed validation step 440\n",
            "Completed validation step 460\n",
            "epoch 2 took 54.885258197784424\n",
            "avg training loss: 2.7840933044433593\n",
            "avg validation loss: 2.828160636743922\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 57, 156, 93, 156, 156, 57, 154, 1181, 13, 57, 160, 109, 570, 160, 13, 96, 57, 983, 97, 94, 216, 205, 208, 57, 154, 109, 109, 57, 291, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with a wearing man wearing wearing a red sweatshirt , a shirt with long shirt , and a drinking at the street standing playing a red with with a large .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 156, 398, 83, 104, 104, 84, 385, 698, 96, 775, 313, 57, 331, 184, 363, 93, 156, 57, 191, 96, 57, 83, 83, 230, 104, 57, 648, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing wearing sits in of of an orange microphone and waiting from a building while another man wearing a blue and a in in front of a mouth .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 1880\n",
            "Completed training step 1900\n",
            "Completed training step 1920\n",
            "Completed training step 1940\n",
            "Completed training step 1960\n",
            "Completed training step 1980\n",
            "Completed training step 2000\n",
            "Completed training step 2020\n",
            "Completed training step 2040\n",
            "Completed training step 2060\n",
            "Completed training step 2080\n",
            "Completed training step 2100\n",
            "Completed training step 2120\n",
            "Completed training step 2140\n",
            "Completed training step 2160\n",
            "Completed training step 2180\n",
            "Completed training step 2200\n",
            "Completed training step 2220\n",
            "Completed training step 2240\n",
            "Completed training step 2260\n",
            "Completed training step 2280\n",
            "Completed training step 2300\n",
            "Completed training step 2320\n",
            "Completed training step 2340\n",
            "Completed training step 2360\n",
            "Completed training step 2380\n",
            "Completed training step 2400\n",
            "Completed training step 2420\n",
            "Completed training step 2440\n",
            "Completed training step 2460\n",
            "Completed training step 2480\n",
            "Completed validation step 480\n",
            "Completed validation step 500\n",
            "Completed validation step 520\n",
            "Completed validation step 540\n",
            "Completed validation step 560\n",
            "Completed validation step 580\n",
            "Completed validation step 600\n",
            "Completed validation step 620\n",
            "epoch 3 took 54.73096776008606\n",
            "avg training loss: 2.3460095169067383\n",
            "avg validation loss: 2.3951585323187956\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 109, 171, 342, 13, 156, 57, 154, 850, 13, 257, 552, 109, 803, 160, 13, 96, 190, 1347, 97, 94, 387, 268, 208, 57, 264, 156, 109, 57, 698, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with with young jacket , wearing a red cellphone , hat shorts with colorful shirt , and one painted at the beach women playing a green wearing with a microphone .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 83, 879, 190, 775, 313, 57, 543, 184, 363, 93, 156, 57, 160, 96, 262, 13, 83, 230, 104, 57, 415, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an in stone one waiting from a room while another man wearing a shirt and looking , in front of a background .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 2500\n",
            "Completed training step 2520\n",
            "Completed training step 2540\n",
            "Completed training step 2560\n",
            "Completed training step 2580\n",
            "Completed training step 2600\n",
            "Completed training step 2620\n",
            "Completed training step 2640\n",
            "Completed training step 2660\n",
            "Completed training step 2680\n",
            "Completed training step 2700\n",
            "Completed training step 2720\n",
            "Completed training step 2740\n",
            "Completed training step 2760\n",
            "Completed training step 2780\n",
            "Completed training step 2800\n",
            "Completed training step 2820\n",
            "Completed training step 2840\n",
            "Completed training step 2860\n",
            "Completed training step 2880\n",
            "Completed training step 2900\n",
            "Completed training step 2920\n",
            "Completed training step 2940\n",
            "Completed training step 2960\n",
            "Completed training step 2980\n",
            "Completed training step 3000\n",
            "Completed training step 3020\n",
            "Completed training step 3040\n",
            "Completed training step 3060\n",
            "Completed training step 3080\n",
            "Completed training step 3100\n",
            "Completed training step 3120\n",
            "Completed validation step 640\n",
            "Completed validation step 660\n",
            "Completed validation step 680\n",
            "Completed validation step 700\n",
            "Completed validation step 720\n",
            "Completed validation step 740\n",
            "Completed validation step 760\n",
            "Completed validation step 780\n",
            "epoch 4 took 55.87970447540283\n",
            "avg training loss: 2.0764065042495727\n",
            "avg validation loss: 2.137197062468073\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 93, 76, 394, 13, 156, 57, 154, 160, 101, 191, 160, 109, 736, 160, 13, 89, 190, 1347, 97, 94, 387, 13, 592, 57, 528, 549, 109, 57, 608, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with man t hair , wearing a red shirt is blue shirt with no shirt , on one painted at the beach , taking a sand under with a jeans .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 299, 1151, 96, 1109, 313, 57, 728, 184, 363, 93, 156, 57, 735, 96, 502, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an old computer and both from a bag while another man wearing a cap and smiling stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 3140\n",
            "Completed training step 3160\n",
            "Completed training step 3180\n",
            "Completed training step 3200\n",
            "Completed training step 3220\n",
            "Completed training step 3240\n",
            "Completed training step 3260\n",
            "Completed training step 3280\n",
            "Completed training step 3300\n",
            "Completed training step 3320\n",
            "Completed training step 3340\n",
            "Completed training step 3360\n",
            "Completed training step 3380\n",
            "Completed training step 3400\n",
            "Completed training step 3420\n",
            "Completed training step 3440\n",
            "Completed training step 3460\n",
            "Completed training step 3480\n",
            "Completed training step 3500\n",
            "Completed training step 3520\n",
            "Completed training step 3540\n",
            "Completed training step 3560\n",
            "Completed training step 3580\n",
            "Completed training step 3600\n",
            "Completed training step 3620\n",
            "Completed training step 3640\n",
            "Completed training step 3660\n",
            "Completed training step 3680\n",
            "Completed training step 3700\n",
            "Completed training step 3720\n",
            "Completed training step 3740\n",
            "Completed validation step 800\n",
            "Completed validation step 820\n",
            "Completed validation step 840\n",
            "Completed validation step 860\n",
            "Completed validation step 880\n",
            "Completed validation step 900\n",
            "Completed validation step 920\n",
            "Completed validation step 940\n",
            "epoch 5 took 54.93317914009094\n",
            "avg training loss: 1.8987062950134277\n",
            "avg validation loss: 2.025022253868686\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 93, 171, 160, 13, 156, 57, 154, 342, 96, 191, 552, 14, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 171, 57, 172, 721, 109, 57, 608, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with man young shirt , wearing a red jacket and blue shorts - no shirt , on one knee at the beach , young a his during with a jeans .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 1064, 1151, 96, 1477, 313, 57, 728, 184, 363, 93, 156, 57, 160, 96, 190, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an adult computer and drinks from a bag while another man wearing a shirt and one stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 3760\n",
            "Completed training step 3780\n",
            "Completed training step 3800\n",
            "Completed training step 3820\n",
            "Completed training step 3840\n",
            "Completed training step 3860\n",
            "Completed training step 3880\n",
            "Completed training step 3900\n",
            "Completed training step 3920\n",
            "Completed training step 3940\n",
            "Completed training step 3960\n",
            "Completed training step 3980\n",
            "Completed training step 4000\n",
            "Completed training step 4020\n",
            "Completed training step 4040\n",
            "Completed training step 4060\n",
            "Completed training step 4080\n",
            "Completed training step 4100\n",
            "Completed training step 4120\n",
            "Completed training step 4140\n",
            "Completed training step 4160\n",
            "Completed training step 4180\n",
            "Completed training step 4200\n",
            "Completed training step 4220\n",
            "Completed training step 4240\n",
            "Completed training step 4260\n",
            "Completed training step 4280\n",
            "Completed training step 4300\n",
            "Completed training step 4320\n",
            "Completed training step 4340\n",
            "Completed training step 4360\n",
            "Completed validation step 960\n",
            "Completed validation step 980\n",
            "Completed validation step 1000\n",
            "Completed validation step 1020\n",
            "Completed validation step 1040\n",
            "Completed validation step 1060\n",
            "Completed validation step 1080\n",
            "epoch 6 took 55.0148983001709\n",
            "avg training loss: 1.777892345237732\n",
            "avg validation loss: 1.8876352249437076\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 492, 83, 394, 13, 156, 57, 154, 177, 96, 191, 830, 109, 736, 160, 13, 89, 190, 1709, 97, 94, 387, 13, 1423, 57, 528, 314, 109, 57, 15, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with blond in hair , wearing a red dog and blue hats with no shirt , on one leather at the beach , this a sand their with a . .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 385, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an orange computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 4380\n",
            "Completed training step 4400\n",
            "Completed training step 4420\n",
            "Completed training step 4440\n",
            "Completed training step 4460\n",
            "Completed training step 4480\n",
            "Completed training step 4500\n",
            "Completed training step 4520\n",
            "Completed training step 4540\n",
            "Completed training step 4560\n",
            "Completed training step 4580\n",
            "Completed training step 4600\n",
            "Completed training step 4620\n",
            "Completed training step 4640\n",
            "Completed training step 4660\n",
            "Completed training step 4680\n",
            "Completed training step 4700\n",
            "Completed training step 4720\n",
            "Completed training step 4740\n",
            "Completed training step 4760\n",
            "Completed training step 4780\n",
            "Completed training step 4800\n",
            "Completed training step 4820\n",
            "Completed training step 4840\n",
            "Completed training step 4860\n",
            "Completed training step 4880\n",
            "Completed training step 4900\n",
            "Completed training step 4920\n",
            "Completed training step 4940\n",
            "Completed training step 4960\n",
            "Completed training step 4980\n",
            "Completed validation step 1100\n",
            "Completed validation step 1120\n",
            "Completed validation step 1140\n",
            "Completed validation step 1160\n",
            "Completed validation step 1180\n",
            "Completed validation step 1200\n",
            "Completed validation step 1220\n",
            "Completed validation step 1240\n",
            "epoch 7 took 55.16791367530823\n",
            "avg training loss: 1.6908063785552978\n",
            "avg validation loss: 1.8109157692854572\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 180, 288, 394, 13, 156, 57, 154, 214, 96, 191, 552, 830, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 57, 57, 528, 1289, 109, 57, 15, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with black little hair , wearing a red group and blue shorts hats no shirt , on one knee at the beach , a a sand court with a . .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 299, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an old computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 5000\n",
            "Completed training step 5020\n",
            "Completed training step 5040\n",
            "Completed training step 5060\n",
            "Completed training step 5080\n",
            "Completed training step 5100\n",
            "Completed training step 5120\n",
            "Completed training step 5140\n",
            "Completed training step 5160\n",
            "Completed training step 5180\n",
            "Completed training step 5200\n",
            "Completed training step 5220\n",
            "Completed training step 5240\n",
            "Completed training step 5260\n",
            "Completed training step 5280\n",
            "Completed training step 5300\n",
            "Completed training step 5320\n",
            "Completed training step 5340\n",
            "Completed training step 5360\n",
            "Completed training step 5380\n",
            "Completed training step 5400\n",
            "Completed training step 5420\n",
            "Completed training step 5440\n",
            "Completed training step 5460\n",
            "Completed training step 5480\n",
            "Completed training step 5500\n",
            "Completed training step 5520\n",
            "Completed training step 5540\n",
            "Completed training step 5560\n",
            "Completed training step 5580\n",
            "Completed training step 5600\n",
            "Completed training step 5620\n",
            "Completed validation step 1260\n",
            "Completed validation step 1280\n",
            "Completed validation step 1300\n",
            "Completed validation step 1320\n",
            "Completed validation step 1340\n",
            "Completed validation step 1360\n",
            "Completed validation step 1380\n",
            "Completed validation step 1400\n",
            "epoch 8 took 56.1899573802948\n",
            "avg training loss: 1.6242271461486817\n",
            "avg validation loss: 1.7539685607715776\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 492, 156, 394, 13, 156, 57, 154, 394, 96, 191, 552, 109, 736, 160, 13, 96, 190, 2042, 97, 94, 387, 13, 728, 57, 528, 2165, 109, 57, 211, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with blond wearing hair , wearing a red hair and blue shorts with no shirt , and one knee at the beach , bag a sand tracks with a child .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 1315, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an apron computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 5640\n",
            "Completed training step 5660\n",
            "Completed training step 5680\n",
            "Completed training step 5700\n",
            "Completed training step 5720\n",
            "Completed training step 5740\n",
            "Completed training step 5760\n",
            "Completed training step 5780\n",
            "Completed training step 5800\n",
            "Completed training step 5820\n",
            "Completed training step 5840\n",
            "Completed training step 5860\n",
            "Completed training step 5880\n",
            "Completed training step 5900\n",
            "Completed training step 5920\n",
            "Completed training step 5940\n",
            "Completed training step 5960\n",
            "Completed training step 5980\n",
            "Completed training step 6000\n",
            "Completed training step 6020\n",
            "Completed training step 6040\n",
            "Completed training step 6060\n",
            "Completed training step 6080\n",
            "Completed training step 6100\n",
            "Completed training step 6120\n",
            "Completed training step 6140\n",
            "Completed training step 6160\n",
            "Completed training step 6180\n",
            "Completed training step 6200\n",
            "Completed training step 6220\n",
            "Completed training step 6240\n",
            "Completed validation step 1420\n",
            "Completed validation step 1440\n",
            "Completed validation step 1460\n",
            "Completed validation step 1480\n",
            "Completed validation step 1500\n",
            "Completed validation step 1520\n",
            "Completed validation step 1540\n",
            "Completed validation step 1560\n",
            "epoch 9 took 54.65521812438965\n",
            "avg training loss: 1.5723916372299194\n",
            "avg validation loss: 1.7061670221340883\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 1582, 13, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 1434, 57, 528, 2165, 109, 57, 1897, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with military , hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , Boy a sand tracks with a brunette .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 6260\n",
            "Completed training step 6280\n",
            "Completed training step 6300\n",
            "Completed training step 6320\n",
            "Completed training step 6340\n",
            "Completed training step 6360\n",
            "Completed training step 6380\n",
            "Completed training step 6400\n",
            "Completed training step 6420\n",
            "Completed training step 6440\n",
            "Completed training step 6460\n",
            "Completed training step 6480\n",
            "Completed training step 6500\n",
            "Completed training step 6520\n",
            "Completed training step 6540\n",
            "Completed training step 6560\n",
            "Completed training step 6580\n",
            "Completed training step 6600\n",
            "Completed training step 6620\n",
            "Completed training step 6640\n",
            "Completed training step 6660\n",
            "Completed training step 6680\n",
            "Completed training step 6700\n",
            "Completed training step 6720\n",
            "Completed training step 6740\n",
            "Completed training step 6760\n",
            "Completed training step 6780\n",
            "Completed training step 6800\n",
            "Completed training step 6820\n",
            "Completed training step 6840\n",
            "Completed training step 6860\n",
            "Completed validation step 1580\n",
            "Completed validation step 1600\n",
            "Completed validation step 1620\n",
            "Completed validation step 1640\n",
            "Completed validation step 1660\n",
            "Completed validation step 1680\n",
            "Completed validation step 1700\n",
            "Completed validation step 1720\n",
            "epoch 10 took 54.522027015686035\n",
            "avg training loss: 1.5317447607040404\n",
            "avg validation loss: 1.6765025641508162\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 288, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 558, 57, 528, 3081, 109, 57, 211, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium little hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , carrying a sand castle with a child .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 471, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an older computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 6880\n",
            "Completed training step 6900\n",
            "Completed training step 6920\n",
            "Completed training step 6940\n",
            "Completed training step 6960\n",
            "Completed training step 6980\n",
            "Completed training step 7000\n",
            "Completed training step 7020\n",
            "Completed training step 7040\n",
            "Completed training step 7060\n",
            "Completed training step 7080\n",
            "Completed training step 7100\n",
            "Completed training step 7120\n",
            "Completed training step 7140\n",
            "Completed training step 7160\n",
            "Completed training step 7180\n",
            "Completed training step 7200\n",
            "Completed training step 7220\n",
            "Completed training step 7240\n",
            "Completed training step 7260\n",
            "Completed training step 7280\n",
            "Completed training step 7300\n",
            "Completed training step 7320\n",
            "Completed training step 7340\n",
            "Completed training step 7360\n",
            "Completed training step 7380\n",
            "Completed training step 7400\n",
            "Completed training step 7420\n",
            "Completed training step 7440\n",
            "Completed training step 7460\n",
            "Completed training step 7480\n",
            "Completed validation step 1740\n",
            "Completed validation step 1760\n",
            "Completed validation step 1780\n",
            "Completed validation step 1800\n",
            "Completed validation step 1820\n",
            "Completed validation step 1840\n",
            "Completed validation step 1860\n",
            "Completed validation step 1880\n",
            "epoch 11 took 53.945822954177856\n",
            "avg training loss: 1.497994881248474\n",
            "avg validation loss: 1.647277756861061\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 291, 4617, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 1701, 57, 528, 3081, 109, 57, 2038, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with large cheerleader hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , helping a sand castle with a surface .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 7500\n",
            "Completed training step 7520\n",
            "Completed training step 7540\n",
            "Completed training step 7560\n",
            "Completed training step 7600\n",
            "Completed training step 7620\n",
            "Completed training step 7640\n",
            "Completed training step 7660\n",
            "Completed training step 7680\n",
            "Completed training step 7700\n",
            "Completed training step 7720\n",
            "Completed training step 7740\n",
            "Completed training step 7760\n",
            "Completed training step 7780\n",
            "Completed training step 7800\n",
            "Completed training step 7820\n",
            "Completed training step 7840\n",
            "Completed training step 7860\n",
            "Completed training step 7880\n",
            "Completed training step 7900\n",
            "Completed training step 7920\n",
            "Completed training step 7940\n",
            "Completed training step 7960\n",
            "Completed training step 7980\n",
            "Completed training step 8000\n",
            "Completed training step 8020\n",
            "Completed training step 8040\n",
            "Completed training step 8060\n",
            "Completed training step 8080\n",
            "Completed training step 8100\n",
            "Completed training step 8120\n",
            "Completed validation step 1900\n",
            "Completed validation step 1920\n",
            "Completed validation step 1940\n",
            "Completed validation step 1960\n",
            "Completed validation step 1980\n",
            "Completed validation step 2000\n",
            "Completed validation step 2020\n",
            "Completed validation step 2040\n",
            "epoch 12 took 54.31346917152405\n",
            "avg training loss: 1.4697366004943848\n",
            "avg validation loss: 1.6201848664860816\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 109, 291, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 558, 57, 528, 3081, 109, 57, 211, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with with large hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , carrying a sand castle with a child .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 8140\n",
            "Completed training step 8160\n",
            "Completed training step 8180\n",
            "Completed training step 8200\n",
            "Completed training step 8220\n",
            "Completed training step 8240\n",
            "Completed training step 8260\n",
            "Completed training step 8280\n",
            "Completed training step 8300\n",
            "Completed training step 8320\n",
            "Completed training step 8340\n",
            "Completed training step 8360\n",
            "Completed training step 8380\n",
            "Completed training step 8400\n",
            "Completed training step 8420\n",
            "Completed training step 8440\n",
            "Completed training step 8460\n",
            "Completed training step 8480\n",
            "Completed training step 8500\n",
            "Completed training step 8520\n",
            "Completed training step 8540\n",
            "Completed training step 8560\n",
            "Completed training step 8580\n",
            "Completed training step 8600\n",
            "Completed training step 8620\n",
            "Completed training step 8640\n",
            "Completed training step 8660\n",
            "Completed training step 8680\n",
            "Completed training step 8700\n",
            "Completed training step 8720\n",
            "Completed training step 8740\n",
            "Completed validation step 2060\n",
            "Completed validation step 2080\n",
            "Completed validation step 2100\n",
            "Completed validation step 2120\n",
            "Completed validation step 2140\n",
            "Completed validation step 2160\n",
            "Completed validation step 2180\n",
            "epoch 13 took 53.56591606140137\n",
            "avg training loss: 1.4457993263244628\n",
            "avg validation loss: 1.606370594850771\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 449, 3648, 101, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 291, 57, 528, 3081, 109, 57, 211, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man dogs medium is hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , large a sand castle with a child .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 8760\n",
            "Completed training step 8780\n",
            "Completed training step 8800\n",
            "Completed training step 8820\n",
            "Completed training step 8840\n",
            "Completed training step 8860\n",
            "Completed training step 8880\n",
            "Completed training step 8900\n",
            "Completed training step 8920\n",
            "Completed training step 8940\n",
            "Completed training step 8960\n",
            "Completed training step 8980\n",
            "Completed training step 9000\n",
            "Completed training step 9020\n",
            "Completed training step 9040\n",
            "Completed training step 9060\n",
            "Completed training step 9080\n",
            "Completed training step 9100\n",
            "Completed training step 9120\n",
            "Completed training step 9140\n",
            "Completed training step 9160\n",
            "Completed training step 9180\n",
            "Completed training step 9200\n",
            "Completed training step 9220\n",
            "Completed training step 9240\n",
            "Completed training step 9260\n",
            "Completed training step 9280\n",
            "Completed training step 9300\n",
            "Completed training step 9320\n",
            "Completed training step 9340\n",
            "Completed training step 9360\n",
            "Completed validation step 2200\n",
            "Completed validation step 2220\n",
            "Completed validation step 2240\n",
            "Completed validation step 2260\n",
            "Completed validation step 2280\n",
            "Completed validation step 2300\n",
            "Completed validation step 2320\n",
            "Completed validation step 2340\n",
            "epoch 14 took 53.72284555435181\n",
            "avg training loss: 1.4261213039398193\n",
            "avg validation loss: 1.5861830453204502\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 492, 288, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 378, 57, 528, 3081, 109, 57, 2217, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with blond little hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , work a sand castle with a checking .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 9380\n",
            "Completed training step 9400\n",
            "Completed training step 9420\n",
            "Completed training step 9440\n",
            "Completed training step 9460\n",
            "Completed training step 9480\n",
            "Completed training step 9500\n",
            "Completed training step 9520\n",
            "Completed training step 9540\n",
            "Completed training step 9560\n",
            "Completed training step 9580\n",
            "Completed training step 9600\n",
            "Completed training step 9620\n",
            "Completed training step 9640\n",
            "Completed training step 9660\n",
            "Completed training step 9680\n",
            "Completed training step 9700\n",
            "Completed training step 9720\n",
            "Completed training step 9740\n",
            "Completed training step 9760\n",
            "Completed training step 9780\n",
            "Completed training step 9800\n",
            "Completed training step 9820\n",
            "Completed training step 9840\n",
            "Completed training step 9860\n",
            "Completed training step 9880\n",
            "Completed training step 9900\n",
            "Completed training step 9920\n",
            "Completed training step 9940\n",
            "Completed training step 9960\n",
            "Completed training step 9980\n",
            "Completed validation step 2360\n",
            "Completed validation step 2380\n",
            "Completed validation step 2400\n",
            "Completed validation step 2420\n",
            "Completed validation step 2440\n",
            "Completed validation step 2460\n",
            "Completed validation step 2480\n",
            "Completed validation step 2500\n",
            "epoch 15 took 53.75573015213013\n",
            "avg training loss: 1.4085331184387206\n",
            "avg validation loss: 1.5756537451106272\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 93, 3648, 104, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 5291, 57, 528, 3081, 109, 57, 975, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man man medium of hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , herding a sand castle with a friend .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 10000\n",
            "Completed training step 10020\n",
            "Completed training step 10040\n",
            "Completed training step 10060\n",
            "Completed training step 10080\n",
            "Completed training step 10100\n",
            "Completed training step 10120\n",
            "Completed training step 10140\n",
            "Completed training step 10160\n",
            "Completed training step 10180\n",
            "Completed training step 10200\n",
            "Completed training step 10220\n",
            "Completed training step 10240\n",
            "Completed training step 10260\n",
            "Completed training step 10280\n",
            "Completed training step 10300\n",
            "Completed training step 10320\n",
            "Completed training step 10340\n",
            "Completed training step 10360\n",
            "Completed training step 10380\n",
            "Completed training step 10400\n",
            "Completed training step 10420\n",
            "Completed training step 10440\n",
            "Completed training step 10460\n",
            "Completed training step 10480\n",
            "Completed training step 10500\n",
            "Completed training step 10520\n",
            "Completed training step 10540\n",
            "Completed training step 10560\n",
            "Completed training step 10580\n",
            "Completed training step 10600\n",
            "Completed training step 10620\n",
            "Completed validation step 2520\n",
            "Completed validation step 2540\n",
            "Completed validation step 2560\n",
            "Completed validation step 2580\n",
            "Completed validation step 2600\n",
            "Completed validation step 2620\n",
            "Completed validation step 2640\n",
            "Completed validation step 2660\n",
            "epoch 16 took 53.774062633514404\n",
            "avg training loss: 1.393922292327881\n",
            "avg validation loss: 1.5621305613001442\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 101, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 2839, 57, 528, 3081, 109, 57, 211, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium is hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , neighborhood a sand castle with a child .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 10640\n",
            "Completed training step 10660\n",
            "Completed training step 10680\n",
            "Completed training step 10700\n",
            "Completed training step 10720\n",
            "Completed training step 10740\n",
            "Completed training step 10760\n",
            "Completed training step 10780\n",
            "Completed training step 10800\n",
            "Completed training step 10820\n",
            "Completed training step 10840\n",
            "Completed training step 10860\n",
            "Completed training step 10880\n",
            "Completed training step 10900\n",
            "Completed training step 10920\n",
            "Completed training step 10940\n",
            "Completed training step 10960\n",
            "Completed training step 10980\n",
            "Completed training step 11000\n",
            "Completed training step 11020\n",
            "Completed training step 11040\n",
            "Completed training step 11060\n",
            "Completed training step 11080\n",
            "Completed training step 11100\n",
            "Completed training step 11120\n",
            "Completed training step 11140\n",
            "Completed training step 11160\n",
            "Completed training step 11180\n",
            "Completed training step 11200\n",
            "Completed training step 11220\n",
            "Completed training step 11240\n",
            "Completed validation step 2680\n",
            "Completed validation step 2700\n",
            "Completed validation step 2720\n",
            "Completed validation step 2740\n",
            "Completed validation step 2760\n",
            "Completed validation step 2780\n",
            "Completed validation step 2800\n",
            "Completed validation step 2820\n",
            "epoch 17 took 53.05846881866455\n",
            "avg training loss: 1.3810633228302003\n",
            "avg validation loss: 1.5546806344560757\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 3419, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 4004, 57, 528, 3081, 109, 57, 2183, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium shirted hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , helicopter a sand castle with a fingers .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 11260\n",
            "Completed training step 11280\n",
            "Completed training step 11300\n",
            "Completed training step 11320\n",
            "Completed training step 11340\n",
            "Completed training step 11360\n",
            "Completed training step 11380\n",
            "Completed training step 11400\n",
            "Completed training step 11420\n",
            "Completed training step 11440\n",
            "Completed training step 11460\n",
            "Completed training step 11480\n",
            "Completed training step 11500\n",
            "Completed training step 11520\n",
            "Completed training step 11540\n",
            "Completed training step 11560\n",
            "Completed training step 11580\n",
            "Completed training step 11600\n",
            "Completed training step 11620\n",
            "Completed training step 11640\n",
            "Completed training step 11660\n",
            "Completed training step 11680\n",
            "Completed training step 11700\n",
            "Completed training step 11720\n",
            "Completed training step 11740\n",
            "Completed training step 11760\n",
            "Completed training step 11780\n",
            "Completed training step 11800\n",
            "Completed training step 11820\n",
            "Completed training step 11840\n",
            "Completed training step 11860\n",
            "Completed validation step 2840\n",
            "Completed validation step 2860\n",
            "Completed validation step 2880\n",
            "Completed validation step 2900\n",
            "Completed validation step 2920\n",
            "Completed validation step 2940\n",
            "Completed validation step 2960\n",
            "Completed validation step 2980\n",
            "epoch 18 took 53.18952441215515\n",
            "avg training loss: 1.3701205543518067\n",
            "avg validation loss: 1.5470137026659243\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 1059, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 1512, 57, 528, 3081, 109, 57, 2392, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium American hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , facing a sand castle with a touching .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 11880\n",
            "Completed training step 11900\n",
            "Completed training step 11920\n",
            "Completed training step 11940\n",
            "Completed training step 11960\n",
            "Completed training step 11980\n",
            "Completed training step 12000\n",
            "Completed training step 12020\n",
            "Completed training step 12040\n",
            "Completed training step 12060\n",
            "Completed training step 12080\n",
            "Completed training step 12100\n",
            "Completed training step 12120\n",
            "Completed training step 12140\n",
            "Completed training step 12160\n",
            "Completed training step 12180\n",
            "Completed training step 12200\n",
            "Completed training step 12220\n",
            "Completed training step 12240\n",
            "Completed training step 12260\n",
            "Completed training step 12280\n",
            "Completed training step 12300\n",
            "Completed training step 12320\n",
            "Completed training step 12340\n",
            "Completed training step 12360\n",
            "Completed training step 12380\n",
            "Completed training step 12400\n",
            "Completed training step 12420\n",
            "Completed training step 12440\n",
            "Completed training step 12460\n",
            "Completed training step 12480\n",
            "Completed validation step 3000\n",
            "Completed validation step 3020\n",
            "Completed validation step 3040\n",
            "Completed validation step 3060\n",
            "Completed validation step 3080\n",
            "Completed validation step 3100\n",
            "Completed validation step 3120\n",
            "epoch 19 took 53.573440074920654\n",
            "avg training loss: 1.3605003999710084\n",
            "avg validation loss: 1.5387986420066493\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 4995, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 9192, 57, 528, 3081, 109, 57, 975, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , Scouts a sand castle with a friend .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 12500\n",
            "Completed training step 12520\n",
            "Completed training step 12540\n",
            "Completed training step 12560\n",
            "Completed training step 12580\n",
            "Completed training step 12600\n",
            "Completed training step 12620\n",
            "Completed training step 12640\n",
            "Completed training step 12660\n",
            "Completed training step 12680\n",
            "Completed training step 12700\n",
            "Completed training step 12720\n",
            "Completed training step 12740\n",
            "Completed training step 12760\n",
            "Completed training step 12780\n",
            "Completed training step 12800\n",
            "Completed training step 12820\n",
            "Completed training step 12840\n",
            "Completed training step 12860\n",
            "Completed training step 12880\n",
            "Completed training step 12900\n",
            "Completed training step 12920\n",
            "Completed training step 12940\n",
            "Completed training step 12960\n",
            "Completed training step 12980\n",
            "Completed training step 13000\n",
            "Completed training step 13020\n",
            "Completed training step 13040\n",
            "Completed training step 13060\n",
            "Completed training step 13080\n",
            "Completed training step 13100\n",
            "Completed training step 13120\n",
            "Completed validation step 3140\n",
            "Completed validation step 3160\n",
            "Completed validation step 3180\n",
            "Completed validation step 3200\n",
            "Completed validation step 3220\n",
            "Completed validation step 3240\n",
            "Completed validation step 3260\n",
            "Completed validation step 3280\n",
            "epoch 20 took 53.21037745475769\n",
            "avg training loss: 1.3516392000198365\n",
            "avg validation loss: 1.534581212481116\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 3371, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 507, 57, 528, 3081, 109, 57, 1429, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium consisting hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , has a sand castle with a surfer .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 13140\n",
            "Completed training step 13160\n",
            "Completed training step 13180\n",
            "Completed training step 13200\n",
            "Completed training step 13220\n",
            "Completed training step 13240\n",
            "Completed training step 13260\n",
            "Completed training step 13280\n",
            "Completed training step 13300\n",
            "Completed training step 13320\n",
            "Completed training step 13340\n",
            "Completed training step 13360\n",
            "Completed training step 13380\n",
            "Completed training step 13400\n",
            "Completed training step 13420\n",
            "Completed training step 13440\n",
            "Completed training step 13460\n",
            "Completed training step 13480\n",
            "Completed training step 13500\n",
            "Completed training step 13520\n",
            "Completed training step 13540\n",
            "Completed training step 13560\n",
            "Completed training step 13580\n",
            "Completed training step 13600\n",
            "Completed training step 13620\n",
            "Completed training step 13640\n",
            "Completed training step 13660\n",
            "Completed training step 13680\n",
            "Completed training step 13700\n",
            "Completed training step 13720\n",
            "Completed training step 13740\n",
            "Completed validation step 3300\n",
            "Completed validation step 3320\n",
            "Completed validation step 3340\n",
            "Completed validation step 3360\n",
            "Completed validation step 3380\n",
            "Completed validation step 3400\n",
            "Completed validation step 3420\n",
            "Completed validation step 3440\n",
            "epoch 21 took 53.428911447525024\n",
            "avg training loss: 1.3448219200134277\n",
            "avg validation loss: 1.526702739630535\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 3648, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 4586, 57, 528, 3081, 109, 57, 2465, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium medium hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , disc a sand castle with a athletic .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 13760\n",
            "Completed training step 13780\n",
            "Completed training step 13800\n",
            "Completed training step 13820\n",
            "Completed training step 13840\n",
            "Completed training step 13860\n",
            "Completed training step 13880\n",
            "Completed training step 13900\n",
            "Completed training step 13920\n",
            "Completed training step 13940\n",
            "Completed training step 13960\n",
            "Completed training step 13980\n",
            "Completed training step 14000\n",
            "Completed training step 14020\n",
            "Completed training step 14040\n",
            "Completed training step 14060\n",
            "Completed training step 14080\n",
            "Completed training step 14100\n",
            "Completed training step 14120\n",
            "Completed training step 14140\n",
            "Completed training step 14160\n",
            "Completed training step 14180\n",
            "Completed training step 14200\n",
            "Completed training step 14220\n",
            "Completed training step 14240\n",
            "Completed training step 14260\n",
            "Completed training step 14280\n",
            "Completed training step 14300\n",
            "Completed training step 14320\n",
            "Completed training step 14340\n",
            "Completed training step 14360\n",
            "Completed validation step 3460\n",
            "Completed validation step 3480\n",
            "Completed validation step 3500\n",
            "Completed validation step 3520\n",
            "Completed validation step 3540\n",
            "Completed validation step 3560\n",
            "Completed validation step 3580\n",
            "Completed validation step 3600\n",
            "epoch 22 took 53.45207190513611\n",
            "avg training loss: 1.338094985961914\n",
            "avg validation loss: 1.5286632594029614\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 3371, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 3718, 57, 528, 3081, 109, 57, 471, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium consisting hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , fat a sand castle with a older .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Average validation loss did not decrease from  1.526702739630535\n",
            "Failed to decrease the average validation loss 1 times.\n",
            "\n",
            "\n",
            "Completed training step 14380\n",
            "Completed training step 14400\n",
            "Completed training step 14420\n",
            "Completed training step 14440\n",
            "Completed training step 14460\n",
            "Completed training step 14480\n",
            "Completed training step 14500\n",
            "Completed training step 14520\n",
            "Completed training step 14540\n",
            "Completed training step 14560\n",
            "Completed training step 14580\n",
            "Completed training step 14600\n",
            "Completed training step 14620\n",
            "Completed training step 14640\n",
            "Completed training step 14660\n",
            "Completed training step 14680\n",
            "Completed training step 14700\n",
            "Completed training step 14720\n",
            "Completed training step 14740\n",
            "Completed training step 14760\n",
            "Completed training step 14780\n",
            "Completed training step 14800\n",
            "Completed training step 14820\n",
            "Completed training step 14840\n",
            "Completed training step 14860\n",
            "Completed training step 14880\n",
            "Completed training step 14900\n",
            "Completed training step 14920\n",
            "Completed training step 14940\n",
            "Completed training step 14960\n",
            "Completed training step 14980\n",
            "Completed validation step 3620\n",
            "Completed validation step 3640\n",
            "Completed validation step 3660\n",
            "Completed validation step 3680\n",
            "Completed validation step 3700\n",
            "Completed validation step 3720\n",
            "Completed validation step 3740\n",
            "Completed validation step 3760\n",
            "epoch 23 took 53.67192983627319\n",
            "avg training loss: 1.3326063804626465\n",
            "avg validation loss: 1.518978771890045\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 93, 3648, 4995, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 757, 57, 528, 3081, 109, 57, 402, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man man medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , elderly a sand castle with a sun .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 5074, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an expression computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 15000\n",
            "Completed training step 15020\n",
            "Completed training step 15040\n",
            "Completed training step 15060\n",
            "Completed training step 15080\n",
            "Completed training step 15100\n",
            "Completed training step 15120\n",
            "Completed training step 15140\n",
            "Completed training step 15160\n",
            "Completed training step 15180\n",
            "Completed training step 15200\n",
            "Completed training step 15220\n",
            "Completed training step 15240\n",
            "Completed training step 15260\n",
            "Completed training step 15280\n",
            "Completed training step 15300\n",
            "Completed training step 15320\n",
            "Completed training step 15340\n",
            "Completed training step 15360\n",
            "Completed training step 15380\n",
            "Completed training step 15400\n",
            "Completed training step 15420\n",
            "Completed training step 15440\n",
            "Completed training step 15460\n",
            "Completed training step 15480\n",
            "Completed training step 15500\n",
            "Completed training step 15520\n",
            "Completed training step 15540\n",
            "Completed training step 15560\n",
            "Completed training step 15580\n",
            "Completed training step 15600\n",
            "Completed training step 15620\n",
            "Completed validation step 3780\n",
            "Completed validation step 3800\n",
            "Completed validation step 3820\n",
            "Completed validation step 3840\n",
            "Completed validation step 3860\n",
            "Completed validation step 3880\n",
            "Completed validation step 3900\n",
            "Completed validation step 3920\n",
            "epoch 24 took 53.46118402481079\n",
            "avg training loss: 1.3277082401275635\n",
            "avg validation loss: 1.518835331983627\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 3371, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 6312, 57, 528, 3081, 109, 57, 2392, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium consisting hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , twirling a sand castle with a touching .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 5651, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 15640\n",
            "Completed training step 15660\n",
            "Completed training step 15680\n",
            "Completed training step 15700\n",
            "Completed training step 15720\n",
            "Completed training step 15740\n",
            "Completed training step 15760\n",
            "Completed training step 15780\n",
            "Completed training step 15800\n",
            "Completed training step 15820\n",
            "Completed training step 15840\n",
            "Completed training step 15860\n",
            "Completed training step 15880\n",
            "Completed training step 15900\n",
            "Completed training step 15920\n",
            "Completed training step 15940\n",
            "Completed training step 15960\n",
            "Completed training step 15980\n",
            "Completed training step 16000\n",
            "Completed training step 16020\n",
            "Completed training step 16040\n",
            "Completed training step 16060\n",
            "Completed training step 16080\n",
            "Completed training step 16100\n",
            "Completed training step 16120\n",
            "Completed training step 16140\n",
            "Completed training step 16160\n",
            "Completed training step 16180\n",
            "Completed training step 16200\n",
            "Completed training step 16220\n",
            "Completed training step 16240\n",
            "Completed validation step 3940\n",
            "Completed validation step 3960\n",
            "Completed validation step 3980\n",
            "Completed validation step 4000\n",
            "Completed validation step 4020\n",
            "Completed validation step 4040\n",
            "Completed validation step 4060\n",
            "Completed validation step 4080\n",
            "epoch 25 took 53.75455665588379\n",
            "avg training loss: 1.3233151268005372\n",
            "avg validation loss: 1.5130244455519755\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 4995, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 9192, 57, 528, 3081, 109, 57, 2364, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , Scouts a sand castle with a six .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 5074, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an expression computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 16260\n",
            "Completed training step 16280\n",
            "Completed training step 16300\n",
            "Completed training step 16320\n",
            "Completed training step 16340\n",
            "Completed training step 16360\n",
            "Completed training step 16380\n",
            "Completed training step 16400\n",
            "Completed training step 16420\n",
            "Completed training step 16440\n",
            "Completed training step 16460\n",
            "Completed training step 16480\n",
            "Completed training step 16500\n",
            "Completed training step 16520\n",
            "Completed training step 16540\n",
            "Completed training step 16560\n",
            "Completed training step 16580\n",
            "Completed training step 16600\n",
            "Completed training step 16620\n",
            "Completed training step 16640\n",
            "Completed training step 16660\n",
            "Completed training step 16680\n",
            "Completed training step 16700\n",
            "Completed training step 16720\n",
            "Completed training step 16740\n",
            "Completed training step 16760\n",
            "Completed training step 16780\n",
            "Completed training step 16800\n",
            "Completed training step 16820\n",
            "Completed training step 16840\n",
            "Completed training step 16860\n",
            "Completed validation step 4100\n",
            "Completed validation step 4120\n",
            "Completed validation step 4140\n",
            "Completed validation step 4160\n",
            "Completed validation step 4180\n",
            "Completed validation step 4200\n",
            "Completed validation step 4220\n",
            "epoch 26 took 53.93298029899597\n",
            "avg training loss: 1.3193232936859132\n",
            "avg validation loss: 1.512207602999013\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 4995, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 1602, 57, 528, 3081, 109, 57, 2958, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sea a sand castle with a sad .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 5634, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an sponsored computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 16880\n",
            "Completed training step 16900\n",
            "Completed training step 16920\n",
            "Completed training step 16940\n",
            "Completed training step 16960\n",
            "Completed training step 16980\n",
            "Completed training step 17000\n",
            "Completed training step 17020\n",
            "Completed training step 17040\n",
            "Completed training step 17060\n",
            "Completed training step 17080\n",
            "Completed training step 17100\n",
            "Completed training step 17120\n",
            "Completed training step 17140\n",
            "Completed training step 17160\n",
            "Completed training step 17180\n",
            "Completed training step 17200\n",
            "Completed training step 17220\n",
            "Completed training step 17240\n",
            "Completed training step 17260\n",
            "Completed training step 17280\n",
            "Completed training step 17300\n",
            "Completed training step 17320\n",
            "Completed training step 17340\n",
            "Completed training step 17360\n",
            "Completed training step 17380\n",
            "Completed training step 17400\n",
            "Completed training step 17420\n",
            "Completed training step 17440\n",
            "Completed training step 17460\n",
            "Completed training step 17480\n",
            "Completed validation step 4240\n",
            "Completed validation step 4260\n",
            "Completed validation step 4280\n",
            "Completed validation step 4300\n",
            "Completed validation step 4320\n",
            "Completed validation step 4340\n",
            "Completed validation step 4360\n",
            "Completed validation step 4380\n",
            "epoch 27 took 54.08231449127197\n",
            "avg training loss: 1.3162896892547606\n",
            "avg validation loss: 1.5066979037728279\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 4995, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 2265, 57, 528, 3081, 109, 57, 402, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , grab a sand castle with a sun .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 5651, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 17500\n",
            "Completed training step 17520\n",
            "Completed training step 17540\n",
            "Completed training step 17560\n",
            "Completed training step 17580\n",
            "Completed training step 17600\n",
            "Completed training step 17620\n",
            "Completed training step 17640\n",
            "Completed training step 17660\n",
            "Completed training step 17680\n",
            "Completed training step 17700\n",
            "Completed training step 17720\n",
            "Completed training step 17740\n",
            "Completed training step 17760\n",
            "Completed training step 17780\n",
            "Completed training step 17800\n",
            "Completed training step 17820\n",
            "Completed training step 17840\n",
            "Completed training step 17860\n",
            "Completed training step 17880\n",
            "Completed training step 17900\n",
            "Completed training step 17920\n",
            "Completed training step 17940\n",
            "Completed training step 17960\n",
            "Completed training step 17980\n",
            "Completed training step 18000\n",
            "Completed training step 18020\n",
            "Completed training step 18040\n",
            "Completed training step 18060\n",
            "Completed training step 18080\n",
            "Completed training step 18100\n",
            "Completed training step 18120\n",
            "Completed validation step 4400\n",
            "Completed validation step 4420\n",
            "Completed validation step 4440\n",
            "Completed validation step 4460\n",
            "Completed validation step 4480\n",
            "Completed validation step 4500\n",
            "Completed validation step 4520\n",
            "Completed validation step 4540\n",
            "epoch 28 took 53.46742296218872\n",
            "avg training loss: 1.313465661239624\n",
            "avg validation loss: 1.5030777393632633\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 4995, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 2364, 57, 528, 3081, 109, 57, 4701, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , six a sand castle with a character .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 5651, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 18140\n",
            "Completed training step 18160\n",
            "Completed training step 18180\n",
            "Completed training step 18200\n",
            "Completed training step 18220\n",
            "Completed training step 18240\n",
            "Completed training step 18260\n",
            "Completed training step 18280\n",
            "Completed training step 18300\n",
            "Completed training step 18320\n",
            "Completed training step 18340\n",
            "Completed training step 18360\n",
            "Completed training step 18380\n",
            "Completed training step 18400\n",
            "Completed training step 18420\n",
            "Completed training step 18440\n",
            "Completed training step 18460\n",
            "Completed training step 18480\n",
            "Completed training step 18500\n",
            "Completed training step 18520\n",
            "Completed training step 18540\n",
            "Completed training step 18560\n",
            "Completed training step 18580\n",
            "Completed training step 18600\n",
            "Completed training step 18620\n",
            "Completed training step 18640\n",
            "Completed training step 18660\n",
            "Completed training step 18680\n",
            "Completed training step 18700\n",
            "Completed training step 18720\n",
            "Completed training step 18740\n",
            "Completed validation step 4560\n",
            "Completed validation step 4580\n",
            "Completed validation step 4600\n",
            "Completed validation step 4620\n",
            "Completed validation step 4640\n",
            "Completed validation step 4660\n",
            "Completed validation step 4680\n",
            "Completed validation step 4700\n",
            "epoch 29 took 53.36142706871033\n",
            "avg training loss: 1.310812916946411\n",
            "avg validation loss: 1.501017732225406\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 4995, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 2193, 57, 528, 3081, 109, 57, 794, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , care a sand castle with a trying .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 5651, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 83, 230, 104, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-02 15\n",
            "\n",
            "\n",
            "Completed training step 18760\n",
            "Completed training step 18780\n",
            "Completed training step 18800\n",
            "Completed training step 18820\n",
            "Completed training step 18840\n",
            "Completed training step 18860\n",
            "Completed training step 18880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_decoder_transformer = EncoderDecoderTransformer(num_encoder_layers, num_decoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, d_e, vocab_size, max_sequence_length, p_dropout, False).to(device)\n"
      ],
      "metadata": {
        "id": "VLT6fo1IjhG3"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict_filename = folder + \"encoder_decoder_transformer_state_dict_\" + \"2024-08-02 15\"\n",
        "state_dict = torch.load(state_dict_filename, map_location = device)\n",
        "print(state_dict.keys())\n",
        "encoder_decoder_transformer.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGBx0brYMJE6",
        "outputId": "d61eb338-07df-4152-ed4c-f32015ac9875"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['src_embedding.table.weight', 'tgt_embedding.table.weight', 'unembedding.weight.weight', 'unembedding.weight.bias', 'positionalEmbedding.table.weight', 'encoder.layers.0.multi_head_attention.weight_query', 'encoder.layers.0.multi_head_attention.weight_key', 'encoder.layers.0.multi_head_attention.weight_value', 'encoder.layers.0.multi_head_attention.weight_out', 'encoder.layers.0.multi_head_attention.bias_query', 'encoder.layers.0.multi_head_attention.bias_key', 'encoder.layers.0.multi_head_attention.bias_value', 'encoder.layers.0.multi_head_attention.bias_out', 'encoder.layers.0.layer_norm1.scale', 'encoder.layers.0.layer_norm1.offset', 'encoder.layers.0.feed_forward.mlp1', 'encoder.layers.0.feed_forward.mlp2', 'encoder.layers.0.feed_forward.mlp1_bias', 'encoder.layers.0.feed_forward.mlp2_bias', 'encoder.layers.0.layer_norm2.scale', 'encoder.layers.0.layer_norm2.offset', 'encoder.layers.1.multi_head_attention.weight_query', 'encoder.layers.1.multi_head_attention.weight_key', 'encoder.layers.1.multi_head_attention.weight_value', 'encoder.layers.1.multi_head_attention.weight_out', 'encoder.layers.1.multi_head_attention.bias_query', 'encoder.layers.1.multi_head_attention.bias_key', 'encoder.layers.1.multi_head_attention.bias_value', 'encoder.layers.1.multi_head_attention.bias_out', 'encoder.layers.1.layer_norm1.scale', 'encoder.layers.1.layer_norm1.offset', 'encoder.layers.1.feed_forward.mlp1', 'encoder.layers.1.feed_forward.mlp2', 'encoder.layers.1.feed_forward.mlp1_bias', 'encoder.layers.1.feed_forward.mlp2_bias', 'encoder.layers.1.layer_norm2.scale', 'encoder.layers.1.layer_norm2.offset', 'encoder.layers.2.multi_head_attention.weight_query', 'encoder.layers.2.multi_head_attention.weight_key', 'encoder.layers.2.multi_head_attention.weight_value', 'encoder.layers.2.multi_head_attention.weight_out', 'encoder.layers.2.multi_head_attention.bias_query', 'encoder.layers.2.multi_head_attention.bias_key', 'encoder.layers.2.multi_head_attention.bias_value', 'encoder.layers.2.multi_head_attention.bias_out', 'encoder.layers.2.layer_norm1.scale', 'encoder.layers.2.layer_norm1.offset', 'encoder.layers.2.feed_forward.mlp1', 'encoder.layers.2.feed_forward.mlp2', 'encoder.layers.2.feed_forward.mlp1_bias', 'encoder.layers.2.feed_forward.mlp2_bias', 'encoder.layers.2.layer_norm2.scale', 'encoder.layers.2.layer_norm2.offset', 'encoder.layers.3.multi_head_attention.weight_query', 'encoder.layers.3.multi_head_attention.weight_key', 'encoder.layers.3.multi_head_attention.weight_value', 'encoder.layers.3.multi_head_attention.weight_out', 'encoder.layers.3.multi_head_attention.bias_query', 'encoder.layers.3.multi_head_attention.bias_key', 'encoder.layers.3.multi_head_attention.bias_value', 'encoder.layers.3.multi_head_attention.bias_out', 'encoder.layers.3.layer_norm1.scale', 'encoder.layers.3.layer_norm1.offset', 'encoder.layers.3.feed_forward.mlp1', 'encoder.layers.3.feed_forward.mlp2', 'encoder.layers.3.feed_forward.mlp1_bias', 'encoder.layers.3.feed_forward.mlp2_bias', 'encoder.layers.3.layer_norm2.scale', 'encoder.layers.3.layer_norm2.offset', 'encoder.final_norm.scale', 'encoder.final_norm.offset', 'decoder.layers.0.multi_head_self_attention.weight_query', 'decoder.layers.0.multi_head_self_attention.weight_key', 'decoder.layers.0.multi_head_self_attention.weight_value', 'decoder.layers.0.multi_head_self_attention.weight_out', 'decoder.layers.0.multi_head_self_attention.bias_query', 'decoder.layers.0.multi_head_self_attention.bias_key', 'decoder.layers.0.multi_head_self_attention.bias_value', 'decoder.layers.0.multi_head_self_attention.bias_out', 'decoder.layers.0.layer_norm1.scale', 'decoder.layers.0.layer_norm1.offset', 'decoder.layers.0.multi_head_global_attention.weight_query', 'decoder.layers.0.multi_head_global_attention.weight_key', 'decoder.layers.0.multi_head_global_attention.weight_value', 'decoder.layers.0.multi_head_global_attention.weight_out', 'decoder.layers.0.multi_head_global_attention.bias_query', 'decoder.layers.0.multi_head_global_attention.bias_key', 'decoder.layers.0.multi_head_global_attention.bias_value', 'decoder.layers.0.multi_head_global_attention.bias_out', 'decoder.layers.0.layer_norm2.scale', 'decoder.layers.0.layer_norm2.offset', 'decoder.layers.0.feed_forward.mlp1', 'decoder.layers.0.feed_forward.mlp2', 'decoder.layers.0.feed_forward.mlp1_bias', 'decoder.layers.0.feed_forward.mlp2_bias', 'decoder.layers.0.layer_norm3.scale', 'decoder.layers.0.layer_norm3.offset', 'decoder.layers.1.multi_head_self_attention.weight_query', 'decoder.layers.1.multi_head_self_attention.weight_key', 'decoder.layers.1.multi_head_self_attention.weight_value', 'decoder.layers.1.multi_head_self_attention.weight_out', 'decoder.layers.1.multi_head_self_attention.bias_query', 'decoder.layers.1.multi_head_self_attention.bias_key', 'decoder.layers.1.multi_head_self_attention.bias_value', 'decoder.layers.1.multi_head_self_attention.bias_out', 'decoder.layers.1.layer_norm1.scale', 'decoder.layers.1.layer_norm1.offset', 'decoder.layers.1.multi_head_global_attention.weight_query', 'decoder.layers.1.multi_head_global_attention.weight_key', 'decoder.layers.1.multi_head_global_attention.weight_value', 'decoder.layers.1.multi_head_global_attention.weight_out', 'decoder.layers.1.multi_head_global_attention.bias_query', 'decoder.layers.1.multi_head_global_attention.bias_key', 'decoder.layers.1.multi_head_global_attention.bias_value', 'decoder.layers.1.multi_head_global_attention.bias_out', 'decoder.layers.1.layer_norm2.scale', 'decoder.layers.1.layer_norm2.offset', 'decoder.layers.1.feed_forward.mlp1', 'decoder.layers.1.feed_forward.mlp2', 'decoder.layers.1.feed_forward.mlp1_bias', 'decoder.layers.1.feed_forward.mlp2_bias', 'decoder.layers.1.layer_norm3.scale', 'decoder.layers.1.layer_norm3.offset', 'decoder.layers.2.multi_head_self_attention.weight_query', 'decoder.layers.2.multi_head_self_attention.weight_key', 'decoder.layers.2.multi_head_self_attention.weight_value', 'decoder.layers.2.multi_head_self_attention.weight_out', 'decoder.layers.2.multi_head_self_attention.bias_query', 'decoder.layers.2.multi_head_self_attention.bias_key', 'decoder.layers.2.multi_head_self_attention.bias_value', 'decoder.layers.2.multi_head_self_attention.bias_out', 'decoder.layers.2.layer_norm1.scale', 'decoder.layers.2.layer_norm1.offset', 'decoder.layers.2.multi_head_global_attention.weight_query', 'decoder.layers.2.multi_head_global_attention.weight_key', 'decoder.layers.2.multi_head_global_attention.weight_value', 'decoder.layers.2.multi_head_global_attention.weight_out', 'decoder.layers.2.multi_head_global_attention.bias_query', 'decoder.layers.2.multi_head_global_attention.bias_key', 'decoder.layers.2.multi_head_global_attention.bias_value', 'decoder.layers.2.multi_head_global_attention.bias_out', 'decoder.layers.2.layer_norm2.scale', 'decoder.layers.2.layer_norm2.offset', 'decoder.layers.2.feed_forward.mlp1', 'decoder.layers.2.feed_forward.mlp2', 'decoder.layers.2.feed_forward.mlp1_bias', 'decoder.layers.2.feed_forward.mlp2_bias', 'decoder.layers.2.layer_norm3.scale', 'decoder.layers.2.layer_norm3.offset', 'decoder.layers.3.multi_head_self_attention.weight_query', 'decoder.layers.3.multi_head_self_attention.weight_key', 'decoder.layers.3.multi_head_self_attention.weight_value', 'decoder.layers.3.multi_head_self_attention.weight_out', 'decoder.layers.3.multi_head_self_attention.bias_query', 'decoder.layers.3.multi_head_self_attention.bias_key', 'decoder.layers.3.multi_head_self_attention.bias_value', 'decoder.layers.3.multi_head_self_attention.bias_out', 'decoder.layers.3.layer_norm1.scale', 'decoder.layers.3.layer_norm1.offset', 'decoder.layers.3.multi_head_global_attention.weight_query', 'decoder.layers.3.multi_head_global_attention.weight_key', 'decoder.layers.3.multi_head_global_attention.weight_value', 'decoder.layers.3.multi_head_global_attention.weight_out', 'decoder.layers.3.multi_head_global_attention.bias_query', 'decoder.layers.3.multi_head_global_attention.bias_key', 'decoder.layers.3.multi_head_global_attention.bias_value', 'decoder.layers.3.multi_head_global_attention.bias_out', 'decoder.layers.3.layer_norm2.scale', 'decoder.layers.3.layer_norm2.offset', 'decoder.layers.3.feed_forward.mlp1', 'decoder.layers.3.feed_forward.mlp2', 'decoder.layers.3.feed_forward.mlp1_bias', 'decoder.layers.3.feed_forward.mlp2_bias', 'decoder.layers.3.layer_norm3.scale', 'decoder.layers.3.layer_norm3.offset', 'decoder.final_norm.scale', 'decoder.final_norm.offset'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_with_one_sample(model):\n",
        "    dataloader_iter = iter(train_dataloader)\n",
        "    for src_batch, tgt_batch in dataloader_iter:\n",
        "        # print(\"x:\", sequence_x)\n",
        "        # print(\"z:\", sequence_z)\n",
        "        # sequence_x, sequence_z = sequenceDataset.__getitem__(i)\n",
        "        src_batch = [src_batch[0]]\n",
        "        tgt_batch = [tgt_batch[0]]\n",
        "        src_tokens = torch.IntTensor([sequence.ids for sequence in src_batch]).to(device)\n",
        "        encoder_input = src_tokens[:, 1:]\n",
        "        print(\"encoder input\", encoder_input)\n",
        "        decoded_input = pad_collate.src_tokenizer.decode(encoder_input[0].tolist())\n",
        "        print(\"decoded input\", decoded_input)\n",
        "        train_tgt_tokens = torch.IntTensor([sequence.ids for sequence in tgt_batch]).to(device)\n",
        "        decoder_input = train_tgt_tokens[:, :-1]\n",
        "        print(\"decoder input\", decoder_input)\n",
        "        decoder_desired_output_train = train_tgt_tokens[:, 1:]\n",
        "        src_masks = torch.IntTensor([sequence.attention_mask for sequence in src_batch])[:, 1:].to(device)\n",
        "        tgt_masks = torch.IntTensor([sequence.attention_mask for sequence in tgt_batch])[:, :-1].to(device)\n",
        "        # print(\"src masks\", src_masks)\n",
        "        # print(\"tgt masks\", tgt_masks)\n",
        "        train_output = encoder_decoder_transformer(encoder_input, decoder_input, src_masks, tgt_masks)\n",
        "        print(train_output.shape)\n",
        "        decoded_output = decode(train_output[0], pad_collate.tgt_tokenizer)\n",
        "        print(\"decoded output\", decoded_output)\n",
        "        break\n",
        "\n",
        "test_model_with_one_sample(encoder_decoder_transformer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bertgYLZzZ7m",
        "outputId": "d317a4a1-4471-4e5b-952f-df11b98fbb28"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder input tensor([[  93,   89,   94, 1602,   15,    2]], device='cuda:0',\n",
            "       dtype=torch.int32)\n",
            "decoded input man on the sea .\n",
            "decoder input tensor([[   0,   30,   93,   89,   94, 1602,   15,    1]], device='cuda:0',\n",
            "       dtype=torch.int32)\n",
            "torch.Size([1, 8, 10000])\n",
            "argmax x: [30, 93, 89, 94, 1602, 15, 1, 2]\n",
            "decoded output A man on the sea .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(encoder_decoder_transformer.state_dict(), state_dict_filename)"
      ],
      "metadata": {
        "id": "kuI-KXWBa1Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_from_tokens(model, input, src_tokenizer, tgt_tokenizer):\n",
        "    model.disable_subsequent_mask()\n",
        "    src_tokenizer.no_padding()\n",
        "    tgt_tokenizer.no_padding()\n",
        "\n",
        "    src_tokenizer.no_truncation()\n",
        "    tgt_tokenizer.no_truncation()\n",
        "    src_sequence = input\n",
        "    print(src_sequence)\n",
        "    src_sequence = src_tokenizer.encode(src_sequence)\n",
        "    print(src_sequence)\n",
        "    print(src_tokenizer.decode(src_sequence.ids))\n",
        "    src_sequence = torch.IntTensor(src_sequence.ids).unsqueeze(0).to(device)\n",
        "    print(\"src tokens\", src_sequence)\n",
        "    tgt_sequence = torch.IntTensor([0]).unsqueeze(0).to(device)\n",
        "    src_mask = torch.ones(src_sequence.shape, dtype=torch.int32).to(device)\n",
        "    print(\"decoder input\", tgt_sequence)\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        length_gen = 10\n",
        "        for i in range(length_gen):\n",
        "            tgt_mask = torch.ones(tgt_sequence.shape, dtype=torch.int32).to(device)\n",
        "            prediction = model(src_sequence, tgt_sequence, src_mask, tgt_mask)\n",
        "            #print(\"prediction:\", prediction)\n",
        "            prediction = torch.softmax(prediction, -1)\n",
        "            #print(\"softmax prediction:\", prediction.shape)\n",
        "            prediction = torch.argmax(prediction, dim=-1)\n",
        "            print(\"argmax prediction:\", prediction)\n",
        "            print(\"actual prediction:\", tgt_tokenizer.decode(prediction[0].tolist()))\n",
        "            last_token = prediction[0][-1]\n",
        "            tgt_sequence = torch.cat((tgt_sequence, last_token.unsqueeze(0).unsqueeze(0)), dim=-1)\n",
        "            if last_token == 1:\n",
        "                break\n",
        "    return tgt_sequence\n",
        "\n",
        "tgt_sequence = predict_from_tokens(encoder_decoder_transformer, \"A man on the sea\", pad_collate.src_tokenizer, pad_collate.tgt_tokenizer)\n",
        "print(tgt_sequence)\n",
        "print(pad_collate.tgt_tokenizer.decode(tgt_sequence[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVEp1SmrhVuf",
        "outputId": "d431d4cd-989d-4bd5-f3bc-d489db4e7eb8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A man on the sea\n",
            "Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "A man on the sea\n",
            "src tokens tensor([[  30,   93,   89,   94, 1602]], device='cuda:0', dtype=torch.int32)\n",
            "decoder input tensor([[0]], device='cuda:0', dtype=torch.int32)\n",
            "argmax prediction: tensor([[1]], device='cuda:0')\n",
            "actual prediction: \n",
            "tensor([[0, 1]], device='cuda:0')\n",
            "\n"
          ]
        }
      ]
    }
  ]
}