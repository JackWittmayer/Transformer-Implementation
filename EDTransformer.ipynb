{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOoQDBDwbPQ+TEtEVSybj6R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JackWittmayer/Transformer-Implementation/blob/main/EDTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37L0J9nog5yz",
        "outputId": "4147a6c4-f68b-41d6-85be-377b263a7ddc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VNf0Rr3ogy-4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import os\n",
        "import pickle\n",
        "from unicodedata import normalize\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import log_softmax, pad\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(25)\n",
        "random.seed(25)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "2Hs0kEZYlwc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "507e2db4-5307-4537-d033-a5fe5893d911"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_X = torch.tensor([[3, 2, 0, 1], [1, 2, 3, 0]], dtype=torch.int32).to(device)\n",
        "SAMPLE_Z = torch.tensor([4, 1, 7, 6], dtype=torch.int32).to(device)"
      ],
      "metadata": {
        "id": "bMXiZFPpEtZa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printIfVerbose(verbose, tag, value):\n",
        "    if verbose:\n",
        "        print(tag, value)"
      ],
      "metadata": {
        "id": "6gsV7Wgv2DKh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        super().__init__()\n",
        "        self.table = nn.Embedding(vocab_size, embedding_size).to(device)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        embeddings = self.table(sequence)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "Pk7xtwam89Hh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_embedding():\n",
        "    torch.manual_seed(25)\n",
        "    vocab_size = 4\n",
        "    embedding = Embedding(vocab_size, 4)\n",
        "    print(\"weight:\", embedding.table.weight)\n",
        "    print(\"SAMPLE_X: \", SAMPLE_X)\n",
        "    output = embedding(SAMPLE_X)\n",
        "    print(\"output:\", output)\n",
        "    for j in range(len(output)):\n",
        "        #print(\"sample:\", sample)\n",
        "        for i in range(vocab_size):\n",
        "            assert output[j, i, :].eq(embedding.table.weight[SAMPLE_X[j, i]]).all()\n",
        "test_embedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StJF4qlIBYVj",
        "outputId": "6036723b-6f67-4819-a391-3d2af6f2d7c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight: Parameter containing:\n",
            "tensor([[ 0.0877, -0.6113,  0.3441, -1.2916],\n",
            "        [-0.5874,  0.8060,  1.3200,  0.4826],\n",
            "        [ 1.6671, -0.2342,  0.1074,  1.7852],\n",
            "        [ 0.7874, -0.2466,  0.2384, -0.6746]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "SAMPLE_X:  tensor([[3, 2, 0, 1],\n",
            "        [1, 2, 3, 0]], device='cuda:0', dtype=torch.int32)\n",
            "output: tensor([[[ 0.7874, -0.2466,  0.2384, -0.6746],\n",
            "         [ 1.6671, -0.2342,  0.1074,  1.7852],\n",
            "         [ 0.0877, -0.6113,  0.3441, -1.2916],\n",
            "         [-0.5874,  0.8060,  1.3200,  0.4826]],\n",
            "\n",
            "        [[-0.5874,  0.8060,  1.3200,  0.4826],\n",
            "         [ 1.6671, -0.2342,  0.1074,  1.7852],\n",
            "         [ 0.7874, -0.2466,  0.2384, -0.6746],\n",
            "         [ 0.0877, -0.6113,  0.3441, -1.2916]]], device='cuda:0',\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Unembedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Linear(embedding_size, vocab_size).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.weight(x)"
      ],
      "metadata": {
        "id": "EpoJIpc_CaX6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_unembedding():\n",
        "    torch.manual_seed(25)\n",
        "    vocab_size = 10\n",
        "    embedding_size = 4\n",
        "    sequence_length = 4\n",
        "    batch_size = 2\n",
        "    input = torch.rand(batch_size, sequence_length, embedding_size).to(device)\n",
        "    unembedding = Unembedding(vocab_size, embedding_size)\n",
        "\n",
        "    print(\"weight:\", unembedding.weight)\n",
        "    print(\"input: \", input)\n",
        "    output = unembedding(input)\n",
        "    print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, sequence_length, vocab_size)\n",
        "test_unembedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLZ0-F4xNOMf",
        "outputId": "cac79970-59cd-43ce-88ca-bc70a6a92b34"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight: Linear(in_features=4, out_features=10, bias=True)\n",
            "input:  tensor([[[0.7518, 0.1929, 0.0629, 0.9118],\n",
            "         [0.3828, 0.2990, 0.5933, 0.2911],\n",
            "         [0.2416, 0.5582, 0.0481, 0.3497],\n",
            "         [0.3520, 0.9528, 0.0284, 0.8488]],\n",
            "\n",
            "        [[0.3947, 0.5181, 0.9726, 0.8813],\n",
            "         [0.0056, 0.3056, 0.9384, 0.7949],\n",
            "         [0.4399, 0.1766, 0.8739, 0.1425],\n",
            "         [0.4682, 0.6254, 0.3040, 0.7923]]], device='cuda:0')\n",
            "output: tensor([[[-4.9334e-01,  3.9030e-01, -3.4348e-03,  2.0479e-01, -1.7155e-01,\n",
            "           3.0325e-01,  9.5298e-01, -9.2740e-01,  3.5209e-01, -2.1405e-02],\n",
            "         [-6.2972e-02, -6.9980e-02, -5.4304e-02,  1.2675e-01, -5.5075e-01,\n",
            "           1.8844e-01,  8.6408e-01, -5.4956e-01,  4.7789e-01,  7.8078e-02],\n",
            "         [-2.9110e-01,  3.9284e-02,  7.2926e-02,  2.0875e-01, -3.4683e-01,\n",
            "           1.1962e-01,  7.2445e-01, -5.6804e-01,  4.2547e-01,  1.0732e-02],\n",
            "         [-3.4147e-01,  6.4171e-02, -5.8167e-02,  1.8160e-01, -2.1651e-01,\n",
            "          -1.4458e-01,  1.0255e+00, -9.2857e-01,  5.6889e-01, -7.0657e-02]],\n",
            "\n",
            "        [[ 3.2716e-02, -1.6396e-01, -2.2999e-01,  1.5606e-01, -5.7223e-01,\n",
            "          -1.2983e-01,  1.2599e+00, -8.8230e-01,  6.9367e-01,  1.1626e-01],\n",
            "         [ 7.3857e-03, -2.2392e-01, -6.1284e-02,  3.6410e-01, -6.3714e-01,\n",
            "          -1.2969e-01,  1.0509e+00, -6.4156e-01,  6.5060e-01,  2.7214e-01],\n",
            "         [ 8.3425e-02, -1.5596e-01, -1.1466e-01,  6.0901e-02, -6.8152e-01,\n",
            "           2.4368e-01,  8.9560e-01, -4.8380e-01,  4.9643e-01,  1.0562e-01],\n",
            "         [-2.5832e-01,  7.1240e-02, -8.8499e-02,  1.6288e-01, -3.2179e-01,\n",
            "          -1.3083e-04,  1.0437e+00, -8.7687e-01,  5.3716e-01, -1.0231e-02]]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, embedding_size, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.table = nn.Embedding(max_sequence_length, embedding_size).to(device)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        positions = torch.zeros(sequence.shape, dtype=torch.int32)\n",
        "        positions[:, ::] = torch.arange(0, sequence.shape[-1])\n",
        "        #print(\"positions\", positions)\n",
        "        positional_embeddings = self.table(positions.to(device))\n",
        "        return positional_embeddings"
      ],
      "metadata": {
        "id": "5xpYM9Zf_KLw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_positional_embedding():\n",
        "    embedding_size = 8\n",
        "    max_sequence_length = 10\n",
        "    batch_size = 2\n",
        "    positional_embedding = PositionalEmbedding(embedding_size, max_sequence_length)\n",
        "    output = positional_embedding(SAMPLE_X)\n",
        "    print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, SAMPLE_X.shape[-1], embedding_size)\n",
        "test_positional_embedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l1Z_ZH8Pb_P",
        "outputId": "e073ef9b-1515-41f6-b037-85bc7c666937"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output: tensor([[[ 0.3412, -0.2888, -0.4162, -1.2433,  0.3364, -2.1897, -0.2086,\n",
            "           0.0196],\n",
            "         [ 0.2461, -0.0812, -0.4464, -1.2595,  0.5963, -1.3647, -0.7684,\n",
            "           0.3472],\n",
            "         [-0.0142,  0.1426,  2.0701, -0.1623, -0.4448,  0.8318, -0.2930,\n",
            "          -0.2068],\n",
            "         [-0.8096,  1.2487,  0.5594, -0.3657,  0.5478, -1.4327, -1.4111,\n",
            "          -0.4237]],\n",
            "\n",
            "        [[ 0.3412, -0.2888, -0.4162, -1.2433,  0.3364, -2.1897, -0.2086,\n",
            "           0.0196],\n",
            "         [ 0.2461, -0.0812, -0.4464, -1.2595,  0.5963, -1.3647, -0.7684,\n",
            "           0.3472],\n",
            "         [-0.0142,  0.1426,  2.0701, -0.1623, -0.4448,  0.8318, -0.2930,\n",
            "          -0.2068],\n",
            "         [-0.8096,  1.2487,  0.5594, -0.3657,  0.5478, -1.4327, -1.4111,\n",
            "          -0.4237]]], device='cuda:0', grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(queries, keys, values, mask, dropout, verbose):\n",
        "    printIfVerbose(verbose, \"queries:\", queries)\n",
        "    printIfVerbose(verbose, \"keys:\", keys)\n",
        "    printIfVerbose(verbose, \"values:\", values)\n",
        "    keys_transposed = torch.transpose(keys, -2, -1)\n",
        "    printIfVerbose(verbose, \"keys_transposed:\", keys_transposed)\n",
        "    scores = torch.matmul(queries, keys_transposed)\n",
        "    #assert scores.shape == (keys.shape[0], keys.shape[-1], queries.shape[-1])\n",
        "    printIfVerbose(verbose, \"scores:\", scores)\n",
        "    printIfVerbose(verbose, \"scores:\", scores.shape)\n",
        "    printIfVerbose(verbose, \"masks:\", mask.shape)\n",
        "    scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    printIfVerbose(verbose, \"masked scores:\", scores)\n",
        "    d_attn = keys.shape[-1]\n",
        "    scaled_scores = scores / math.sqrt(d_attn)\n",
        "    printIfVerbose(verbose, \"scaled_scores:\", scaled_scores)\n",
        "    softmax_scores = torch.softmax(scaled_scores, -1)\n",
        "    softmax_scores = dropout(softmax_scores)\n",
        "    printIfVerbose(verbose, \"softmax_scores:\", softmax_scores)\n",
        "    printIfVerbose(verbose, \"softmax_socres shape:\", softmax_scores.shape)\n",
        "    printIfVerbose(verbose, \"values:\", values)\n",
        "    v_out = torch.matmul(softmax_scores, values)\n",
        "    return v_out"
      ],
      "metadata": {
        "id": "DL3t3E_ThGF2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_attention():\n",
        "    d_attn = 4\n",
        "    length_x = 4\n",
        "    length_z = 3\n",
        "    batch_size = 2\n",
        "    d_out = 2\n",
        "\n",
        "    queries = torch.rand(batch_size, length_x, d_attn)\n",
        "    keys = torch.rand(batch_size, length_z, d_attn)\n",
        "    values = torch.rand(batch_size, length_z, d_out)\n",
        "    mask = torch.tril(torch.ones(length_x, length_z) == 1)\n",
        "    padding_mask = torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0]], dtype=torch.int32)\n",
        "\n",
        "    v_out = attention(queries, keys, values, mask, nn.Dropout(0.1), True)\n",
        "    #print(\"output:\", v_out)\n",
        "    assert v_out.shape == (batch_size, length_x, d_out)\n",
        "test_attention()"
      ],
      "metadata": {
        "id": "KhGXLYp6i61z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7efe888e-bc50-4209-ce70-d5452c62ad00"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "queries: tensor([[[0.4961, 0.6278, 0.3572, 0.5220],\n",
            "         [0.1997, 0.5286, 0.4723, 0.0238],\n",
            "         [0.1838, 0.2010, 0.1765, 0.8587],\n",
            "         [0.7776, 0.1199, 0.8638, 0.1066]],\n",
            "\n",
            "        [[0.1084, 0.8448, 0.7043, 0.9275],\n",
            "         [0.3953, 0.2704, 0.6228, 0.6078],\n",
            "         [0.7686, 0.3296, 0.4959, 0.0065],\n",
            "         [0.9125, 0.8358, 0.6698, 0.4129]]])\n",
            "keys: tensor([[[0.0129, 0.5052, 0.5967, 0.3134],\n",
            "         [0.1648, 0.4834, 0.2368, 0.7654],\n",
            "         [0.9255, 0.3393, 0.5612, 0.0953]],\n",
            "\n",
            "        [[0.5582, 0.5739, 0.5244, 0.6292],\n",
            "         [0.7426, 0.3134, 0.7793, 0.9385],\n",
            "         [0.1588, 0.3427, 0.3863, 0.2306]]])\n",
            "values: tensor([[[0.1533, 0.0876],\n",
            "         [0.9218, 0.8859],\n",
            "         [0.6448, 0.5202]],\n",
            "\n",
            "        [[0.3174, 0.8487],\n",
            "         [0.8658, 0.5804],\n",
            "         [0.1021, 0.1329]]])\n",
            "keys_transposed: tensor([[[0.0129, 0.1648, 0.9255],\n",
            "         [0.5052, 0.4834, 0.3393],\n",
            "         [0.5967, 0.2368, 0.5612],\n",
            "         [0.3134, 0.7654, 0.0953]],\n",
            "\n",
            "        [[0.5582, 0.7426, 0.1588],\n",
            "         [0.5739, 0.3134, 0.3427],\n",
            "         [0.5244, 0.7793, 0.3863],\n",
            "         [0.6292, 0.9385, 0.2306]]])\n",
            "scores: tensor([[[0.7003, 0.8693, 0.9223],\n",
            "         [0.5590, 0.4186, 0.6315],\n",
            "         [0.4784, 0.8265, 0.4192],\n",
            "         [0.6194, 0.4722, 1.2552]],\n",
            "\n",
            "        [[1.4984, 1.7646, 0.7927],\n",
            "         [1.0850, 1.4341, 0.5363],\n",
            "         [0.8825, 1.0667, 0.4282],\n",
            "         [1.6002, 1.8490, 0.7854]]])\n",
            "scores: torch.Size([2, 4, 3])\n",
            "masks: torch.Size([4, 3])\n",
            "masked scores: tensor([[[ 7.0027e-01, -1.0000e+09, -1.0000e+09],\n",
            "         [ 5.5897e-01,  4.1855e-01, -1.0000e+09],\n",
            "         [ 4.7836e-01,  8.2648e-01,  4.1922e-01],\n",
            "         [ 6.1941e-01,  4.7223e-01,  1.2552e+00]],\n",
            "\n",
            "        [[ 1.4984e+00, -1.0000e+09, -1.0000e+09],\n",
            "         [ 1.0850e+00,  1.4341e+00, -1.0000e+09],\n",
            "         [ 8.8246e-01,  1.0667e+00,  4.2816e-01],\n",
            "         [ 1.6002e+00,  1.8490e+00,  7.8537e-01]]])\n",
            "scaled_scores: tensor([[[ 3.5014e-01, -5.0000e+08, -5.0000e+08],\n",
            "         [ 2.7948e-01,  2.0928e-01, -5.0000e+08],\n",
            "         [ 2.3918e-01,  4.1324e-01,  2.0961e-01],\n",
            "         [ 3.0970e-01,  2.3612e-01,  6.2761e-01]],\n",
            "\n",
            "        [[ 7.4918e-01, -5.0000e+08, -5.0000e+08],\n",
            "         [ 5.4248e-01,  7.1705e-01, -5.0000e+08],\n",
            "         [ 4.4123e-01,  5.3335e-01,  2.1408e-01],\n",
            "         [ 8.0008e-01,  9.2452e-01,  3.9269e-01]]])\n",
            "softmax_scores: tensor([[[1.1111, 0.0000, 0.0000],\n",
            "         [0.5751, 0.5361, 0.0000],\n",
            "         [0.3515, 0.4183, 0.3413],\n",
            "         [0.3364, 0.3125, 0.0000]],\n",
            "\n",
            "        [[1.1111, 0.0000, 0.0000],\n",
            "         [0.5072, 0.6039, 0.0000],\n",
            "         [0.0000, 0.4211, 0.3060],\n",
            "         [0.0000, 0.4497, 0.2642]]])\n",
            "softmax_socres shape: torch.Size([2, 4, 3])\n",
            "values: tensor([[[0.1533, 0.0876],\n",
            "         [0.9218, 0.8859],\n",
            "         [0.6448, 0.5202]],\n",
            "\n",
            "        [[0.3174, 0.8487],\n",
            "         [0.8658, 0.5804],\n",
            "         [0.1021, 0.1329]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "class MaskStrategy(Enum):\n",
        "    UNMASKED = 1\n",
        "    MASKED = 2"
      ],
      "metadata": {
        "id": "wUJ-CbaCB9g-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, num_heads, d_attn, d_x, d_z, d_out, d_mid, maskStrategy, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.num_heads = num_heads\n",
        "        self.d_attn = d_attn\n",
        "        self.d_x = d_x\n",
        "        self.d_z = d_z\n",
        "        self.d_out = d_out\n",
        "        self.d_mid = d_mid\n",
        "        self.maskStrategy = maskStrategy\n",
        "        self.weight_query = nn.Linear(d_x, d_attn).to(device)\n",
        "        self.weight_key = nn.Linear(d_z, d_attn).to(device)\n",
        "        self.weight_value = nn.Linear(d_z, d_mid).to(device)\n",
        "        self.weight_out = nn.Linear(d_mid, d_out).to(device)\n",
        "        self.dropout = nn.Dropout(p_dropout)\n",
        "\n",
        "    def forward(self, z, x, padding_mask):\n",
        "        length_z = z.shape[-2]\n",
        "        length_x = x.shape[-2]\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        queries = self.weight_query(x).view(batch_size, length_x, self.num_heads, -1).transpose(1, 2)\n",
        "        keys = self.weight_key(z).view(batch_size, length_z, self.num_heads, -1).transpose(1, 2)\n",
        "        values = self.weight_value(z).view(batch_size, length_z, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        assert queries.shape == (batch_size, self.num_heads, length_x, self.d_attn / self.num_heads)\n",
        "        assert keys.shape == (batch_size, self.num_heads, length_z, self.d_attn / self.num_heads)\n",
        "        assert values.shape == (batch_size, self.num_heads, length_z, self.d_mid / self.num_heads)\n",
        "\n",
        "        if self.maskStrategy == MaskStrategy['UNMASKED']:\n",
        "            mask = padding_mask.unsqueeze(-2)\n",
        "        elif self.maskStrategy == MaskStrategy['MASKED']:\n",
        "            padding_mask = padding_mask.unsqueeze(-2)\n",
        "            mask = torch.tril(torch.ones(length_x, length_z) == 1).to(device)\n",
        "            printIfVerbose(self.verbose, \"padding mask:\", padding_mask.shape)\n",
        "            printIfVerbose(self.verbose, \"mask tril\", mask)\n",
        "            mask = mask & padding_mask\n",
        "            printIfVerbose(self.verbose, \"merged mask:\", mask)\n",
        "        mask = mask.unsqueeze(1)\n",
        "        printIfVerbose(self.verbose, \"mask\", mask)\n",
        "        printIfVerbose(self.verbose, \"mask\", mask.shape)\n",
        "        v_out = attention(queries, keys, values, mask, self.dropout, self.verbose)\n",
        "        printIfVerbose(self.verbose, \"v_out shape\", v_out.shape)\n",
        "        assert v_out.shape == (batch_size, self.num_heads, length_x, self.d_mid / self.num_heads)\n",
        "        printIfVerbose(self.verbose, \"v_out:\", v_out)\n",
        "        printIfVerbose(self.verbose, \"v_out shape before:\", v_out.shape)\n",
        "        v_out = v_out.transpose(1, 2).reshape(batch_size, length_x, -1)\n",
        "        printIfVerbose(self.verbose, \"v_out shape:\", v_out.shape)\n",
        "        printIfVerbose(self.verbose, \"v_out reshaped:\", v_out)\n",
        "        output = self.weight_out(v_out)\n",
        "        printIfVerbose(self.verbose, \"output shape\", output.shape)\n",
        "        assert output.shape == (batch_size, length_x, self.d_out)\n",
        "        return output\n",
        "\n",
        "    def disable_subsequent_mask(self):\n",
        "        self.maskStrategy = MaskStrategy['UNMASKED']\n",
        "\n",
        "    def enable_subsequent_mask(self):\n",
        "        self.maskStrategy = MaskStrategy['MASKED']\n"
      ],
      "metadata": {
        "id": "CX2A1i9Z4lVO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_headed_attention_encoder_fixed():\n",
        "    num_heads = 1\n",
        "    d_attn = 4\n",
        "    d_x = 4\n",
        "    d_z = 4\n",
        "    d_out = 1\n",
        "    d_mid = 3\n",
        "    length_z = 3\n",
        "    batch_size = 1\n",
        "    padding_mask = torch.tensor([[1, 1, 0]], dtype=torch.int32).to(device)\n",
        "\n",
        "    multi_headed_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], 0.0, True).to(device)\n",
        "    z = torch.tensor([[[1, 0, 1, 0], [0, 2, 0, 2], [1, 1, 1, 1]]], dtype=torch.float32).to(device)\n",
        "    #print(\"z:\", z\n",
        "    output = multi_headed_attention(z, z, padding_mask)\n",
        "    #print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, length_z, d_out)\n",
        "test_multi_headed_attention_encoder_fixed()"
      ],
      "metadata": {
        "id": "kJuRy-3dTMY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db909e35-5aa6-4a82-90aa-b565cf45cac8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mask tensor([[[[1, 1, 0]]]], device='cuda:0', dtype=torch.int32)\n",
            "mask torch.Size([1, 1, 1, 3])\n",
            "queries: tensor([[[[-0.8182,  0.9362,  0.3153,  0.4970],\n",
            "          [ 0.8439,  0.4537,  0.0540,  1.0113],\n",
            "          [-0.2946,  0.9878,  0.2263,  0.8214]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "keys: tensor([[[[-0.9612,  0.6361, -0.7727,  1.0188],\n",
            "          [-0.5443, -1.1392, -0.0485,  1.6070],\n",
            "          [-1.0001, -0.1185, -0.8126,  1.5968]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "values: tensor([[[[ 0.6987, -0.6661, -0.9703],\n",
            "          [ 0.6950, -0.5483, -0.6054],\n",
            "          [ 0.9841, -0.7190, -1.0553]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "keys_transposed: tensor([[[[-0.9612, -0.5443, -1.0001],\n",
            "          [ 0.6361, -1.1392, -0.1185],\n",
            "          [-0.7727, -0.0485, -0.8126],\n",
            "          [ 1.0188,  1.6070,  1.5968]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "scores: tensor([[[[1.6446, 0.1622, 1.2447],\n",
            "          [0.4659, 0.6464, 0.6732],\n",
            "          [1.5734, 0.3442, 1.3053]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "scores: torch.Size([1, 1, 3, 3])\n",
            "masks: torch.Size([1, 1, 1, 3])\n",
            "masked scores: tensor([[[[ 1.6446e+00,  1.6222e-01, -1.0000e+09],\n",
            "          [ 4.6595e-01,  6.4635e-01, -1.0000e+09],\n",
            "          [ 1.5734e+00,  3.4417e-01, -1.0000e+09]]]], device='cuda:0',\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "scaled_scores: tensor([[[[ 8.2229e-01,  8.1108e-02, -5.0000e+08],\n",
            "          [ 2.3297e-01,  3.2318e-01, -5.0000e+08],\n",
            "          [ 7.8672e-01,  1.7209e-01, -5.0000e+08]]]], device='cuda:0',\n",
            "       grad_fn=<DivBackward0>)\n",
            "softmax_scores: tensor([[[[0.6773, 0.3227, 0.0000],\n",
            "          [0.4775, 0.5225, 0.0000],\n",
            "          [0.6490, 0.3510, 0.0000]]]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "softmax_socres shape: torch.Size([1, 1, 3, 3])\n",
            "values: tensor([[[[ 0.6987, -0.6661, -0.9703],\n",
            "          [ 0.6950, -0.5483, -0.6054],\n",
            "          [ 0.9841, -0.7190, -1.0553]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "v_out shape torch.Size([1, 1, 3, 3])\n",
            "v_out: tensor([[[[ 0.6976, -0.6281, -0.8525],\n",
            "          [ 0.6968, -0.6046, -0.7796],\n",
            "          [ 0.6974, -0.6248, -0.8422]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "v_out shape before: torch.Size([1, 1, 3, 3])\n",
            "v_out shape: torch.Size([1, 3, 3])\n",
            "v_out reshaped: tensor([[[ 0.6976, -0.6281, -0.8525],\n",
            "         [ 0.6968, -0.6046, -0.7796],\n",
            "         [ 0.6974, -0.6248, -0.8422]]], device='cuda:0',\n",
            "       grad_fn=<ViewBackward0>)\n",
            "output shape torch.Size([1, 3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_headed_attention_encoder():\n",
        "    num_heads = 4\n",
        "    d_attn = 4\n",
        "    d_x = 4\n",
        "    d_z = 4\n",
        "    d_out = 1\n",
        "    d_mid = 4\n",
        "    length_z = 3\n",
        "    batch_size = 3\n",
        "    padding_mask = torch.tensor([[1, 1, 0], [1, 1, 0], [1, 1, 1]], dtype=torch.int32).to(device)\n",
        "\n",
        "    multi_headed_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], 0.0, True).to(device)\n",
        "    z = torch.tensor([[[1, 0, 1, 0], [0, 2, 0, 2], [1, 1, 1, 1]],\n",
        "                      [[1, 0, 1, 0], [0, 2, 0, 2], [1, 1, 1, 1]],\n",
        "                      [[1, 0, 1, 0], [0, 2, 0, 2], [1, 1, 1, 1]]], dtype=torch.float32).to(device)\n",
        "    #print(\"z:\", z\n",
        "    output = multi_headed_attention(z, z, padding_mask)\n",
        "    #print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, length_z, d_out)\n",
        "test_multi_headed_attention_encoder()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9CYHYJbMObt",
        "outputId": "aa7f0773-426e-4e53-e170-b800f153e641"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mask tensor([[[[1, 1, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 1]]]], device='cuda:0', dtype=torch.int32)\n",
            "mask torch.Size([3, 1, 1, 3])\n",
            "queries: tensor([[[[-0.4361],\n",
            "          [-1.3334],\n",
            "          [-0.8685]],\n",
            "\n",
            "         [[-0.2439],\n",
            "          [ 0.0877],\n",
            "          [-0.1861]],\n",
            "\n",
            "         [[ 0.5704],\n",
            "          [-0.6079],\n",
            "          [ 0.1567]],\n",
            "\n",
            "         [[-0.9431],\n",
            "          [-0.2173],\n",
            "          [-0.9932]]],\n",
            "\n",
            "\n",
            "        [[[-0.4361],\n",
            "          [-1.3334],\n",
            "          [-0.8685]],\n",
            "\n",
            "         [[-0.2439],\n",
            "          [ 0.0877],\n",
            "          [-0.1861]],\n",
            "\n",
            "         [[ 0.5704],\n",
            "          [-0.6079],\n",
            "          [ 0.1567]],\n",
            "\n",
            "         [[-0.9431],\n",
            "          [-0.2173],\n",
            "          [-0.9932]]],\n",
            "\n",
            "\n",
            "        [[[-0.4361],\n",
            "          [-1.3334],\n",
            "          [-0.8685]],\n",
            "\n",
            "         [[-0.2439],\n",
            "          [ 0.0877],\n",
            "          [-0.1861]],\n",
            "\n",
            "         [[ 0.5704],\n",
            "          [-0.6079],\n",
            "          [ 0.1567]],\n",
            "\n",
            "         [[-0.9431],\n",
            "          [-0.2173],\n",
            "          [-0.9932]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "keys: tensor([[[[ 0.9490],\n",
            "          [-0.1129],\n",
            "          [ 0.7279]],\n",
            "\n",
            "         [[-0.5212],\n",
            "          [ 1.1913],\n",
            "          [-0.0508]],\n",
            "\n",
            "         [[ 0.6641],\n",
            "          [ 0.0447],\n",
            "          [ 0.4939]],\n",
            "\n",
            "         [[-0.2617],\n",
            "          [ 0.8361],\n",
            "          [ 0.1769]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9490],\n",
            "          [-0.1129],\n",
            "          [ 0.7279]],\n",
            "\n",
            "         [[-0.5212],\n",
            "          [ 1.1913],\n",
            "          [-0.0508]],\n",
            "\n",
            "         [[ 0.6641],\n",
            "          [ 0.0447],\n",
            "          [ 0.4939]],\n",
            "\n",
            "         [[-0.2617],\n",
            "          [ 0.8361],\n",
            "          [ 0.1769]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9490],\n",
            "          [-0.1129],\n",
            "          [ 0.7279]],\n",
            "\n",
            "         [[-0.5212],\n",
            "          [ 1.1913],\n",
            "          [-0.0508]],\n",
            "\n",
            "         [[ 0.6641],\n",
            "          [ 0.0447],\n",
            "          [ 0.4939]],\n",
            "\n",
            "         [[-0.2617],\n",
            "          [ 0.8361],\n",
            "          [ 0.1769]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "values: tensor([[[[-0.1733],\n",
            "          [ 0.1845],\n",
            "          [-0.2711]],\n",
            "\n",
            "         [[ 0.3856],\n",
            "          [-0.1036],\n",
            "          [ 0.1591]],\n",
            "\n",
            "         [[-0.0492],\n",
            "          [-1.5056],\n",
            "          [-0.5631]],\n",
            "\n",
            "         [[ 0.7909],\n",
            "          [-0.8326],\n",
            "          [ 0.1586]]],\n",
            "\n",
            "\n",
            "        [[[-0.1733],\n",
            "          [ 0.1845],\n",
            "          [-0.2711]],\n",
            "\n",
            "         [[ 0.3856],\n",
            "          [-0.1036],\n",
            "          [ 0.1591]],\n",
            "\n",
            "         [[-0.0492],\n",
            "          [-1.5056],\n",
            "          [-0.5631]],\n",
            "\n",
            "         [[ 0.7909],\n",
            "          [-0.8326],\n",
            "          [ 0.1586]]],\n",
            "\n",
            "\n",
            "        [[[-0.1733],\n",
            "          [ 0.1845],\n",
            "          [-0.2711]],\n",
            "\n",
            "         [[ 0.3856],\n",
            "          [-0.1036],\n",
            "          [ 0.1591]],\n",
            "\n",
            "         [[-0.0492],\n",
            "          [-1.5056],\n",
            "          [-0.5631]],\n",
            "\n",
            "         [[ 0.7909],\n",
            "          [-0.8326],\n",
            "          [ 0.1586]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "keys_transposed: tensor([[[[ 0.9490, -0.1129,  0.7279]],\n",
            "\n",
            "         [[-0.5212,  1.1913, -0.0508]],\n",
            "\n",
            "         [[ 0.6641,  0.0447,  0.4939]],\n",
            "\n",
            "         [[-0.2617,  0.8361,  0.1769]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9490, -0.1129,  0.7279]],\n",
            "\n",
            "         [[-0.5212,  1.1913, -0.0508]],\n",
            "\n",
            "         [[ 0.6641,  0.0447,  0.4939]],\n",
            "\n",
            "         [[-0.2617,  0.8361,  0.1769]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9490, -0.1129,  0.7279]],\n",
            "\n",
            "         [[-0.5212,  1.1913, -0.0508]],\n",
            "\n",
            "         [[ 0.6641,  0.0447,  0.4939]],\n",
            "\n",
            "         [[-0.2617,  0.8361,  0.1769]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "scores: tensor([[[[-0.4139,  0.0493, -0.3175],\n",
            "          [-1.2654,  0.1506, -0.9706],\n",
            "          [-0.8242,  0.0981, -0.6322]],\n",
            "\n",
            "         [[ 0.1271, -0.2905,  0.0124],\n",
            "          [-0.0457,  0.1045, -0.0045],\n",
            "          [ 0.0970, -0.2217,  0.0095]],\n",
            "\n",
            "         [[ 0.3788,  0.0255,  0.2817],\n",
            "          [-0.4037, -0.0271, -0.3002],\n",
            "          [ 0.1040,  0.0070,  0.0774]],\n",
            "\n",
            "         [[ 0.2468, -0.7885, -0.1668],\n",
            "          [ 0.0569, -0.1817, -0.0384],\n",
            "          [ 0.2599, -0.8304, -0.1757]]],\n",
            "\n",
            "\n",
            "        [[[-0.4139,  0.0493, -0.3175],\n",
            "          [-1.2654,  0.1506, -0.9706],\n",
            "          [-0.8242,  0.0981, -0.6322]],\n",
            "\n",
            "         [[ 0.1271, -0.2905,  0.0124],\n",
            "          [-0.0457,  0.1045, -0.0045],\n",
            "          [ 0.0970, -0.2217,  0.0095]],\n",
            "\n",
            "         [[ 0.3788,  0.0255,  0.2817],\n",
            "          [-0.4037, -0.0271, -0.3002],\n",
            "          [ 0.1040,  0.0070,  0.0774]],\n",
            "\n",
            "         [[ 0.2468, -0.7885, -0.1668],\n",
            "          [ 0.0569, -0.1817, -0.0384],\n",
            "          [ 0.2599, -0.8304, -0.1757]]],\n",
            "\n",
            "\n",
            "        [[[-0.4139,  0.0493, -0.3175],\n",
            "          [-1.2654,  0.1506, -0.9706],\n",
            "          [-0.8242,  0.0981, -0.6322]],\n",
            "\n",
            "         [[ 0.1271, -0.2905,  0.0124],\n",
            "          [-0.0457,  0.1045, -0.0045],\n",
            "          [ 0.0970, -0.2217,  0.0095]],\n",
            "\n",
            "         [[ 0.3788,  0.0255,  0.2817],\n",
            "          [-0.4037, -0.0271, -0.3002],\n",
            "          [ 0.1040,  0.0070,  0.0774]],\n",
            "\n",
            "         [[ 0.2468, -0.7885, -0.1668],\n",
            "          [ 0.0569, -0.1817, -0.0384],\n",
            "          [ 0.2599, -0.8304, -0.1757]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "scores: torch.Size([3, 4, 3, 3])\n",
            "masks: torch.Size([3, 1, 1, 3])\n",
            "masked scores: tensor([[[[-4.1389e-01,  4.9254e-02, -1.0000e+09],\n",
            "          [-1.2654e+00,  1.5059e-01, -1.0000e+09],\n",
            "          [-8.2422e-01,  9.8084e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.2710e-01, -2.9051e-01, -1.0000e+09],\n",
            "          [-4.5728e-02,  1.0452e-01, -1.0000e+09],\n",
            "          [ 9.6988e-02, -2.2169e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.7880e-01,  2.5476e-02, -1.0000e+09],\n",
            "          [-4.0367e-01, -2.7149e-02, -1.0000e+09],\n",
            "          [ 1.0404e-01,  6.9973e-03, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.4679e-01, -7.8853e-01, -1.0000e+09],\n",
            "          [ 5.6863e-02, -1.8169e-01, -1.0000e+09],\n",
            "          [ 2.5991e-01, -8.3045e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-4.1389e-01,  4.9254e-02, -1.0000e+09],\n",
            "          [-1.2654e+00,  1.5059e-01, -1.0000e+09],\n",
            "          [-8.2422e-01,  9.8084e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.2710e-01, -2.9051e-01, -1.0000e+09],\n",
            "          [-4.5728e-02,  1.0452e-01, -1.0000e+09],\n",
            "          [ 9.6988e-02, -2.2169e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.7880e-01,  2.5476e-02, -1.0000e+09],\n",
            "          [-4.0367e-01, -2.7149e-02, -1.0000e+09],\n",
            "          [ 1.0404e-01,  6.9973e-03, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.4679e-01, -7.8853e-01, -1.0000e+09],\n",
            "          [ 5.6863e-02, -1.8169e-01, -1.0000e+09],\n",
            "          [ 2.5991e-01, -8.3045e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-4.1389e-01,  4.9254e-02, -3.1747e-01],\n",
            "          [-1.2654e+00,  1.5059e-01, -9.7064e-01],\n",
            "          [-8.2422e-01,  9.8084e-02, -6.3221e-01]],\n",
            "\n",
            "         [[ 1.2710e-01, -2.9051e-01,  1.2385e-02],\n",
            "          [-4.5728e-02,  1.0452e-01, -4.4561e-03],\n",
            "          [ 9.6988e-02, -2.2169e-01,  9.4512e-03]],\n",
            "\n",
            "         [[ 3.7880e-01,  2.5476e-02,  2.8173e-01],\n",
            "          [-4.0367e-01, -2.7149e-02, -3.0023e-01],\n",
            "          [ 1.0404e-01,  6.9973e-03,  7.7381e-02]],\n",
            "\n",
            "         [[ 2.4679e-01, -7.8853e-01, -1.6683e-01],\n",
            "          [ 5.6863e-02, -1.8169e-01, -3.8439e-02],\n",
            "          [ 2.5991e-01, -8.3045e-01, -1.7570e-01]]]], device='cuda:0',\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "scaled_scores: tensor([[[[-4.1389e-01,  4.9254e-02, -1.0000e+09],\n",
            "          [-1.2654e+00,  1.5059e-01, -1.0000e+09],\n",
            "          [-8.2422e-01,  9.8084e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.2710e-01, -2.9051e-01, -1.0000e+09],\n",
            "          [-4.5728e-02,  1.0452e-01, -1.0000e+09],\n",
            "          [ 9.6988e-02, -2.2169e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.7880e-01,  2.5476e-02, -1.0000e+09],\n",
            "          [-4.0367e-01, -2.7149e-02, -1.0000e+09],\n",
            "          [ 1.0404e-01,  6.9973e-03, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.4679e-01, -7.8853e-01, -1.0000e+09],\n",
            "          [ 5.6863e-02, -1.8169e-01, -1.0000e+09],\n",
            "          [ 2.5991e-01, -8.3045e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-4.1389e-01,  4.9254e-02, -1.0000e+09],\n",
            "          [-1.2654e+00,  1.5059e-01, -1.0000e+09],\n",
            "          [-8.2422e-01,  9.8084e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.2710e-01, -2.9051e-01, -1.0000e+09],\n",
            "          [-4.5728e-02,  1.0452e-01, -1.0000e+09],\n",
            "          [ 9.6988e-02, -2.2169e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.7880e-01,  2.5476e-02, -1.0000e+09],\n",
            "          [-4.0367e-01, -2.7149e-02, -1.0000e+09],\n",
            "          [ 1.0404e-01,  6.9973e-03, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.4679e-01, -7.8853e-01, -1.0000e+09],\n",
            "          [ 5.6863e-02, -1.8169e-01, -1.0000e+09],\n",
            "          [ 2.5991e-01, -8.3045e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-4.1389e-01,  4.9254e-02, -3.1747e-01],\n",
            "          [-1.2654e+00,  1.5059e-01, -9.7064e-01],\n",
            "          [-8.2422e-01,  9.8084e-02, -6.3221e-01]],\n",
            "\n",
            "         [[ 1.2710e-01, -2.9051e-01,  1.2385e-02],\n",
            "          [-4.5728e-02,  1.0452e-01, -4.4561e-03],\n",
            "          [ 9.6988e-02, -2.2169e-01,  9.4512e-03]],\n",
            "\n",
            "         [[ 3.7880e-01,  2.5476e-02,  2.8173e-01],\n",
            "          [-4.0367e-01, -2.7149e-02, -3.0023e-01],\n",
            "          [ 1.0404e-01,  6.9973e-03,  7.7381e-02]],\n",
            "\n",
            "         [[ 2.4679e-01, -7.8853e-01, -1.6683e-01],\n",
            "          [ 5.6863e-02, -1.8169e-01, -3.8439e-02],\n",
            "          [ 2.5991e-01, -8.3045e-01, -1.7570e-01]]]], device='cuda:0',\n",
            "       grad_fn=<DivBackward0>)\n",
            "softmax_scores: tensor([[[[0.3862, 0.6138, 0.0000],\n",
            "          [0.1953, 0.8047, 0.0000],\n",
            "          [0.2845, 0.7155, 0.0000]],\n",
            "\n",
            "         [[0.6029, 0.3971, 0.0000],\n",
            "          [0.4625, 0.5375, 0.0000],\n",
            "          [0.5790, 0.4210, 0.0000]],\n",
            "\n",
            "         [[0.5874, 0.4126, 0.0000],\n",
            "          [0.4070, 0.5930, 0.0000],\n",
            "          [0.5242, 0.4758, 0.0000]],\n",
            "\n",
            "         [[0.7379, 0.2621, 0.0000],\n",
            "          [0.5594, 0.4406, 0.0000],\n",
            "          [0.7484, 0.2516, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.3862, 0.6138, 0.0000],\n",
            "          [0.1953, 0.8047, 0.0000],\n",
            "          [0.2845, 0.7155, 0.0000]],\n",
            "\n",
            "         [[0.6029, 0.3971, 0.0000],\n",
            "          [0.4625, 0.5375, 0.0000],\n",
            "          [0.5790, 0.4210, 0.0000]],\n",
            "\n",
            "         [[0.5874, 0.4126, 0.0000],\n",
            "          [0.4070, 0.5930, 0.0000],\n",
            "          [0.5242, 0.4758, 0.0000]],\n",
            "\n",
            "         [[0.7379, 0.2621, 0.0000],\n",
            "          [0.5594, 0.4406, 0.0000],\n",
            "          [0.7484, 0.2516, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.2710, 0.4306, 0.2984],\n",
            "          [0.1547, 0.6375, 0.2078],\n",
            "          [0.2116, 0.5321, 0.2563]],\n",
            "\n",
            "         [[0.3921, 0.2583, 0.3496],\n",
            "          [0.3121, 0.3627, 0.3252],\n",
            "          [0.3783, 0.2751, 0.3466]],\n",
            "\n",
            "         [[0.3832, 0.2691, 0.3477],\n",
            "          [0.2804, 0.4086, 0.3110],\n",
            "          [0.3471, 0.3150, 0.3379]],\n",
            "\n",
            "         [[0.4959, 0.1761, 0.3279],\n",
            "          [0.3708, 0.2921, 0.3371],\n",
            "          [0.5043, 0.1695, 0.3262]]]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "softmax_socres shape: torch.Size([3, 4, 3, 3])\n",
            "values: tensor([[[[-0.1733],\n",
            "          [ 0.1845],\n",
            "          [-0.2711]],\n",
            "\n",
            "         [[ 0.3856],\n",
            "          [-0.1036],\n",
            "          [ 0.1591]],\n",
            "\n",
            "         [[-0.0492],\n",
            "          [-1.5056],\n",
            "          [-0.5631]],\n",
            "\n",
            "         [[ 0.7909],\n",
            "          [-0.8326],\n",
            "          [ 0.1586]]],\n",
            "\n",
            "\n",
            "        [[[-0.1733],\n",
            "          [ 0.1845],\n",
            "          [-0.2711]],\n",
            "\n",
            "         [[ 0.3856],\n",
            "          [-0.1036],\n",
            "          [ 0.1591]],\n",
            "\n",
            "         [[-0.0492],\n",
            "          [-1.5056],\n",
            "          [-0.5631]],\n",
            "\n",
            "         [[ 0.7909],\n",
            "          [-0.8326],\n",
            "          [ 0.1586]]],\n",
            "\n",
            "\n",
            "        [[[-0.1733],\n",
            "          [ 0.1845],\n",
            "          [-0.2711]],\n",
            "\n",
            "         [[ 0.3856],\n",
            "          [-0.1036],\n",
            "          [ 0.1591]],\n",
            "\n",
            "         [[-0.0492],\n",
            "          [-1.5056],\n",
            "          [-0.5631]],\n",
            "\n",
            "         [[ 0.7909],\n",
            "          [-0.8326],\n",
            "          [ 0.1586]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "v_out shape torch.Size([3, 4, 3, 1])\n",
            "v_out: tensor([[[[ 0.0463],\n",
            "          [ 0.1146],\n",
            "          [ 0.0827]],\n",
            "\n",
            "         [[ 0.1914],\n",
            "          [ 0.1227],\n",
            "          [ 0.1797]],\n",
            "\n",
            "         [[-0.6501],\n",
            "          [-0.9129],\n",
            "          [-0.7421]],\n",
            "\n",
            "         [[ 0.3655],\n",
            "          [ 0.0755],\n",
            "          [ 0.3825]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0463],\n",
            "          [ 0.1146],\n",
            "          [ 0.0827]],\n",
            "\n",
            "         [[ 0.1914],\n",
            "          [ 0.1227],\n",
            "          [ 0.1797]],\n",
            "\n",
            "         [[-0.6501],\n",
            "          [-0.9129],\n",
            "          [-0.7421]],\n",
            "\n",
            "         [[ 0.3655],\n",
            "          [ 0.0755],\n",
            "          [ 0.3825]]],\n",
            "\n",
            "\n",
            "        [[[-0.0484],\n",
            "          [ 0.0345],\n",
            "          [-0.0080]],\n",
            "\n",
            "         [[ 0.1801],\n",
            "          [ 0.1345],\n",
            "          [ 0.1725]],\n",
            "\n",
            "         [[-0.6198],\n",
            "          [-0.8041],\n",
            "          [-0.6816]],\n",
            "\n",
            "         [[ 0.2976],\n",
            "          [ 0.1035],\n",
            "          [ 0.3095]]]], device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
            "v_out shape before: torch.Size([3, 4, 3, 1])\n",
            "v_out shape: torch.Size([3, 3, 4])\n",
            "v_out reshaped: tensor([[[ 0.0463,  0.1914, -0.6501,  0.3655],\n",
            "         [ 0.1146,  0.1227, -0.9129,  0.0755],\n",
            "         [ 0.0827,  0.1797, -0.7421,  0.3825]],\n",
            "\n",
            "        [[ 0.0463,  0.1914, -0.6501,  0.3655],\n",
            "         [ 0.1146,  0.1227, -0.9129,  0.0755],\n",
            "         [ 0.0827,  0.1797, -0.7421,  0.3825]],\n",
            "\n",
            "        [[-0.0484,  0.1801, -0.6198,  0.2976],\n",
            "         [ 0.0345,  0.1345, -0.8041,  0.1035],\n",
            "         [-0.0080,  0.1725, -0.6816,  0.3095]]], device='cuda:0',\n",
            "       grad_fn=<ReshapeAliasBackward0>)\n",
            "output shape torch.Size([3, 3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_headed_attention_encoder_decoder():\n",
        "    num_heads = 4\n",
        "    d_attn = 4\n",
        "    d_x = 4\n",
        "    d_z = 4\n",
        "    d_out = 4\n",
        "    d_mid = 4\n",
        "    length_x = 3\n",
        "    length_z = 3\n",
        "    batch_size = 4\n",
        "    padding_mask = torch.tensor([[1, 1, 0]], dtype=torch.int32).to(device)\n",
        "\n",
        "    multi_headed_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], 0.0, True).to(device)\n",
        "    x = torch.rand(batch_size, length_x, d_x).to(device)\n",
        "    z = torch.rand(batch_size, length_z, d_z).to(device)\n",
        "    output = multi_headed_attention(z, x, padding_mask)\n",
        "    print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, length_x, d_out)\n",
        "test_multi_headed_attention_encoder_decoder()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbYma85ffEHr",
        "outputId": "01691df4-fe34-461e-8daa-fed792bff696"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mask tensor([[[[1, 1, 0]]]], device='cuda:0', dtype=torch.int32)\n",
            "mask torch.Size([1, 1, 1, 3])\n",
            "queries: tensor([[[[-1.0411e-01],\n",
            "          [-2.0415e-01],\n",
            "          [-2.1368e-01]],\n",
            "\n",
            "         [[ 5.9094e-01],\n",
            "          [ 1.3446e-01],\n",
            "          [ 3.9921e-01]],\n",
            "\n",
            "         [[-7.2403e-02],\n",
            "          [-3.6016e-02],\n",
            "          [-2.3203e-01]],\n",
            "\n",
            "         [[ 9.5991e-01],\n",
            "          [ 5.6997e-01],\n",
            "          [ 7.0617e-01]]],\n",
            "\n",
            "\n",
            "        [[[-2.4329e-01],\n",
            "          [-1.9518e-01],\n",
            "          [-3.2224e-01]],\n",
            "\n",
            "         [[ 1.6908e-01],\n",
            "          [ 3.7678e-01],\n",
            "          [-9.6929e-02]],\n",
            "\n",
            "         [[-1.3857e-01],\n",
            "          [-2.0413e-01],\n",
            "          [-2.2908e-01]],\n",
            "\n",
            "         [[ 6.4452e-01],\n",
            "          [ 7.0216e-01],\n",
            "          [ 2.3602e-01]]],\n",
            "\n",
            "\n",
            "        [[[-2.8424e-01],\n",
            "          [-7.9439e-02],\n",
            "          [-5.5061e-01]],\n",
            "\n",
            "         [[-7.3381e-04],\n",
            "          [ 2.5058e-01],\n",
            "          [ 8.3200e-02]],\n",
            "\n",
            "         [[-8.2078e-02],\n",
            "          [ 1.1935e-01],\n",
            "          [-5.9572e-01]],\n",
            "\n",
            "         [[ 5.0838e-01],\n",
            "          [ 6.3067e-01],\n",
            "          [ 4.1011e-01]]],\n",
            "\n",
            "\n",
            "        [[[-4.6082e-01],\n",
            "          [-3.1239e-01],\n",
            "          [-3.6973e-01]],\n",
            "\n",
            "         [[-3.4119e-01],\n",
            "          [ 1.7469e-02],\n",
            "          [-8.8304e-02]],\n",
            "\n",
            "         [[-2.1045e-01],\n",
            "          [-2.0426e-01],\n",
            "          [-2.1878e-01]],\n",
            "\n",
            "         [[ 8.7209e-02],\n",
            "          [ 4.4796e-01],\n",
            "          [ 2.4669e-01]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "keys: tensor([[[[-0.0881],\n",
            "          [-0.2534],\n",
            "          [-0.0891]],\n",
            "\n",
            "         [[ 0.1898],\n",
            "          [ 0.4992],\n",
            "          [ 0.0648]],\n",
            "\n",
            "         [[-0.2223],\n",
            "          [ 0.0325],\n",
            "          [ 0.0394]],\n",
            "\n",
            "         [[-0.2684],\n",
            "          [-0.1598],\n",
            "          [ 0.0137]]],\n",
            "\n",
            "\n",
            "        [[[-0.1425],\n",
            "          [-0.4905],\n",
            "          [-0.1038]],\n",
            "\n",
            "         [[ 0.0720],\n",
            "          [ 0.3563],\n",
            "          [ 0.0187]],\n",
            "\n",
            "         [[-0.0123],\n",
            "          [ 0.1892],\n",
            "          [ 0.1024]],\n",
            "\n",
            "         [[-0.0531],\n",
            "          [ 0.0313],\n",
            "          [ 0.0491]]],\n",
            "\n",
            "\n",
            "        [[[-0.1663],\n",
            "          [-0.4228],\n",
            "          [-0.0223]],\n",
            "\n",
            "         [[ 0.2988],\n",
            "          [ 0.3028],\n",
            "          [-0.0400]],\n",
            "\n",
            "         [[-0.1352],\n",
            "          [ 0.0653],\n",
            "          [ 0.0378]],\n",
            "\n",
            "         [[-0.2422],\n",
            "          [-0.0461],\n",
            "          [ 0.0143]]],\n",
            "\n",
            "\n",
            "        [[[-0.2064],\n",
            "          [ 0.0502],\n",
            "          [-0.3310]],\n",
            "\n",
            "         [[ 0.2967],\n",
            "          [ 0.3649],\n",
            "          [ 0.2105]],\n",
            "\n",
            "         [[ 0.1187],\n",
            "          [-0.3282],\n",
            "          [ 0.4159]],\n",
            "\n",
            "         [[-0.0504],\n",
            "          [-0.4078],\n",
            "          [ 0.2281]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "values: tensor([[[[ 0.2884],\n",
            "          [ 0.3883],\n",
            "          [ 0.3333]],\n",
            "\n",
            "         [[ 0.3308],\n",
            "          [ 0.5760],\n",
            "          [ 0.1992]],\n",
            "\n",
            "         [[ 0.5081],\n",
            "          [ 0.6069],\n",
            "          [ 0.1414]],\n",
            "\n",
            "         [[-0.0852],\n",
            "          [-0.0851],\n",
            "          [-0.2525]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3293],\n",
            "          [ 0.1077],\n",
            "          [ 0.4903]],\n",
            "\n",
            "         [[ 0.1901],\n",
            "          [ 0.6429],\n",
            "          [ 0.0179]],\n",
            "\n",
            "         [[ 0.2864],\n",
            "          [ 0.4678],\n",
            "          [ 0.1755]],\n",
            "\n",
            "         [[-0.2465],\n",
            "          [-0.2850],\n",
            "          [-0.3125]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3421],\n",
            "          [ 0.0446],\n",
            "          [ 0.5226]],\n",
            "\n",
            "         [[ 0.3960],\n",
            "          [ 0.6372],\n",
            "          [-0.0653]],\n",
            "\n",
            "         [[ 0.5885],\n",
            "          [ 0.4674],\n",
            "          [ 0.1451]],\n",
            "\n",
            "         [[-0.0917],\n",
            "          [-0.2391],\n",
            "          [-0.2918]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5690],\n",
            "          [ 0.4085],\n",
            "          [ 0.5948]],\n",
            "\n",
            "         [[ 0.2244],\n",
            "          [ 0.4260],\n",
            "          [ 0.1287]],\n",
            "\n",
            "         [[ 0.4745],\n",
            "          [ 0.6144],\n",
            "          [ 0.2387]],\n",
            "\n",
            "         [[-0.2144],\n",
            "          [ 0.0858],\n",
            "          [-0.4159]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "keys_transposed: tensor([[[[-0.0881, -0.2534, -0.0891]],\n",
            "\n",
            "         [[ 0.1898,  0.4992,  0.0648]],\n",
            "\n",
            "         [[-0.2223,  0.0325,  0.0394]],\n",
            "\n",
            "         [[-0.2684, -0.1598,  0.0137]]],\n",
            "\n",
            "\n",
            "        [[[-0.1425, -0.4905, -0.1038]],\n",
            "\n",
            "         [[ 0.0720,  0.3563,  0.0187]],\n",
            "\n",
            "         [[-0.0123,  0.1892,  0.1024]],\n",
            "\n",
            "         [[-0.0531,  0.0313,  0.0491]]],\n",
            "\n",
            "\n",
            "        [[[-0.1663, -0.4228, -0.0223]],\n",
            "\n",
            "         [[ 0.2988,  0.3028, -0.0400]],\n",
            "\n",
            "         [[-0.1352,  0.0653,  0.0378]],\n",
            "\n",
            "         [[-0.2422, -0.0461,  0.0143]]],\n",
            "\n",
            "\n",
            "        [[[-0.2064,  0.0502, -0.3310]],\n",
            "\n",
            "         [[ 0.2967,  0.3649,  0.2105]],\n",
            "\n",
            "         [[ 0.1187, -0.3282,  0.4159]],\n",
            "\n",
            "         [[-0.0504, -0.4078,  0.2281]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "scores: tensor([[[[ 9.1767e-03,  2.6385e-02,  9.2747e-03],\n",
            "          [ 1.7994e-02,  5.1738e-02,  1.8187e-02],\n",
            "          [ 1.8835e-02,  5.4155e-02,  1.9036e-02]],\n",
            "\n",
            "         [[ 1.1217e-01,  2.9499e-01,  3.8319e-02],\n",
            "          [ 2.5522e-02,  6.7119e-02,  8.7189e-03],\n",
            "          [ 7.5775e-02,  1.9928e-01,  2.5886e-02]],\n",
            "\n",
            "         [[ 1.6097e-02, -2.3529e-03, -2.8501e-03],\n",
            "          [ 8.0070e-03, -1.1704e-03, -1.4177e-03],\n",
            "          [ 5.1585e-02, -7.5405e-03, -9.1339e-03]],\n",
            "\n",
            "         [[-2.5762e-01, -1.5340e-01,  1.3114e-02],\n",
            "          [-1.5297e-01, -9.1086e-02,  7.7867e-03],\n",
            "          [-1.8952e-01, -1.1285e-01,  9.6474e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 3.4664e-02,  1.1934e-01,  2.5258e-02],\n",
            "          [ 2.7809e-02,  9.5740e-02,  2.0263e-02],\n",
            "          [ 4.5912e-02,  1.5807e-01,  3.3455e-02]],\n",
            "\n",
            "         [[ 1.2168e-02,  6.0246e-02,  3.1623e-03],\n",
            "          [ 2.7115e-02,  1.3425e-01,  7.0468e-03],\n",
            "          [-6.9755e-03, -3.4537e-02, -1.8128e-03]],\n",
            "\n",
            "         [[ 1.7043e-03, -2.6212e-02, -1.4193e-02],\n",
            "          [ 2.5106e-03, -3.8612e-02, -2.0907e-02],\n",
            "          [ 2.8176e-03, -4.3334e-02, -2.3464e-02]],\n",
            "\n",
            "         [[-3.4222e-02,  2.0190e-02,  3.1639e-02],\n",
            "          [-3.7282e-02,  2.1995e-02,  3.4468e-02],\n",
            "          [-1.2532e-02,  7.3935e-03,  1.1586e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.7261e-02,  1.2018e-01,  6.3481e-03],\n",
            "          [ 1.3209e-02,  3.3588e-02,  1.7742e-03],\n",
            "          [ 9.1551e-02,  2.3281e-01,  1.2297e-02]],\n",
            "\n",
            "         [[-2.1928e-04, -2.2220e-04,  2.9380e-05],\n",
            "          [ 7.4878e-02,  7.5876e-02, -1.0033e-02],\n",
            "          [ 2.4862e-02,  2.5193e-02, -3.3312e-03]],\n",
            "\n",
            "         [[ 1.1095e-02, -5.3611e-03, -3.1026e-03],\n",
            "          [-1.6133e-02,  7.7960e-03,  4.5117e-03],\n",
            "          [ 8.0524e-02, -3.8911e-02, -2.2518e-02]],\n",
            "\n",
            "         [[-1.2315e-01, -2.3451e-02,  7.2858e-03],\n",
            "          [-1.5278e-01, -2.9092e-02,  9.0383e-03],\n",
            "          [-9.9349e-02, -1.8918e-02,  5.8775e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 9.5128e-02, -2.3148e-02,  1.5255e-01],\n",
            "          [ 6.4486e-02, -1.5692e-02,  1.0341e-01],\n",
            "          [ 7.6323e-02, -1.8572e-02,  1.2239e-01]],\n",
            "\n",
            "         [[-1.0123e-01, -1.2449e-01, -7.1830e-02],\n",
            "          [ 5.1831e-03,  6.3741e-03,  3.6778e-03],\n",
            "          [-2.6200e-02, -3.2220e-02, -1.8590e-02]],\n",
            "\n",
            "         [[-2.4978e-02,  6.9063e-02, -8.7523e-02],\n",
            "          [-2.4244e-02,  6.7033e-02, -8.4950e-02],\n",
            "          [-2.5968e-02,  7.1800e-02, -9.0991e-02]],\n",
            "\n",
            "         [[-4.3951e-03, -3.5561e-02,  1.9896e-02],\n",
            "          [-2.2576e-02, -1.8266e-01,  1.0220e-01],\n",
            "          [-1.2433e-02, -1.0059e-01,  5.6280e-02]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "scores: torch.Size([4, 4, 3, 3])\n",
            "masks: torch.Size([1, 1, 1, 3])\n",
            "masked scores: tensor([[[[ 9.1767e-03,  2.6385e-02, -1.0000e+09],\n",
            "          [ 1.7994e-02,  5.1738e-02, -1.0000e+09],\n",
            "          [ 1.8835e-02,  5.4155e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.1217e-01,  2.9499e-01, -1.0000e+09],\n",
            "          [ 2.5522e-02,  6.7119e-02, -1.0000e+09],\n",
            "          [ 7.5775e-02,  1.9928e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.6097e-02, -2.3529e-03, -1.0000e+09],\n",
            "          [ 8.0070e-03, -1.1704e-03, -1.0000e+09],\n",
            "          [ 5.1585e-02, -7.5405e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-2.5762e-01, -1.5340e-01, -1.0000e+09],\n",
            "          [-1.5297e-01, -9.1086e-02, -1.0000e+09],\n",
            "          [-1.8952e-01, -1.1285e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 3.4664e-02,  1.1934e-01, -1.0000e+09],\n",
            "          [ 2.7809e-02,  9.5740e-02, -1.0000e+09],\n",
            "          [ 4.5912e-02,  1.5807e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.2168e-02,  6.0246e-02, -1.0000e+09],\n",
            "          [ 2.7115e-02,  1.3425e-01, -1.0000e+09],\n",
            "          [-6.9755e-03, -3.4537e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.7043e-03, -2.6212e-02, -1.0000e+09],\n",
            "          [ 2.5106e-03, -3.8612e-02, -1.0000e+09],\n",
            "          [ 2.8176e-03, -4.3334e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-3.4222e-02,  2.0190e-02, -1.0000e+09],\n",
            "          [-3.7282e-02,  2.1995e-02, -1.0000e+09],\n",
            "          [-1.2532e-02,  7.3935e-03, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 4.7261e-02,  1.2018e-01, -1.0000e+09],\n",
            "          [ 1.3209e-02,  3.3588e-02, -1.0000e+09],\n",
            "          [ 9.1551e-02,  2.3281e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-2.1928e-04, -2.2220e-04, -1.0000e+09],\n",
            "          [ 7.4878e-02,  7.5876e-02, -1.0000e+09],\n",
            "          [ 2.4862e-02,  2.5193e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.1095e-02, -5.3611e-03, -1.0000e+09],\n",
            "          [-1.6133e-02,  7.7960e-03, -1.0000e+09],\n",
            "          [ 8.0524e-02, -3.8911e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-1.2315e-01, -2.3451e-02, -1.0000e+09],\n",
            "          [-1.5278e-01, -2.9092e-02, -1.0000e+09],\n",
            "          [-9.9349e-02, -1.8918e-02, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 9.5128e-02, -2.3148e-02, -1.0000e+09],\n",
            "          [ 6.4486e-02, -1.5692e-02, -1.0000e+09],\n",
            "          [ 7.6323e-02, -1.8572e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-1.0123e-01, -1.2449e-01, -1.0000e+09],\n",
            "          [ 5.1831e-03,  6.3741e-03, -1.0000e+09],\n",
            "          [-2.6200e-02, -3.2220e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-2.4978e-02,  6.9063e-02, -1.0000e+09],\n",
            "          [-2.4244e-02,  6.7033e-02, -1.0000e+09],\n",
            "          [-2.5968e-02,  7.1800e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-4.3951e-03, -3.5561e-02, -1.0000e+09],\n",
            "          [-2.2576e-02, -1.8266e-01, -1.0000e+09],\n",
            "          [-1.2433e-02, -1.0059e-01, -1.0000e+09]]]], device='cuda:0',\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "scaled_scores: tensor([[[[ 9.1767e-03,  2.6385e-02, -1.0000e+09],\n",
            "          [ 1.7994e-02,  5.1738e-02, -1.0000e+09],\n",
            "          [ 1.8835e-02,  5.4155e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.1217e-01,  2.9499e-01, -1.0000e+09],\n",
            "          [ 2.5522e-02,  6.7119e-02, -1.0000e+09],\n",
            "          [ 7.5775e-02,  1.9928e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.6097e-02, -2.3529e-03, -1.0000e+09],\n",
            "          [ 8.0070e-03, -1.1704e-03, -1.0000e+09],\n",
            "          [ 5.1585e-02, -7.5405e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-2.5762e-01, -1.5340e-01, -1.0000e+09],\n",
            "          [-1.5297e-01, -9.1086e-02, -1.0000e+09],\n",
            "          [-1.8952e-01, -1.1285e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 3.4664e-02,  1.1934e-01, -1.0000e+09],\n",
            "          [ 2.7809e-02,  9.5740e-02, -1.0000e+09],\n",
            "          [ 4.5912e-02,  1.5807e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.2168e-02,  6.0246e-02, -1.0000e+09],\n",
            "          [ 2.7115e-02,  1.3425e-01, -1.0000e+09],\n",
            "          [-6.9755e-03, -3.4537e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.7043e-03, -2.6212e-02, -1.0000e+09],\n",
            "          [ 2.5106e-03, -3.8612e-02, -1.0000e+09],\n",
            "          [ 2.8176e-03, -4.3334e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-3.4222e-02,  2.0190e-02, -1.0000e+09],\n",
            "          [-3.7282e-02,  2.1995e-02, -1.0000e+09],\n",
            "          [-1.2532e-02,  7.3935e-03, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 4.7261e-02,  1.2018e-01, -1.0000e+09],\n",
            "          [ 1.3209e-02,  3.3588e-02, -1.0000e+09],\n",
            "          [ 9.1551e-02,  2.3281e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-2.1928e-04, -2.2220e-04, -1.0000e+09],\n",
            "          [ 7.4878e-02,  7.5876e-02, -1.0000e+09],\n",
            "          [ 2.4862e-02,  2.5193e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.1095e-02, -5.3611e-03, -1.0000e+09],\n",
            "          [-1.6133e-02,  7.7960e-03, -1.0000e+09],\n",
            "          [ 8.0524e-02, -3.8911e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-1.2315e-01, -2.3451e-02, -1.0000e+09],\n",
            "          [-1.5278e-01, -2.9092e-02, -1.0000e+09],\n",
            "          [-9.9349e-02, -1.8918e-02, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 9.5128e-02, -2.3148e-02, -1.0000e+09],\n",
            "          [ 6.4486e-02, -1.5692e-02, -1.0000e+09],\n",
            "          [ 7.6323e-02, -1.8572e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-1.0123e-01, -1.2449e-01, -1.0000e+09],\n",
            "          [ 5.1831e-03,  6.3741e-03, -1.0000e+09],\n",
            "          [-2.6200e-02, -3.2220e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-2.4978e-02,  6.9063e-02, -1.0000e+09],\n",
            "          [-2.4244e-02,  6.7033e-02, -1.0000e+09],\n",
            "          [-2.5968e-02,  7.1800e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-4.3951e-03, -3.5561e-02, -1.0000e+09],\n",
            "          [-2.2576e-02, -1.8266e-01, -1.0000e+09],\n",
            "          [-1.2433e-02, -1.0059e-01, -1.0000e+09]]]], device='cuda:0',\n",
            "       grad_fn=<DivBackward0>)\n",
            "softmax_scores: tensor([[[[0.4957, 0.5043, 0.0000],\n",
            "          [0.4916, 0.5084, 0.0000],\n",
            "          [0.4912, 0.5088, 0.0000]],\n",
            "\n",
            "         [[0.4544, 0.5456, 0.0000],\n",
            "          [0.4896, 0.5104, 0.0000],\n",
            "          [0.4692, 0.5308, 0.0000]],\n",
            "\n",
            "         [[0.5046, 0.4954, 0.0000],\n",
            "          [0.5023, 0.4977, 0.0000],\n",
            "          [0.5148, 0.4852, 0.0000]],\n",
            "\n",
            "         [[0.4740, 0.5260, 0.0000],\n",
            "          [0.4845, 0.5155, 0.0000],\n",
            "          [0.4808, 0.5192, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.4788, 0.5212, 0.0000],\n",
            "          [0.4830, 0.5170, 0.0000],\n",
            "          [0.4720, 0.5280, 0.0000]],\n",
            "\n",
            "         [[0.4880, 0.5120, 0.0000],\n",
            "          [0.4732, 0.5268, 0.0000],\n",
            "          [0.5069, 0.4931, 0.0000]],\n",
            "\n",
            "         [[0.5070, 0.4930, 0.0000],\n",
            "          [0.5103, 0.4897, 0.0000],\n",
            "          [0.5115, 0.4885, 0.0000]],\n",
            "\n",
            "         [[0.4864, 0.5136, 0.0000],\n",
            "          [0.4852, 0.5148, 0.0000],\n",
            "          [0.4950, 0.5050, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.4818, 0.5182, 0.0000],\n",
            "          [0.4949, 0.5051, 0.0000],\n",
            "          [0.4647, 0.5353, 0.0000]],\n",
            "\n",
            "         [[0.5000, 0.5000, 0.0000],\n",
            "          [0.4998, 0.5002, 0.0000],\n",
            "          [0.4999, 0.5001, 0.0000]],\n",
            "\n",
            "         [[0.5041, 0.4959, 0.0000],\n",
            "          [0.4940, 0.5060, 0.0000],\n",
            "          [0.5298, 0.4702, 0.0000]],\n",
            "\n",
            "         [[0.4751, 0.5249, 0.0000],\n",
            "          [0.4691, 0.5309, 0.0000],\n",
            "          [0.4799, 0.5201, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.5295, 0.4705, 0.0000],\n",
            "          [0.5200, 0.4800, 0.0000],\n",
            "          [0.5237, 0.4763, 0.0000]],\n",
            "\n",
            "         [[0.5058, 0.4942, 0.0000],\n",
            "          [0.4997, 0.5003, 0.0000],\n",
            "          [0.5015, 0.4985, 0.0000]],\n",
            "\n",
            "         [[0.4765, 0.5235, 0.0000],\n",
            "          [0.4772, 0.5228, 0.0000],\n",
            "          [0.4756, 0.5244, 0.0000]],\n",
            "\n",
            "         [[0.5078, 0.4922, 0.0000],\n",
            "          [0.5399, 0.4601, 0.0000],\n",
            "          [0.5220, 0.4780, 0.0000]]]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "softmax_socres shape: torch.Size([4, 4, 3, 3])\n",
            "values: tensor([[[[ 0.2884],\n",
            "          [ 0.3883],\n",
            "          [ 0.3333]],\n",
            "\n",
            "         [[ 0.3308],\n",
            "          [ 0.5760],\n",
            "          [ 0.1992]],\n",
            "\n",
            "         [[ 0.5081],\n",
            "          [ 0.6069],\n",
            "          [ 0.1414]],\n",
            "\n",
            "         [[-0.0852],\n",
            "          [-0.0851],\n",
            "          [-0.2525]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3293],\n",
            "          [ 0.1077],\n",
            "          [ 0.4903]],\n",
            "\n",
            "         [[ 0.1901],\n",
            "          [ 0.6429],\n",
            "          [ 0.0179]],\n",
            "\n",
            "         [[ 0.2864],\n",
            "          [ 0.4678],\n",
            "          [ 0.1755]],\n",
            "\n",
            "         [[-0.2465],\n",
            "          [-0.2850],\n",
            "          [-0.3125]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3421],\n",
            "          [ 0.0446],\n",
            "          [ 0.5226]],\n",
            "\n",
            "         [[ 0.3960],\n",
            "          [ 0.6372],\n",
            "          [-0.0653]],\n",
            "\n",
            "         [[ 0.5885],\n",
            "          [ 0.4674],\n",
            "          [ 0.1451]],\n",
            "\n",
            "         [[-0.0917],\n",
            "          [-0.2391],\n",
            "          [-0.2918]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5690],\n",
            "          [ 0.4085],\n",
            "          [ 0.5948]],\n",
            "\n",
            "         [[ 0.2244],\n",
            "          [ 0.4260],\n",
            "          [ 0.1287]],\n",
            "\n",
            "         [[ 0.4745],\n",
            "          [ 0.6144],\n",
            "          [ 0.2387]],\n",
            "\n",
            "         [[-0.2144],\n",
            "          [ 0.0858],\n",
            "          [-0.4159]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "v_out shape torch.Size([4, 4, 3, 1])\n",
            "v_out: tensor([[[[ 0.3388],\n",
            "          [ 0.3392],\n",
            "          [ 0.3392]],\n",
            "\n",
            "         [[ 0.4646],\n",
            "          [ 0.4560],\n",
            "          [ 0.4610]],\n",
            "\n",
            "         [[ 0.5571],\n",
            "          [ 0.5573],\n",
            "          [ 0.5561]],\n",
            "\n",
            "         [[-0.0852],\n",
            "          [-0.0852],\n",
            "          [-0.0852]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2138],\n",
            "          [ 0.2147],\n",
            "          [ 0.2123]],\n",
            "\n",
            "         [[ 0.4219],\n",
            "          [ 0.4286],\n",
            "          [ 0.4134]],\n",
            "\n",
            "         [[ 0.3758],\n",
            "          [ 0.3752],\n",
            "          [ 0.3750]],\n",
            "\n",
            "         [[-0.2663],\n",
            "          [-0.2663],\n",
            "          [-0.2659]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1879],\n",
            "          [ 0.1918],\n",
            "          [ 0.1829]],\n",
            "\n",
            "         [[ 0.5166],\n",
            "          [ 0.5166],\n",
            "          [ 0.5166]],\n",
            "\n",
            "         [[ 0.5284],\n",
            "          [ 0.5272],\n",
            "          [ 0.5316]],\n",
            "\n",
            "         [[-0.1690],\n",
            "          [-0.1699],\n",
            "          [-0.1683]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4935],\n",
            "          [ 0.4920],\n",
            "          [ 0.4926]],\n",
            "\n",
            "         [[ 0.3240],\n",
            "          [ 0.3252],\n",
            "          [ 0.3249]],\n",
            "\n",
            "         [[ 0.5477],\n",
            "          [ 0.5477],\n",
            "          [ 0.5479]],\n",
            "\n",
            "         [[-0.0667],\n",
            "          [-0.0763],\n",
            "          [-0.0709]]]], device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
            "v_out shape before: torch.Size([4, 4, 3, 1])\n",
            "v_out shape: torch.Size([4, 3, 4])\n",
            "v_out reshaped: tensor([[[ 0.3388,  0.4646,  0.5571, -0.0852],\n",
            "         [ 0.3392,  0.4560,  0.5573, -0.0852],\n",
            "         [ 0.3392,  0.4610,  0.5561, -0.0852]],\n",
            "\n",
            "        [[ 0.2138,  0.4219,  0.3758, -0.2663],\n",
            "         [ 0.2147,  0.4286,  0.3752, -0.2663],\n",
            "         [ 0.2123,  0.4134,  0.3750, -0.2659]],\n",
            "\n",
            "        [[ 0.1879,  0.5166,  0.5284, -0.1690],\n",
            "         [ 0.1918,  0.5166,  0.5272, -0.1699],\n",
            "         [ 0.1829,  0.5166,  0.5316, -0.1683]],\n",
            "\n",
            "        [[ 0.4935,  0.3240,  0.5477, -0.0667],\n",
            "         [ 0.4920,  0.3252,  0.5477, -0.0763],\n",
            "         [ 0.4926,  0.3249,  0.5479, -0.0709]]], device='cuda:0',\n",
            "       grad_fn=<ReshapeAliasBackward0>)\n",
            "output shape torch.Size([4, 3, 4])\n",
            "output: tensor([[[ 2.1614e-01,  9.5655e-04,  5.1873e-01,  2.2889e-01],\n",
            "         [ 2.1527e-01, -2.5983e-03,  5.1560e-01,  2.3126e-01],\n",
            "         [ 2.1557e-01, -2.2977e-04,  5.1745e-01,  2.2985e-01]],\n",
            "\n",
            "        [[ 1.0195e-01,  2.9458e-02,  6.0318e-01,  9.9157e-02],\n",
            "         [ 1.0307e-01,  3.2262e-02,  6.0543e-01,  9.7611e-02],\n",
            "         [ 9.9860e-02,  2.6361e-02,  6.0035e-01,  1.0100e-01]],\n",
            "\n",
            "        [[ 1.4677e-01,  3.1956e-02,  5.9467e-01,  1.2987e-01],\n",
            "         [ 1.4822e-01,  3.2060e-02,  5.9444e-01,  1.3041e-01],\n",
            "         [ 1.4537e-01,  3.1386e-02,  5.9504e-01,  1.2909e-01]],\n",
            "\n",
            "        [[ 2.6569e-01, -5.9729e-02,  4.3724e-01,  3.1699e-01],\n",
            "         [ 2.6536e-01, -5.9837e-02,  4.4142e-01,  3.1168e-01],\n",
            "         [ 2.6552e-01, -5.9683e-02,  4.3924e-01,  3.1450e-01]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_headed_attention_decoder_self():\n",
        "    num_heads = 8\n",
        "    d_attn = 8\n",
        "    d_x = 8\n",
        "    d_out = 8\n",
        "    d_mid = 8\n",
        "    length_x = 3\n",
        "    batch_size = 4\n",
        "    padding_mask = torch.tensor([[1, 1, 0], [1, 1, 0], [1, 0, 0], [1, 1, 1]], dtype=torch.int32).to(device)\n",
        "\n",
        "    multi_headed_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_x, d_out, d_mid, MaskStrategy['UNMASKED'], 0.0, True).to(device)\n",
        "    multi_headed_attention.enable_subsequent_mask()\n",
        "    x = torch.rand(batch_size, length_x, d_x).to(device)\n",
        "    output = multi_headed_attention(x, x, padding_mask)\n",
        "    print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, length_x, d_out)\n",
        "test_multi_headed_attention_decoder_self()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vn9s5ak_Yr1B",
        "outputId": "4ec21b9f-426a-4712-f3f9-940ae91368fd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "padding mask: torch.Size([4, 1, 3])\n",
            "mask tril tensor([[ True, False, False],\n",
            "        [ True,  True, False],\n",
            "        [ True,  True,  True]], device='cuda:0')\n",
            "merged mask: tensor([[[1, 0, 0],\n",
            "         [1, 1, 0],\n",
            "         [1, 1, 0]],\n",
            "\n",
            "        [[1, 0, 0],\n",
            "         [1, 1, 0],\n",
            "         [1, 1, 0]],\n",
            "\n",
            "        [[1, 0, 0],\n",
            "         [1, 0, 0],\n",
            "         [1, 0, 0]],\n",
            "\n",
            "        [[1, 0, 0],\n",
            "         [1, 1, 0],\n",
            "         [1, 1, 1]]], device='cuda:0', dtype=torch.int32)\n",
            "mask tensor([[[[1, 0, 0],\n",
            "          [1, 1, 0],\n",
            "          [1, 1, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 0, 0],\n",
            "          [1, 1, 0],\n",
            "          [1, 1, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 0, 0],\n",
            "          [1, 0, 0],\n",
            "          [1, 0, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 0, 0],\n",
            "          [1, 1, 0],\n",
            "          [1, 1, 1]]]], device='cuda:0', dtype=torch.int32)\n",
            "mask torch.Size([4, 1, 3, 3])\n",
            "queries: tensor([[[[-0.2834],\n",
            "          [-0.0325],\n",
            "          [-0.1563]],\n",
            "\n",
            "         [[-0.3054],\n",
            "          [-0.5627],\n",
            "          [-0.3479]],\n",
            "\n",
            "         [[ 0.5136],\n",
            "          [ 0.6240],\n",
            "          [ 0.5573]],\n",
            "\n",
            "         [[-0.0092],\n",
            "          [-0.2937],\n",
            "          [-0.0189]],\n",
            "\n",
            "         [[ 0.5461],\n",
            "          [ 0.3906],\n",
            "          [ 0.4660]],\n",
            "\n",
            "         [[ 0.0081],\n",
            "          [-0.0855],\n",
            "          [ 0.1399]],\n",
            "\n",
            "         [[-0.4123],\n",
            "          [-0.1175],\n",
            "          [-0.5517]],\n",
            "\n",
            "         [[-0.6651],\n",
            "          [-0.1720],\n",
            "          [-0.6851]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1071],\n",
            "          [ 0.1356],\n",
            "          [-0.0403]],\n",
            "\n",
            "         [[-0.2869],\n",
            "          [-0.4760],\n",
            "          [-0.6664]],\n",
            "\n",
            "         [[ 0.1368],\n",
            "          [ 0.3841],\n",
            "          [ 0.4333]],\n",
            "\n",
            "         [[ 0.1180],\n",
            "          [ 0.0977],\n",
            "          [ 0.1675]],\n",
            "\n",
            "         [[ 0.4424],\n",
            "          [ 0.2198],\n",
            "          [ 0.2676]],\n",
            "\n",
            "         [[-0.2083],\n",
            "          [-0.1703],\n",
            "          [ 0.1334]],\n",
            "\n",
            "         [[-0.0217],\n",
            "          [-0.2691],\n",
            "          [-0.1641]],\n",
            "\n",
            "         [[-0.4819],\n",
            "          [-0.6200],\n",
            "          [-0.4004]]],\n",
            "\n",
            "\n",
            "        [[[-0.0029],\n",
            "          [-0.1966],\n",
            "          [-0.1875]],\n",
            "\n",
            "         [[-0.2935],\n",
            "          [-0.5342],\n",
            "          [-0.4865]],\n",
            "\n",
            "         [[ 0.2201],\n",
            "          [ 0.3827],\n",
            "          [ 0.3512]],\n",
            "\n",
            "         [[-0.0121],\n",
            "          [-0.0563],\n",
            "          [-0.1109]],\n",
            "\n",
            "         [[ 0.6744],\n",
            "          [ 0.1261],\n",
            "          [ 0.1479]],\n",
            "\n",
            "         [[-0.1913],\n",
            "          [-0.0886],\n",
            "          [-0.1135]],\n",
            "\n",
            "         [[-0.0335],\n",
            "          [ 0.0910],\n",
            "          [ 0.0193]],\n",
            "\n",
            "         [[-0.3907],\n",
            "          [-0.4938],\n",
            "          [-0.7103]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0929],\n",
            "          [-0.2171],\n",
            "          [-0.1207]],\n",
            "\n",
            "         [[-0.4165],\n",
            "          [-0.3660],\n",
            "          [-0.3891]],\n",
            "\n",
            "         [[ 0.5076],\n",
            "          [ 0.3277],\n",
            "          [ 0.4630]],\n",
            "\n",
            "         [[-0.1576],\n",
            "          [-0.0924],\n",
            "          [-0.1835]],\n",
            "\n",
            "         [[ 0.3749],\n",
            "          [ 0.1281],\n",
            "          [ 0.2739]],\n",
            "\n",
            "         [[-0.3032],\n",
            "          [-0.0857],\n",
            "          [-0.2124]],\n",
            "\n",
            "         [[-0.2320],\n",
            "          [ 0.2150],\n",
            "          [-0.0447]],\n",
            "\n",
            "         [[-0.4264],\n",
            "          [-0.2294],\n",
            "          [-0.3815]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "keys: tensor([[[[-0.0915],\n",
            "          [-0.1013],\n",
            "          [-0.4483]],\n",
            "\n",
            "         [[ 0.3286],\n",
            "          [ 0.4379],\n",
            "          [ 0.6392]],\n",
            "\n",
            "         [[ 0.1402],\n",
            "          [ 0.3893],\n",
            "          [ 0.3491]],\n",
            "\n",
            "         [[-0.0659],\n",
            "          [ 0.0301],\n",
            "          [-0.2924]],\n",
            "\n",
            "         [[-0.3220],\n",
            "          [-0.6576],\n",
            "          [-0.7043]],\n",
            "\n",
            "         [[-0.3685],\n",
            "          [-0.2857],\n",
            "          [-0.4017]],\n",
            "\n",
            "         [[-0.0768],\n",
            "          [-0.0060],\n",
            "          [-0.1383]],\n",
            "\n",
            "         [[ 0.5755],\n",
            "          [ 0.5911],\n",
            "          [ 0.3665]]],\n",
            "\n",
            "\n",
            "        [[[-0.7655],\n",
            "          [-0.5507],\n",
            "          [-0.3851]],\n",
            "\n",
            "         [[ 0.6013],\n",
            "          [ 0.3422],\n",
            "          [ 0.5535]],\n",
            "\n",
            "         [[ 0.7890],\n",
            "          [ 0.5393],\n",
            "          [ 0.5045]],\n",
            "\n",
            "         [[-0.2204],\n",
            "          [-0.6232],\n",
            "          [-0.2774]],\n",
            "\n",
            "         [[-0.6812],\n",
            "          [-0.4547],\n",
            "          [-0.7121]],\n",
            "\n",
            "         [[-0.2035],\n",
            "          [-0.4806],\n",
            "          [-0.3136]],\n",
            "\n",
            "         [[-0.0775],\n",
            "          [-0.2396],\n",
            "          [-0.0186]],\n",
            "\n",
            "         [[ 0.1126],\n",
            "          [ 0.2444],\n",
            "          [ 0.3360]]],\n",
            "\n",
            "\n",
            "        [[[-0.3785],\n",
            "          [-0.1095],\n",
            "          [-0.1117]],\n",
            "\n",
            "         [[ 0.4952],\n",
            "          [ 0.1649],\n",
            "          [ 0.0690]],\n",
            "\n",
            "         [[ 0.4671],\n",
            "          [ 0.3759],\n",
            "          [ 0.2433]],\n",
            "\n",
            "         [[ 0.0565],\n",
            "          [-0.0979],\n",
            "          [-0.2314]],\n",
            "\n",
            "         [[-0.5526],\n",
            "          [-0.4007],\n",
            "          [-0.3422]],\n",
            "\n",
            "         [[-0.1153],\n",
            "          [-0.5790],\n",
            "          [-0.7606]],\n",
            "\n",
            "         [[ 0.1737],\n",
            "          [-0.1705],\n",
            "          [-0.1440]],\n",
            "\n",
            "         [[ 0.2708],\n",
            "          [ 0.8315],\n",
            "          [ 0.9084]]],\n",
            "\n",
            "\n",
            "        [[[-0.2036],\n",
            "          [-0.3369],\n",
            "          [-0.0148]],\n",
            "\n",
            "         [[ 0.2010],\n",
            "          [ 0.4838],\n",
            "          [ 0.1547]],\n",
            "\n",
            "         [[ 0.3304],\n",
            "          [ 0.6733],\n",
            "          [ 0.2271]],\n",
            "\n",
            "         [[-0.3592],\n",
            "          [ 0.1005],\n",
            "          [-0.1730]],\n",
            "\n",
            "         [[-0.2987],\n",
            "          [-0.6718],\n",
            "          [-0.2876]],\n",
            "\n",
            "         [[-0.3892],\n",
            "          [-0.3327],\n",
            "          [-0.4544]],\n",
            "\n",
            "         [[-0.1367],\n",
            "          [-0.2562],\n",
            "          [-0.1296]],\n",
            "\n",
            "         [[ 0.4080],\n",
            "          [ 0.6506],\n",
            "          [ 0.6558]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "values: tensor([[[[ 0.0806],\n",
            "          [-0.0513],\n",
            "          [ 0.1985]],\n",
            "\n",
            "         [[-0.2967],\n",
            "          [ 0.0786],\n",
            "          [-0.2241]],\n",
            "\n",
            "         [[-0.4151],\n",
            "          [-0.0953],\n",
            "          [-0.1230]],\n",
            "\n",
            "         [[-0.5342],\n",
            "          [-0.0968],\n",
            "          [-0.4016]],\n",
            "\n",
            "         [[-0.5054],\n",
            "          [-0.5340],\n",
            "          [-0.4670]],\n",
            "\n",
            "         [[ 0.1392],\n",
            "          [ 0.2579],\n",
            "          [-0.0288]],\n",
            "\n",
            "         [[ 0.6056],\n",
            "          [ 0.3262],\n",
            "          [ 0.6784]],\n",
            "\n",
            "         [[-0.5180],\n",
            "          [-0.3360],\n",
            "          [-0.2886]]],\n",
            "\n",
            "\n",
            "        [[[-0.1952],\n",
            "          [-0.1164],\n",
            "          [ 0.1127]],\n",
            "\n",
            "         [[-0.3607],\n",
            "          [-0.3278],\n",
            "          [-0.1549]],\n",
            "\n",
            "         [[ 0.1705],\n",
            "          [ 0.0294],\n",
            "          [ 0.0639]],\n",
            "\n",
            "         [[-0.2869],\n",
            "          [-0.3732],\n",
            "          [-0.2865]],\n",
            "\n",
            "         [[-0.6248],\n",
            "          [-0.5526],\n",
            "          [-0.6166]],\n",
            "\n",
            "         [[ 0.0863],\n",
            "          [-0.2570],\n",
            "          [ 0.1010]],\n",
            "\n",
            "         [[ 0.6042],\n",
            "          [ 0.7129],\n",
            "          [ 0.5794]],\n",
            "\n",
            "         [[-0.2629],\n",
            "          [-0.3482],\n",
            "          [-0.2357]]],\n",
            "\n",
            "\n",
            "        [[[-0.1422],\n",
            "          [ 0.0268],\n",
            "          [ 0.1016]],\n",
            "\n",
            "         [[-0.3559],\n",
            "          [-0.1993],\n",
            "          [-0.3849]],\n",
            "\n",
            "         [[-0.0819],\n",
            "          [-0.2657],\n",
            "          [-0.3830]],\n",
            "\n",
            "         [[-0.2351],\n",
            "          [-0.4204],\n",
            "          [-0.4658]],\n",
            "\n",
            "         [[-0.5112],\n",
            "          [-0.7527],\n",
            "          [-0.6952]],\n",
            "\n",
            "         [[ 0.3651],\n",
            "          [-0.0946],\n",
            "          [-0.3065]],\n",
            "\n",
            "         [[ 0.4502],\n",
            "          [ 0.5334],\n",
            "          [ 0.6304]],\n",
            "\n",
            "         [[-0.3166],\n",
            "          [-0.4002],\n",
            "          [-0.3647]]],\n",
            "\n",
            "\n",
            "        [[[-0.2170],\n",
            "          [-0.0333],\n",
            "          [-0.0611]],\n",
            "\n",
            "         [[-0.1932],\n",
            "          [ 0.0512],\n",
            "          [-0.1327]],\n",
            "\n",
            "         [[-0.1662],\n",
            "          [-0.0133],\n",
            "          [-0.2968]],\n",
            "\n",
            "         [[-0.2684],\n",
            "          [-0.2930],\n",
            "          [-0.3111]],\n",
            "\n",
            "         [[-0.4570],\n",
            "          [-0.7990],\n",
            "          [-0.5404]],\n",
            "\n",
            "         [[-0.0282],\n",
            "          [ 0.0987],\n",
            "          [-0.0238]],\n",
            "\n",
            "         [[ 0.4922],\n",
            "          [ 0.3625],\n",
            "          [ 0.3889]],\n",
            "\n",
            "         [[-0.4469],\n",
            "          [-0.2650],\n",
            "          [-0.3673]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "keys_transposed: tensor([[[[-0.0915, -0.1013, -0.4483]],\n",
            "\n",
            "         [[ 0.3286,  0.4379,  0.6392]],\n",
            "\n",
            "         [[ 0.1402,  0.3893,  0.3491]],\n",
            "\n",
            "         [[-0.0659,  0.0301, -0.2924]],\n",
            "\n",
            "         [[-0.3220, -0.6576, -0.7043]],\n",
            "\n",
            "         [[-0.3685, -0.2857, -0.4017]],\n",
            "\n",
            "         [[-0.0768, -0.0060, -0.1383]],\n",
            "\n",
            "         [[ 0.5755,  0.5911,  0.3665]]],\n",
            "\n",
            "\n",
            "        [[[-0.7655, -0.5507, -0.3851]],\n",
            "\n",
            "         [[ 0.6013,  0.3422,  0.5535]],\n",
            "\n",
            "         [[ 0.7890,  0.5393,  0.5045]],\n",
            "\n",
            "         [[-0.2204, -0.6232, -0.2774]],\n",
            "\n",
            "         [[-0.6812, -0.4547, -0.7121]],\n",
            "\n",
            "         [[-0.2035, -0.4806, -0.3136]],\n",
            "\n",
            "         [[-0.0775, -0.2396, -0.0186]],\n",
            "\n",
            "         [[ 0.1126,  0.2444,  0.3360]]],\n",
            "\n",
            "\n",
            "        [[[-0.3785, -0.1095, -0.1117]],\n",
            "\n",
            "         [[ 0.4952,  0.1649,  0.0690]],\n",
            "\n",
            "         [[ 0.4671,  0.3759,  0.2433]],\n",
            "\n",
            "         [[ 0.0565, -0.0979, -0.2314]],\n",
            "\n",
            "         [[-0.5526, -0.4007, -0.3422]],\n",
            "\n",
            "         [[-0.1153, -0.5790, -0.7606]],\n",
            "\n",
            "         [[ 0.1737, -0.1705, -0.1440]],\n",
            "\n",
            "         [[ 0.2708,  0.8315,  0.9084]]],\n",
            "\n",
            "\n",
            "        [[[-0.2036, -0.3369, -0.0148]],\n",
            "\n",
            "         [[ 0.2010,  0.4838,  0.1547]],\n",
            "\n",
            "         [[ 0.3304,  0.6733,  0.2271]],\n",
            "\n",
            "         [[-0.3592,  0.1005, -0.1730]],\n",
            "\n",
            "         [[-0.2987, -0.6718, -0.2876]],\n",
            "\n",
            "         [[-0.3892, -0.3327, -0.4544]],\n",
            "\n",
            "         [[-0.1367, -0.2562, -0.1296]],\n",
            "\n",
            "         [[ 0.4080,  0.6506,  0.6558]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "scores: tensor([[[[ 2.5922e-02,  2.8695e-02,  1.2704e-01],\n",
            "          [ 2.9714e-03,  3.2893e-03,  1.4563e-02],\n",
            "          [ 1.4299e-02,  1.5829e-02,  7.0081e-02]],\n",
            "\n",
            "         [[-1.0033e-01, -1.3373e-01, -1.9520e-01],\n",
            "          [-1.8489e-01, -2.4644e-01, -3.5972e-01],\n",
            "          [-1.1430e-01, -1.5235e-01, -2.2239e-01]],\n",
            "\n",
            "         [[ 7.2019e-02,  1.9994e-01,  1.7931e-01],\n",
            "          [ 8.7505e-02,  2.4293e-01,  2.1787e-01],\n",
            "          [ 7.8149e-02,  2.1696e-01,  1.9457e-01]],\n",
            "\n",
            "         [[ 6.0773e-04, -2.7709e-04,  2.6944e-03],\n",
            "          [ 1.9366e-02, -8.8298e-03,  8.5862e-02],\n",
            "          [ 1.2493e-03, -5.6960e-04,  5.5389e-03]],\n",
            "\n",
            "         [[-1.7583e-01, -3.5911e-01, -3.8457e-01],\n",
            "          [-1.2575e-01, -2.5684e-01, -2.7505e-01],\n",
            "          [-1.5004e-01, -3.0645e-01, -3.2818e-01]],\n",
            "\n",
            "         [[-2.9673e-03, -2.3012e-03, -3.2354e-03],\n",
            "          [ 3.1512e-02,  2.4439e-02,  3.4359e-02],\n",
            "          [-5.1559e-02, -3.9985e-02, -5.6216e-02]],\n",
            "\n",
            "         [[ 3.1652e-02,  2.4642e-03,  5.7032e-02],\n",
            "          [ 9.0235e-03,  7.0251e-04,  1.6259e-02],\n",
            "          [ 4.2353e-02,  3.2973e-03,  7.6315e-02]],\n",
            "\n",
            "         [[-3.8278e-01, -3.9309e-01, -2.4373e-01],\n",
            "          [-9.8966e-02, -1.0163e-01, -6.3015e-02],\n",
            "          [-3.9432e-01, -4.0494e-01, -2.5108e-01]]],\n",
            "\n",
            "\n",
            "        [[[-8.1984e-02, -5.8978e-02, -4.1244e-02],\n",
            "          [-1.0377e-01, -7.4650e-02, -5.2204e-02],\n",
            "          [ 3.0885e-02,  2.2218e-02,  1.5537e-02]],\n",
            "\n",
            "         [[-1.7255e-01, -9.8183e-02, -1.5882e-01],\n",
            "          [-2.8621e-01, -1.6286e-01, -2.6343e-01],\n",
            "          [-4.0073e-01, -2.2802e-01, -3.6884e-01]],\n",
            "\n",
            "         [[ 1.0795e-01,  7.3776e-02,  6.9022e-02],\n",
            "          [ 3.0310e-01,  2.0715e-01,  1.9380e-01],\n",
            "          [ 3.4192e-01,  2.3368e-01,  2.1862e-01]],\n",
            "\n",
            "         [[-2.6014e-02, -7.3546e-02, -3.2739e-02],\n",
            "          [-2.1527e-02, -6.0861e-02, -2.7092e-02],\n",
            "          [-3.6920e-02, -1.0438e-01, -4.6465e-02]],\n",
            "\n",
            "         [[-3.0135e-01, -2.0116e-01, -3.1504e-01],\n",
            "          [-1.4974e-01, -9.9951e-02, -1.5654e-01],\n",
            "          [-1.8226e-01, -1.2166e-01, -1.9054e-01]],\n",
            "\n",
            "         [[ 4.2378e-02,  1.0009e-01,  6.5309e-02],\n",
            "          [ 3.4650e-02,  8.1834e-02,  5.3399e-02],\n",
            "          [-2.7145e-02, -6.4111e-02, -4.1834e-02]],\n",
            "\n",
            "         [[ 1.6807e-03,  5.1985e-03,  4.0248e-04],\n",
            "          [ 2.0846e-02,  6.4476e-02,  4.9919e-03],\n",
            "          [ 1.2711e-02,  3.9315e-02,  3.0439e-03]],\n",
            "\n",
            "         [[-5.4266e-02, -1.1777e-01, -1.6191e-01],\n",
            "          [-6.9824e-02, -1.5154e-01, -2.0833e-01],\n",
            "          [-4.5090e-02, -9.7859e-02, -1.3453e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0789e-03,  3.1203e-04,  3.1838e-04],\n",
            "          [ 7.4412e-02,  2.1521e-02,  2.1959e-02],\n",
            "          [ 7.0996e-02,  2.0534e-02,  2.0951e-02]],\n",
            "\n",
            "         [[-1.4533e-01, -4.8396e-02, -2.0247e-02],\n",
            "          [-2.6458e-01, -8.8104e-02, -3.6859e-02],\n",
            "          [-2.4094e-01, -8.0232e-02, -3.3565e-02]],\n",
            "\n",
            "         [[ 1.0283e-01,  8.2747e-02,  5.3572e-02],\n",
            "          [ 1.7877e-01,  1.4386e-01,  9.3134e-02],\n",
            "          [ 1.6405e-01,  1.3201e-01,  8.5468e-02]],\n",
            "\n",
            "         [[-6.8602e-04,  1.1880e-03,  2.8078e-03],\n",
            "          [-3.1825e-03,  5.5113e-03,  1.3026e-02],\n",
            "          [-6.2692e-03,  1.0857e-02,  2.5659e-02]],\n",
            "\n",
            "         [[-3.7268e-01, -2.7025e-01, -2.3082e-01],\n",
            "          [-6.9671e-02, -5.0523e-02, -4.3152e-02],\n",
            "          [-8.1741e-02, -5.9275e-02, -5.0627e-02]],\n",
            "\n",
            "         [[ 2.2061e-02,  1.1076e-01,  1.4550e-01],\n",
            "          [ 1.0220e-02,  5.1307e-02,  6.7401e-02],\n",
            "          [ 1.3083e-02,  6.5686e-02,  8.6290e-02]],\n",
            "\n",
            "         [[-5.8115e-03,  5.7042e-03,  4.8176e-03],\n",
            "          [ 1.5810e-02, -1.5518e-02, -1.3106e-02],\n",
            "          [ 3.3595e-03, -3.2975e-03, -2.7850e-03]],\n",
            "\n",
            "         [[-1.0580e-01, -3.2492e-01, -3.5495e-01],\n",
            "          [-1.3370e-01, -4.1062e-01, -4.4857e-01],\n",
            "          [-1.9231e-01, -5.9063e-01, -6.4522e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.8920e-02, -3.1316e-02, -1.3729e-03],\n",
            "          [ 4.4202e-02,  7.3161e-02,  3.2075e-03],\n",
            "          [ 2.4571e-02,  4.0669e-02,  1.7830e-03]],\n",
            "\n",
            "         [[-8.3702e-02, -2.0151e-01, -6.4430e-02],\n",
            "          [-7.3546e-02, -1.7706e-01, -5.6613e-02],\n",
            "          [-7.8202e-02, -1.8827e-01, -6.0197e-02]],\n",
            "\n",
            "         [[ 1.6771e-01,  3.4177e-01,  1.1526e-01],\n",
            "          [ 1.0826e-01,  2.2063e-01,  7.4404e-02],\n",
            "          [ 1.5299e-01,  3.1179e-01,  1.0515e-01]],\n",
            "\n",
            "         [[ 5.6617e-02, -1.5845e-02,  2.7268e-02],\n",
            "          [ 3.3207e-02, -9.2936e-03,  1.5993e-02],\n",
            "          [ 6.5908e-02, -1.8446e-02,  3.1743e-02]],\n",
            "\n",
            "         [[-1.1197e-01, -2.5186e-01, -1.0783e-01],\n",
            "          [-3.8251e-02, -8.6044e-02, -3.6839e-02],\n",
            "          [-8.1811e-02, -1.8403e-01, -7.8791e-02]],\n",
            "\n",
            "         [[ 1.1798e-01,  1.0088e-01,  1.3776e-01],\n",
            "          [ 3.3335e-02,  2.8502e-02,  3.8925e-02],\n",
            "          [ 8.2650e-02,  7.0667e-02,  9.6509e-02]],\n",
            "\n",
            "         [[ 3.1709e-02,  5.9444e-02,  3.0063e-02],\n",
            "          [-2.9387e-02, -5.5092e-02, -2.7863e-02],\n",
            "          [ 6.1028e-03,  1.1441e-02,  5.7861e-03]],\n",
            "\n",
            "         [[-1.7398e-01, -2.7743e-01, -2.7965e-01],\n",
            "          [-9.3614e-02, -1.4928e-01, -1.5047e-01],\n",
            "          [-1.5565e-01, -2.4820e-01, -2.5018e-01]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "scores: torch.Size([4, 8, 3, 3])\n",
            "masks: torch.Size([4, 1, 3, 3])\n",
            "masked scores: tensor([[[[ 2.5922e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.9714e-03,  3.2893e-03, -1.0000e+09],\n",
            "          [ 1.4299e-02,  1.5829e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-1.0033e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.8489e-01, -2.4644e-01, -1.0000e+09],\n",
            "          [-1.1430e-01, -1.5235e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 7.2019e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 8.7505e-02,  2.4293e-01, -1.0000e+09],\n",
            "          [ 7.8149e-02,  2.1696e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 6.0773e-04, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.9366e-02, -8.8298e-03, -1.0000e+09],\n",
            "          [ 1.2493e-03, -5.6960e-04, -1.0000e+09]],\n",
            "\n",
            "         [[-1.7583e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.2575e-01, -2.5684e-01, -1.0000e+09],\n",
            "          [-1.5004e-01, -3.0645e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-2.9673e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.1512e-02,  2.4439e-02, -1.0000e+09],\n",
            "          [-5.1559e-02, -3.9985e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.1652e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 9.0235e-03,  7.0251e-04, -1.0000e+09],\n",
            "          [ 4.2353e-02,  3.2973e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-3.8278e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-9.8966e-02, -1.0163e-01, -1.0000e+09],\n",
            "          [-3.9432e-01, -4.0494e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-8.1984e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.0377e-01, -7.4650e-02, -1.0000e+09],\n",
            "          [ 3.0885e-02,  2.2218e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-1.7255e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-2.8621e-01, -1.6286e-01, -1.0000e+09],\n",
            "          [-4.0073e-01, -2.2802e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.0795e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.0310e-01,  2.0715e-01, -1.0000e+09],\n",
            "          [ 3.4192e-01,  2.3368e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-2.6014e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-2.1527e-02, -6.0861e-02, -1.0000e+09],\n",
            "          [-3.6920e-02, -1.0438e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-3.0135e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.4974e-01, -9.9951e-02, -1.0000e+09],\n",
            "          [-1.8226e-01, -1.2166e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 4.2378e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.4650e-02,  8.1834e-02, -1.0000e+09],\n",
            "          [-2.7145e-02, -6.4111e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.6807e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.0846e-02,  6.4476e-02, -1.0000e+09],\n",
            "          [ 1.2711e-02,  3.9315e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-5.4266e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-6.9824e-02, -1.5154e-01, -1.0000e+09],\n",
            "          [-4.5090e-02, -9.7859e-02, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0789e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 7.4412e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 7.0996e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[-1.4533e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-2.6458e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-2.4094e-01, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.0283e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.7877e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.6405e-01, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[-6.8602e-04, -1.0000e+09, -1.0000e+09],\n",
            "          [-3.1825e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [-6.2692e-03, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[-3.7268e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-6.9671e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-8.1741e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.2061e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.0220e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.3083e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[-5.8115e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.5810e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.3595e-03, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[-1.0580e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.3370e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.9231e-01, -1.0000e+09, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.8920e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.4202e-02,  7.3161e-02, -1.0000e+09],\n",
            "          [ 2.4571e-02,  4.0669e-02,  1.7830e-03]],\n",
            "\n",
            "         [[-8.3702e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-7.3546e-02, -1.7706e-01, -1.0000e+09],\n",
            "          [-7.8202e-02, -1.8827e-01, -6.0197e-02]],\n",
            "\n",
            "         [[ 1.6771e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.0826e-01,  2.2063e-01, -1.0000e+09],\n",
            "          [ 1.5299e-01,  3.1179e-01,  1.0515e-01]],\n",
            "\n",
            "         [[ 5.6617e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.3207e-02, -9.2936e-03, -1.0000e+09],\n",
            "          [ 6.5908e-02, -1.8446e-02,  3.1743e-02]],\n",
            "\n",
            "         [[-1.1197e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-3.8251e-02, -8.6044e-02, -1.0000e+09],\n",
            "          [-8.1811e-02, -1.8403e-01, -7.8791e-02]],\n",
            "\n",
            "         [[ 1.1798e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.3335e-02,  2.8502e-02, -1.0000e+09],\n",
            "          [ 8.2650e-02,  7.0667e-02,  9.6509e-02]],\n",
            "\n",
            "         [[ 3.1709e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-2.9387e-02, -5.5092e-02, -1.0000e+09],\n",
            "          [ 6.1028e-03,  1.1441e-02,  5.7861e-03]],\n",
            "\n",
            "         [[-1.7398e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-9.3614e-02, -1.4928e-01, -1.0000e+09],\n",
            "          [-1.5565e-01, -2.4820e-01, -2.5018e-01]]]], device='cuda:0',\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "scaled_scores: tensor([[[[ 2.5922e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.9714e-03,  3.2893e-03, -1.0000e+09],\n",
            "          [ 1.4299e-02,  1.5829e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-1.0033e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.8489e-01, -2.4644e-01, -1.0000e+09],\n",
            "          [-1.1430e-01, -1.5235e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 7.2019e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 8.7505e-02,  2.4293e-01, -1.0000e+09],\n",
            "          [ 7.8149e-02,  2.1696e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 6.0773e-04, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.9366e-02, -8.8298e-03, -1.0000e+09],\n",
            "          [ 1.2493e-03, -5.6960e-04, -1.0000e+09]],\n",
            "\n",
            "         [[-1.7583e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.2575e-01, -2.5684e-01, -1.0000e+09],\n",
            "          [-1.5004e-01, -3.0645e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-2.9673e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.1512e-02,  2.4439e-02, -1.0000e+09],\n",
            "          [-5.1559e-02, -3.9985e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.1652e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 9.0235e-03,  7.0251e-04, -1.0000e+09],\n",
            "          [ 4.2353e-02,  3.2973e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-3.8278e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-9.8966e-02, -1.0163e-01, -1.0000e+09],\n",
            "          [-3.9432e-01, -4.0494e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-8.1984e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.0377e-01, -7.4650e-02, -1.0000e+09],\n",
            "          [ 3.0885e-02,  2.2218e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-1.7255e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-2.8621e-01, -1.6286e-01, -1.0000e+09],\n",
            "          [-4.0073e-01, -2.2802e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.0795e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.0310e-01,  2.0715e-01, -1.0000e+09],\n",
            "          [ 3.4192e-01,  2.3368e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-2.6014e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-2.1527e-02, -6.0861e-02, -1.0000e+09],\n",
            "          [-3.6920e-02, -1.0438e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-3.0135e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.4974e-01, -9.9951e-02, -1.0000e+09],\n",
            "          [-1.8226e-01, -1.2166e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 4.2378e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.4650e-02,  8.1834e-02, -1.0000e+09],\n",
            "          [-2.7145e-02, -6.4111e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.6807e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.0846e-02,  6.4476e-02, -1.0000e+09],\n",
            "          [ 1.2711e-02,  3.9315e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-5.4266e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-6.9824e-02, -1.5154e-01, -1.0000e+09],\n",
            "          [-4.5090e-02, -9.7859e-02, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0789e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 7.4412e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 7.0996e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[-1.4533e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-2.6458e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-2.4094e-01, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.0283e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.7877e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.6405e-01, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[-6.8602e-04, -1.0000e+09, -1.0000e+09],\n",
            "          [-3.1825e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [-6.2692e-03, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[-3.7268e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-6.9671e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-8.1741e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.2061e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.0220e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.3083e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[-5.8115e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.5810e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.3595e-03, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[-1.0580e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.3370e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.9231e-01, -1.0000e+09, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.8920e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.4202e-02,  7.3161e-02, -1.0000e+09],\n",
            "          [ 2.4571e-02,  4.0669e-02,  1.7830e-03]],\n",
            "\n",
            "         [[-8.3702e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-7.3546e-02, -1.7706e-01, -1.0000e+09],\n",
            "          [-7.8202e-02, -1.8827e-01, -6.0197e-02]],\n",
            "\n",
            "         [[ 1.6771e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.0826e-01,  2.2063e-01, -1.0000e+09],\n",
            "          [ 1.5299e-01,  3.1179e-01,  1.0515e-01]],\n",
            "\n",
            "         [[ 5.6617e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.3207e-02, -9.2936e-03, -1.0000e+09],\n",
            "          [ 6.5908e-02, -1.8446e-02,  3.1743e-02]],\n",
            "\n",
            "         [[-1.1197e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-3.8251e-02, -8.6044e-02, -1.0000e+09],\n",
            "          [-8.1811e-02, -1.8403e-01, -7.8791e-02]],\n",
            "\n",
            "         [[ 1.1798e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.3335e-02,  2.8502e-02, -1.0000e+09],\n",
            "          [ 8.2650e-02,  7.0667e-02,  9.6509e-02]],\n",
            "\n",
            "         [[ 3.1709e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-2.9387e-02, -5.5092e-02, -1.0000e+09],\n",
            "          [ 6.1028e-03,  1.1441e-02,  5.7861e-03]],\n",
            "\n",
            "         [[-1.7398e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-9.3614e-02, -1.4928e-01, -1.0000e+09],\n",
            "          [-1.5565e-01, -2.4820e-01, -2.5018e-01]]]], device='cuda:0',\n",
            "       grad_fn=<DivBackward0>)\n",
            "softmax_scores: tensor([[[[1.0000, 0.0000, 0.0000],\n",
            "          [0.4999, 0.5001, 0.0000],\n",
            "          [0.4996, 0.5004, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5154, 0.4846, 0.0000],\n",
            "          [0.5095, 0.4905, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4612, 0.5388, 0.0000],\n",
            "          [0.4654, 0.5346, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5070, 0.4930, 0.0000],\n",
            "          [0.5005, 0.4995, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5327, 0.4673, 0.0000],\n",
            "          [0.5390, 0.4610, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5018, 0.4982, 0.0000],\n",
            "          [0.4971, 0.5029, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5021, 0.4979, 0.0000],\n",
            "          [0.5098, 0.4902, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5007, 0.4993, 0.0000],\n",
            "          [0.5027, 0.4973, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000],\n",
            "          [0.4927, 0.5073, 0.0000],\n",
            "          [0.5022, 0.4978, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4692, 0.5308, 0.0000],\n",
            "          [0.4569, 0.5431, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5240, 0.4760, 0.0000],\n",
            "          [0.5270, 0.4730, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5098, 0.4902, 0.0000],\n",
            "          [0.5169, 0.4831, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4876, 0.5124, 0.0000],\n",
            "          [0.4849, 0.5151, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4882, 0.5118, 0.0000],\n",
            "          [0.5092, 0.4908, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4891, 0.5109, 0.0000],\n",
            "          [0.4933, 0.5067, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5204, 0.4796, 0.0000],\n",
            "          [0.5132, 0.4868, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000],\n",
            "          [0.4928, 0.5072, 0.0000],\n",
            "          [0.3340, 0.3395, 0.3265]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5259, 0.4741, 0.0000],\n",
            "          [0.3432, 0.3074, 0.3494]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4719, 0.5281, 0.0000],\n",
            "          [0.3200, 0.3750, 0.3050]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5106, 0.4894, 0.0000],\n",
            "          [0.3466, 0.3185, 0.3349]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5119, 0.4881, 0.0000],\n",
            "          [0.3441, 0.3107, 0.3452]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5012, 0.4988, 0.0000],\n",
            "          [0.3331, 0.3291, 0.3378]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5064, 0.4936, 0.0000],\n",
            "          [0.3328, 0.3346, 0.3327]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5139, 0.4861, 0.0000],\n",
            "          [0.3544, 0.3231, 0.3225]]]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "softmax_socres shape: torch.Size([4, 8, 3, 3])\n",
            "values: tensor([[[[ 0.0806],\n",
            "          [-0.0513],\n",
            "          [ 0.1985]],\n",
            "\n",
            "         [[-0.2967],\n",
            "          [ 0.0786],\n",
            "          [-0.2241]],\n",
            "\n",
            "         [[-0.4151],\n",
            "          [-0.0953],\n",
            "          [-0.1230]],\n",
            "\n",
            "         [[-0.5342],\n",
            "          [-0.0968],\n",
            "          [-0.4016]],\n",
            "\n",
            "         [[-0.5054],\n",
            "          [-0.5340],\n",
            "          [-0.4670]],\n",
            "\n",
            "         [[ 0.1392],\n",
            "          [ 0.2579],\n",
            "          [-0.0288]],\n",
            "\n",
            "         [[ 0.6056],\n",
            "          [ 0.3262],\n",
            "          [ 0.6784]],\n",
            "\n",
            "         [[-0.5180],\n",
            "          [-0.3360],\n",
            "          [-0.2886]]],\n",
            "\n",
            "\n",
            "        [[[-0.1952],\n",
            "          [-0.1164],\n",
            "          [ 0.1127]],\n",
            "\n",
            "         [[-0.3607],\n",
            "          [-0.3278],\n",
            "          [-0.1549]],\n",
            "\n",
            "         [[ 0.1705],\n",
            "          [ 0.0294],\n",
            "          [ 0.0639]],\n",
            "\n",
            "         [[-0.2869],\n",
            "          [-0.3732],\n",
            "          [-0.2865]],\n",
            "\n",
            "         [[-0.6248],\n",
            "          [-0.5526],\n",
            "          [-0.6166]],\n",
            "\n",
            "         [[ 0.0863],\n",
            "          [-0.2570],\n",
            "          [ 0.1010]],\n",
            "\n",
            "         [[ 0.6042],\n",
            "          [ 0.7129],\n",
            "          [ 0.5794]],\n",
            "\n",
            "         [[-0.2629],\n",
            "          [-0.3482],\n",
            "          [-0.2357]]],\n",
            "\n",
            "\n",
            "        [[[-0.1422],\n",
            "          [ 0.0268],\n",
            "          [ 0.1016]],\n",
            "\n",
            "         [[-0.3559],\n",
            "          [-0.1993],\n",
            "          [-0.3849]],\n",
            "\n",
            "         [[-0.0819],\n",
            "          [-0.2657],\n",
            "          [-0.3830]],\n",
            "\n",
            "         [[-0.2351],\n",
            "          [-0.4204],\n",
            "          [-0.4658]],\n",
            "\n",
            "         [[-0.5112],\n",
            "          [-0.7527],\n",
            "          [-0.6952]],\n",
            "\n",
            "         [[ 0.3651],\n",
            "          [-0.0946],\n",
            "          [-0.3065]],\n",
            "\n",
            "         [[ 0.4502],\n",
            "          [ 0.5334],\n",
            "          [ 0.6304]],\n",
            "\n",
            "         [[-0.3166],\n",
            "          [-0.4002],\n",
            "          [-0.3647]]],\n",
            "\n",
            "\n",
            "        [[[-0.2170],\n",
            "          [-0.0333],\n",
            "          [-0.0611]],\n",
            "\n",
            "         [[-0.1932],\n",
            "          [ 0.0512],\n",
            "          [-0.1327]],\n",
            "\n",
            "         [[-0.1662],\n",
            "          [-0.0133],\n",
            "          [-0.2968]],\n",
            "\n",
            "         [[-0.2684],\n",
            "          [-0.2930],\n",
            "          [-0.3111]],\n",
            "\n",
            "         [[-0.4570],\n",
            "          [-0.7990],\n",
            "          [-0.5404]],\n",
            "\n",
            "         [[-0.0282],\n",
            "          [ 0.0987],\n",
            "          [-0.0238]],\n",
            "\n",
            "         [[ 0.4922],\n",
            "          [ 0.3625],\n",
            "          [ 0.3889]],\n",
            "\n",
            "         [[-0.4469],\n",
            "          [-0.2650],\n",
            "          [-0.3673]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "v_out shape torch.Size([4, 8, 3, 1])\n",
            "v_out: tensor([[[[ 0.0806],\n",
            "          [ 0.0147],\n",
            "          [ 0.0146]],\n",
            "\n",
            "         [[-0.2967],\n",
            "          [-0.1148],\n",
            "          [-0.1126]],\n",
            "\n",
            "         [[-0.4151],\n",
            "          [-0.2428],\n",
            "          [-0.2441]],\n",
            "\n",
            "         [[-0.5342],\n",
            "          [-0.3186],\n",
            "          [-0.3157]],\n",
            "\n",
            "         [[-0.5054],\n",
            "          [-0.5188],\n",
            "          [-0.5186]],\n",
            "\n",
            "         [[ 0.1392],\n",
            "          [ 0.1984],\n",
            "          [ 0.1989]],\n",
            "\n",
            "         [[ 0.6056],\n",
            "          [ 0.4665],\n",
            "          [ 0.4686]],\n",
            "\n",
            "         [[-0.5180],\n",
            "          [-0.4271],\n",
            "          [-0.4275]]],\n",
            "\n",
            "\n",
            "        [[[-0.1952],\n",
            "          [-0.1552],\n",
            "          [-0.1560]],\n",
            "\n",
            "         [[-0.3607],\n",
            "          [-0.3432],\n",
            "          [-0.3428]],\n",
            "\n",
            "         [[ 0.1705],\n",
            "          [ 0.1033],\n",
            "          [ 0.1038]],\n",
            "\n",
            "         [[-0.2869],\n",
            "          [-0.3292],\n",
            "          [-0.3286]],\n",
            "\n",
            "         [[-0.6248],\n",
            "          [-0.5878],\n",
            "          [-0.5876]],\n",
            "\n",
            "         [[ 0.0863],\n",
            "          [-0.0894],\n",
            "          [-0.0822]],\n",
            "\n",
            "         [[ 0.6042],\n",
            "          [ 0.6597],\n",
            "          [ 0.6593]],\n",
            "\n",
            "         [[-0.2629],\n",
            "          [-0.3038],\n",
            "          [-0.3044]]],\n",
            "\n",
            "\n",
            "        [[[-0.1422],\n",
            "          [-0.1422],\n",
            "          [-0.1422]],\n",
            "\n",
            "         [[-0.3559],\n",
            "          [-0.3559],\n",
            "          [-0.3559]],\n",
            "\n",
            "         [[-0.0819],\n",
            "          [-0.0819],\n",
            "          [-0.0819]],\n",
            "\n",
            "         [[-0.2351],\n",
            "          [-0.2351],\n",
            "          [-0.2351]],\n",
            "\n",
            "         [[-0.5112],\n",
            "          [-0.5112],\n",
            "          [-0.5112]],\n",
            "\n",
            "         [[ 0.3651],\n",
            "          [ 0.3651],\n",
            "          [ 0.3651]],\n",
            "\n",
            "         [[ 0.4502],\n",
            "          [ 0.4502],\n",
            "          [ 0.4502]],\n",
            "\n",
            "         [[-0.3166],\n",
            "          [-0.3166],\n",
            "          [-0.3166]]],\n",
            "\n",
            "\n",
            "        [[[-0.2170],\n",
            "          [-0.1238],\n",
            "          [-0.1038]],\n",
            "\n",
            "         [[-0.1932],\n",
            "          [-0.0774],\n",
            "          [-0.0970]],\n",
            "\n",
            "         [[-0.1662],\n",
            "          [-0.0855],\n",
            "          [-0.1487]],\n",
            "\n",
            "         [[-0.2684],\n",
            "          [-0.2805],\n",
            "          [-0.2905]],\n",
            "\n",
            "         [[-0.4570],\n",
            "          [-0.6239],\n",
            "          [-0.5920]],\n",
            "\n",
            "         [[-0.0282],\n",
            "          [ 0.0351],\n",
            "          [ 0.0150]],\n",
            "\n",
            "         [[ 0.4922],\n",
            "          [ 0.4282],\n",
            "          [ 0.4145]],\n",
            "\n",
            "         [[-0.4469],\n",
            "          [-0.3585],\n",
            "          [-0.3625]]]], device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
            "v_out shape before: torch.Size([4, 8, 3, 1])\n",
            "v_out shape: torch.Size([4, 3, 8])\n",
            "v_out reshaped: tensor([[[ 0.0806, -0.2967, -0.4151, -0.5342, -0.5054,  0.1392,  0.6056,\n",
            "          -0.5180],\n",
            "         [ 0.0147, -0.1148, -0.2428, -0.3186, -0.5188,  0.1984,  0.4665,\n",
            "          -0.4271],\n",
            "         [ 0.0146, -0.1126, -0.2441, -0.3157, -0.5186,  0.1989,  0.4686,\n",
            "          -0.4275]],\n",
            "\n",
            "        [[-0.1952, -0.3607,  0.1705, -0.2869, -0.6248,  0.0863,  0.6042,\n",
            "          -0.2629],\n",
            "         [-0.1552, -0.3432,  0.1033, -0.3292, -0.5878, -0.0894,  0.6597,\n",
            "          -0.3038],\n",
            "         [-0.1560, -0.3428,  0.1038, -0.3286, -0.5876, -0.0822,  0.6593,\n",
            "          -0.3044]],\n",
            "\n",
            "        [[-0.1422, -0.3559, -0.0819, -0.2351, -0.5112,  0.3651,  0.4502,\n",
            "          -0.3166],\n",
            "         [-0.1422, -0.3559, -0.0819, -0.2351, -0.5112,  0.3651,  0.4502,\n",
            "          -0.3166],\n",
            "         [-0.1422, -0.3559, -0.0819, -0.2351, -0.5112,  0.3651,  0.4502,\n",
            "          -0.3166]],\n",
            "\n",
            "        [[-0.2170, -0.1932, -0.1662, -0.2684, -0.4570, -0.0282,  0.4922,\n",
            "          -0.4469],\n",
            "         [-0.1238, -0.0774, -0.0855, -0.2805, -0.6239,  0.0351,  0.4282,\n",
            "          -0.3585],\n",
            "         [-0.1038, -0.0970, -0.1487, -0.2905, -0.5920,  0.0150,  0.4145,\n",
            "          -0.3625]]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
            "output shape torch.Size([4, 3, 8])\n",
            "output: tensor([[[-0.4441,  0.3979,  0.2389, -0.7696, -0.1736,  0.5899, -0.4127,\n",
            "           0.0868],\n",
            "         [-0.3663,  0.3429,  0.2932, -0.5886, -0.1831,  0.5718, -0.4420,\n",
            "           0.0300],\n",
            "         [-0.3656,  0.3437,  0.2924, -0.5873, -0.1832,  0.5731, -0.4441,\n",
            "           0.0303]],\n",
            "\n",
            "        [[-0.3813,  0.2834,  0.3562, -0.5319, -0.1298,  0.5142, -0.3100,\n",
            "          -0.0862],\n",
            "         [-0.3549,  0.2863,  0.3820, -0.5416, -0.1222,  0.5135, -0.2818,\n",
            "          -0.0259],\n",
            "         [-0.3562,  0.2875,  0.3805, -0.5415, -0.1226,  0.5148, -0.2842,\n",
            "          -0.0272]],\n",
            "\n",
            "        [[-0.4630,  0.2884,  0.1909, -0.5762, -0.1981,  0.5202, -0.3992,\n",
            "          -0.1024],\n",
            "         [-0.4630,  0.2884,  0.1909, -0.5762, -0.1981,  0.5202, -0.3992,\n",
            "          -0.1024],\n",
            "         [-0.4630,  0.2884,  0.1909, -0.5762, -0.1981,  0.5202, -0.3992,\n",
            "          -0.1024]],\n",
            "\n",
            "        [[-0.4069,  0.3135,  0.2996, -0.4819, -0.1533,  0.5509, -0.3576,\n",
            "           0.0330],\n",
            "         [-0.3249,  0.3103,  0.4003, -0.5057, -0.1338,  0.5474, -0.3696,\n",
            "           0.0271],\n",
            "         [-0.3336,  0.3019,  0.3755, -0.5256, -0.1424,  0.5359, -0.3588,\n",
            "           0.0416]]], device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, feature_length):\n",
        "        super().__init__()\n",
        "        self.scale = nn.Parameter(torch.ones(feature_length))\n",
        "        self.offset = nn.Parameter(torch.zeros(feature_length))\n",
        "\n",
        "    def forward(self, activations):\n",
        "        mean = torch.mean(activations, -1, keepdim=True)\n",
        "        #print(\"mean:\", mean)\n",
        "        #print(\"activations - mean\", activations - mean)\n",
        "        variance = torch.var(activations, -1, keepdim=True, unbiased=False)\n",
        "        normalized_activations = (activations - mean) / torch.sqrt(variance + 1e-6)\n",
        "        return (normalized_activations * self.scale) + self.offset"
      ],
      "metadata": {
        "id": "-Si4-i6PTlFu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_layer_norm():\n",
        "    feature_length = 4\n",
        "    length_x = 3\n",
        "    batch_size = 5\n",
        "    layer_norm = LayerNorm(feature_length)\n",
        "\n",
        "    activations = torch.rand(batch_size, length_x, feature_length)\n",
        "\n",
        "    print(\"activations:\", activations)\n",
        "    print(\"layer_normed:\", layer_norm(activations))\n",
        "    assert layer_norm(activations).shape == activations.shape\n",
        "\n",
        "test_layer_norm()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyzgWHqrWXL4",
        "outputId": "559323be-a1fa-495a-eb17-9d8a8e2ef46a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "activations: tensor([[[0.5992, 0.4175, 0.3157, 0.9827],\n",
            "         [0.7162, 0.8391, 0.7896, 0.7189],\n",
            "         [0.4911, 0.8424, 0.8687, 0.0647]],\n",
            "\n",
            "        [[0.5847, 0.6227, 0.4764, 0.1564],\n",
            "         [0.2567, 0.7449, 0.0281, 0.6179],\n",
            "         [0.3166, 0.7580, 0.0795, 0.9892]],\n",
            "\n",
            "        [[0.3658, 0.3713, 0.9095, 0.2346],\n",
            "         [0.4220, 0.1864, 0.4479, 0.7047],\n",
            "         [0.4035, 0.5706, 0.9836, 0.7179]],\n",
            "\n",
            "        [[0.6039, 0.7702, 0.2363, 0.8699],\n",
            "         [0.3465, 0.5148, 0.4237, 0.1254],\n",
            "         [0.5822, 0.1521, 0.6643, 0.8174]],\n",
            "\n",
            "        [[0.0329, 0.2912, 0.9359, 0.8245],\n",
            "         [0.2528, 0.2244, 0.9205, 0.2973],\n",
            "         [0.2273, 0.3442, 0.5416, 0.2200]]])\n",
            "layer_normed: tensor([[[ 0.0802, -0.6342, -1.0341,  1.5881],\n",
            "         [-0.9662,  1.4206,  0.4597, -0.9141],\n",
            "         [-0.2319,  0.8459,  0.9264, -1.5403]],\n",
            "\n",
            "        [[ 0.6799,  0.8870,  0.0893, -1.6562],\n",
            "         [-0.5448,  1.1687, -1.3470,  0.7231],\n",
            "         [-0.6132,  0.6214, -1.2765,  1.2682]],\n",
            "\n",
            "        [[-0.4030, -0.3816,  1.6931, -0.9085],\n",
            "         [-0.0994, -1.3830,  0.0418,  1.4407],\n",
            "         [-1.2457, -0.4615,  1.4772,  0.2301]],\n",
            "\n",
            "        [[-0.0669,  0.6225, -1.5919,  1.0363],\n",
            "         [-0.0422,  1.1260,  0.4933, -1.5770],\n",
            "         [ 0.1142, -1.6277,  0.4467,  1.0668]],\n",
            "\n",
            "        [[-1.3103, -0.6172,  1.1131,  0.8143],\n",
            "         [-0.5937, -0.6923,  1.7250, -0.4391],\n",
            "         [-0.8153,  0.0839,  1.6027, -0.8713]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, hiddenLayerWidth, d_e, p_dropout):\n",
        "        super().__init__()\n",
        "        self.mlp1 = nn.Parameter(torch.rand(d_e, hiddenLayerWidth))\n",
        "        self.mlp2 = nn.Parameter(torch.rand(hiddenLayerWidth, d_e))\n",
        "        self.mlp1_bias = nn.Parameter(torch.zeros(hiddenLayerWidth))\n",
        "        self.mlp2_bias = nn.Parameter(torch.zeros(d_e))\n",
        "        self.dropout = nn.Dropout(p_dropout)\n",
        "\n",
        "    def forward(self, activations):\n",
        "        activations = torch.matmul(activations, self.mlp1) + self.mlp1_bias\n",
        "        activations = activations.relu()\n",
        "        activations = torch.matmul(activations, self.mlp2) + self.mlp2_bias\n",
        "        activations = self.dropout(activations)\n",
        "        return activations\n"
      ],
      "metadata": {
        "id": "X7rAEAkFoNI5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_feed_forward():\n",
        "    hiddenLayerWidth = 3\n",
        "    d_e = 4\n",
        "    feed_forward = FeedForward(hiddenLayerWidth, d_e, 0.1)\n",
        "    activations = torch.rand(10, 5, d_e)\n",
        "\n",
        "    print(\"activations:\", activations)\n",
        "    output = feed_forward(activations)\n",
        "    print(\"feed forward:\", output)\n",
        "    assert output.shape == activations.shape\n",
        "\n",
        "test_feed_forward()"
      ],
      "metadata": {
        "id": "mLQj6GjmUFN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6644cdc-c677-4aa2-d820-1c418c06c9f1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "activations: tensor([[[0.7340, 0.5315, 0.3123, 0.1216],\n",
            "         [0.5905, 0.7827, 0.5246, 0.2818],\n",
            "         [0.3251, 0.9486, 0.7465, 0.8361],\n",
            "         [0.2152, 0.9253, 0.1532, 0.9265],\n",
            "         [0.2236, 0.7171, 0.8762, 0.0173]],\n",
            "\n",
            "        [[0.8524, 0.3550, 0.3433, 0.9243],\n",
            "         [0.8896, 0.5362, 0.7230, 0.5878],\n",
            "         [0.9095, 0.1898, 0.3825, 0.5456],\n",
            "         [0.3005, 0.6267, 0.9540, 0.6107],\n",
            "         [0.5523, 0.0066, 0.2676, 0.8121]],\n",
            "\n",
            "        [[0.9950, 0.5809, 0.3101, 0.2751],\n",
            "         [0.7424, 0.8780, 0.5843, 0.2509],\n",
            "         [0.9813, 0.0220, 0.9977, 0.5921],\n",
            "         [0.8650, 0.0410, 0.4478, 0.3561],\n",
            "         [0.9486, 0.0837, 0.5711, 0.3485]],\n",
            "\n",
            "        [[0.4403, 0.4605, 0.8871, 0.5350],\n",
            "         [0.1355, 0.8206, 0.3544, 0.7126],\n",
            "         [0.6834, 0.8336, 0.2763, 0.3298],\n",
            "         [0.4444, 0.8771, 0.9834, 0.1246],\n",
            "         [0.5706, 0.4069, 0.6391, 0.0773]],\n",
            "\n",
            "        [[0.1389, 0.9018, 0.4024, 0.0963],\n",
            "         [0.7001, 0.1936, 0.8411, 0.3009],\n",
            "         [0.3786, 0.3271, 0.6998, 0.0040],\n",
            "         [0.6517, 0.6713, 0.5993, 0.2971],\n",
            "         [0.9258, 0.1954, 0.3357, 0.1849]],\n",
            "\n",
            "        [[0.7486, 0.9722, 0.1511, 0.8022],\n",
            "         [0.8970, 0.7180, 0.6911, 0.3254],\n",
            "         [0.2980, 0.8007, 0.2312, 0.7990],\n",
            "         [0.9744, 0.0737, 0.7119, 0.0933],\n",
            "         [0.4882, 0.1985, 0.4847, 0.8580]],\n",
            "\n",
            "        [[0.7931, 0.7668, 0.0856, 0.5596],\n",
            "         [0.5907, 0.3563, 0.1798, 0.3588],\n",
            "         [0.6432, 0.3515, 0.6941, 0.5675],\n",
            "         [0.0396, 0.7319, 0.7740, 0.1062],\n",
            "         [0.4545, 0.8089, 0.9320, 0.0410]],\n",
            "\n",
            "        [[0.3789, 0.0083, 0.5629, 0.9603],\n",
            "         [0.4423, 0.4755, 0.0441, 0.7417],\n",
            "         [0.3104, 0.0465, 0.5967, 0.8351],\n",
            "         [0.9137, 0.7965, 0.5291, 0.6840],\n",
            "         [0.8655, 0.6908, 0.0942, 0.5899]],\n",
            "\n",
            "        [[0.5401, 0.5072, 0.8252, 0.8950],\n",
            "         [0.9357, 0.8639, 0.7286, 0.4936],\n",
            "         [0.1566, 0.5692, 0.1448, 0.0348],\n",
            "         [0.0785, 0.8113, 0.3020, 0.6845],\n",
            "         [0.5123, 0.6169, 0.3547, 0.4803]],\n",
            "\n",
            "        [[0.8766, 0.0255, 0.9677, 0.2960],\n",
            "         [0.1883, 0.8392, 0.4773, 0.1977],\n",
            "         [0.2555, 0.3456, 0.0164, 0.5640],\n",
            "         [0.6846, 0.8769, 0.0053, 0.1364],\n",
            "         [0.4524, 0.5967, 0.0216, 0.7232]]])\n",
            "feed forward: tensor([[[1.2012, 0.6953, 1.1721, 1.8521],\n",
            "         [1.4851, 0.8497, 0.0000, 0.0000],\n",
            "         [1.7776, 0.9839, 1.8207, 2.7763],\n",
            "         [1.3635, 0.7477, 0.0000, 2.1394],\n",
            "         [1.2602, 0.7261, 1.2164, 1.9506]],\n",
            "\n",
            "        [[1.4960, 0.8165, 1.6098, 2.3314],\n",
            "         [1.7542, 0.9824, 1.8138, 2.7207],\n",
            "         [1.2755, 0.7083, 1.3559, 1.9766],\n",
            "         [1.5344, 0.8480, 1.5938, 2.3926],\n",
            "         [0.9060, 0.4759, 1.0412, 0.0000]],\n",
            "\n",
            "        [[1.4958, 0.8591, 1.4846, 2.3083],\n",
            "         [1.6962, 0.9751, 1.6561, 2.6237],\n",
            "         [1.5786, 0.8688, 1.7162, 2.4463],\n",
            "         [1.0856, 0.0000, 0.0000, 1.6769],\n",
            "         [1.2513, 0.7013, 1.3248, 1.9321]],\n",
            "\n",
            "        [[1.4358, 0.7948, 1.5022, 2.2344],\n",
            "         [1.2527, 0.6907, 1.2738, 0.0000],\n",
            "         [1.4672, 0.8419, 1.4294, 2.2722],\n",
            "         [1.6585, 0.9523, 1.6169, 2.5672],\n",
            "         [1.1561, 0.6636, 1.1505, 1.7839]],\n",
            "\n",
            "        [[0.0000, 0.0000, 1.0180, 1.7002],\n",
            "         [1.2941, 0.0000, 1.3583, 2.0020],\n",
            "         [0.9561, 0.5488, 0.9513, 1.4753],\n",
            "         [1.4926, 0.8506, 1.4852, 2.3110],\n",
            "         [1.1169, 0.6385, 0.0000, 1.7191]],\n",
            "\n",
            "        [[1.7450, 0.9795, 1.7543, 2.7162],\n",
            "         [1.7776, 1.0146, 1.7743, 2.7490],\n",
            "         [1.3202, 0.7274, 1.3517, 2.0665],\n",
            "         [1.2414, 0.7082, 1.2852, 1.9083],\n",
            "         [1.1578, 0.6179, 1.2858, 1.8122]],\n",
            "\n",
            "        [[1.4770, 0.8369, 1.4731, 2.2922],\n",
            "         [0.0000, 0.0000, 0.9952, 1.5106],\n",
            "         [1.3992, 0.7744, 1.4760, 2.1745],\n",
            "         [1.1112, 0.6355, 1.0728, 1.7261],\n",
            "         [1.5472, 0.8927, 1.4994, 2.3918]],\n",
            "\n",
            "        [[1.0138, 0.5238, 0.0000, 1.5941],\n",
            "         [1.0353, 0.5648, 1.0906, 0.0000],\n",
            "         [0.9620, 0.5007, 1.1098, 1.5113],\n",
            "         [0.0000, 1.0703, 1.9358, 2.9547],\n",
            "         [1.4880, 0.8406, 1.4988, 2.3086]],\n",
            "\n",
            "        [[1.6577, 0.9044, 1.7693, 2.5877],\n",
            "         [2.0132, 1.1429, 2.0187, 3.1188],\n",
            "         [0.6677, 0.3915, 0.6099, 1.0338],\n",
            "         [1.1606, 0.6394, 1.1759, 0.0000],\n",
            "         [1.2761, 0.7170, 1.2903, 1.9838]],\n",
            "\n",
            "        [[1.3648, 0.0000, 1.4561, 2.1077],\n",
            "         [1.1689, 0.6709, 1.1175, 1.8154],\n",
            "         [0.7045, 0.3812, 0.7473, 1.1049],\n",
            "         [1.2632, 0.7402, 1.1786, 1.9498],\n",
            "         [0.0000, 0.0000, 1.1547, 1.7475]]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.multi_head_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], p_dropout, verbose)\n",
        "        self.layer_norm1 = LayerNorm(d_z)\n",
        "        self.feed_forward = FeedForward(d_mlp, d_z, p_dropout)\n",
        "        self.layer_norm2 = LayerNorm(d_z)\n",
        "\n",
        "    def forward(self, z, padding_mask):\n",
        "        z = self.layer_norm1(z)\n",
        "        z = z + self.multi_head_attention(z, z, padding_mask)\n",
        "        z = self.layer_norm2(z)\n",
        "        z = z + self.feed_forward(z)\n",
        "        return z"
      ],
      "metadata": {
        "id": "ikM15oD-qghT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        for i in range(num_layers):\n",
        "            encoder_layer = EncoderLayer(num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose)\n",
        "            self.layers.append(encoder_layer)\n",
        "        self.layers = nn.ModuleList(self.layers)\n",
        "        self.final_norm = LayerNorm(d_z)\n",
        "\n",
        "    def forward(self, z, padding_mask):\n",
        "        for layer in self.layers:\n",
        "            z = layer(z, padding_mask)\n",
        "        return self.final_norm(z)"
      ],
      "metadata": {
        "id": "zKGc6Xwr46Vh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.multi_head_self_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['MASKED'], p_dropout, verbose)\n",
        "        self.layer_norm1 = LayerNorm(d_x)\n",
        "        self.multi_head_global_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], p_dropout, verbose)\n",
        "        self.layer_norm2 = LayerNorm(d_x)\n",
        "        self.feed_forward = FeedForward(d_mlp, d_x, p_dropout)\n",
        "        self.layer_norm3 = LayerNorm(d_x)\n",
        "\n",
        "    def forward(self, z, x, src_mask, tgt_mask):\n",
        "        x = self.layer_norm1(x)\n",
        "        x = x + self.multi_head_self_attention(x, x, tgt_mask)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = x + self.multi_head_global_attention(z, x, src_mask)\n",
        "        x = self.layer_norm3(x)\n",
        "        x = x + self.feed_forward(x)\n",
        "        return x\n",
        "\n",
        "    def disable_subsequent_mask(self):\n",
        "        self.multi_head_self_attention.disable_subsequent_mask()"
      ],
      "metadata": {
        "id": "GSqElGm56xaK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        for i in range(num_layers):\n",
        "            decoder_layer = DecoderLayer(num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose)\n",
        "            self.layers.append(decoder_layer)\n",
        "        self.layers = nn.ModuleList(self.layers)\n",
        "        self.final_norm = LayerNorm(d_x)\n",
        "\n",
        "    def forward(self, z, x, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(z, x, src_mask, tgt_mask)\n",
        "        return self.final_norm(x)\n",
        "\n",
        "    def disable_subsequent_mask(self):\n",
        "        for layer in self.layers:\n",
        "            layer.multi_head_self_attention.disable_subsequent_mask()"
      ],
      "metadata": {
        "id": "LGX007WN8Bw6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoderTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, d_e, vocab_size, max_sequence_length, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.src_embedding = Embedding(vocab_size, d_e)\n",
        "        self.tgt_embedding = Embedding(vocab_size, d_e)\n",
        "        self.unembedding = Unembedding(vocab_size, d_e)\n",
        "        self.embedding_dropout = nn.Dropout(p_dropout)\n",
        "        self.positionalEmbedding = PositionalEmbedding(d_e, max_sequence_length)\n",
        "        self.encoder = Encoder(num_encoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose)\n",
        "        self.decoder = Decoder(num_decoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose)\n",
        "\n",
        "    def forward(self, z, x, src_mask, tgt_mask):\n",
        "        z = self.src_embedding(z) + self.positionalEmbedding(z)\n",
        "        z = self.embedding_dropout(z)\n",
        "        z = self.encoder(z, src_mask)\n",
        "        x = self.tgt_embedding(x) + self.positionalEmbedding(x)\n",
        "        x = self.embedding_dropout(x)\n",
        "        x = self.decoder(z, x, src_mask, tgt_mask)\n",
        "        #print(\"x after decoder:\", x.shape)\n",
        "        x = self.unembedding(x)\n",
        "        #print(\"x after unembedding:\", x.shape)\n",
        "        return x\n",
        "\n",
        "    def disable_subsequent_mask(self):\n",
        "        self.decoder.disable_subsequent_mask()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ixXJDrPF8RU9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enRawName = \"drive/MyDrive/colab data/multi30kEnTrain.txt\"\n",
        "deRawName = \"drive/MyDrive/colab data/multi30kDeTrain.txt\"\n",
        "en30kVal = \"drive/MyDrive/colab data/multi30kEnVal.txt\"\n",
        "de30kVal = \"drive/MyDrive/colab data/multi30kDeVal.txt\"\n",
        "englishCleanName = \"data/english_tokens.pkl\"\n",
        "germanCleanName = \"data/german_tokens.pkl\"\n",
        "englishSortedName = \"data/englishSorted.pkl\"\n",
        "germanSortedName = \"data/germanSorted.pkl\"\n",
        "\n",
        "truncEn = \"drive/MyDrive/colab data/truncEn.pkl\"\n",
        "truncDe = \"drive/MyDrive/colab data/truncDe.pkl\"\n",
        "\n",
        "enTokenizerName = \"drive/MyDrive/colab data/enTokenizer.pkl\"\n",
        "deTokenizerName = \"drive/MyDrive/colab data/deTokenizer.pkl\"\n",
        "pairsName = \"drive/MyDrive/colab data/pairs.pkl\"\n",
        "folder = \"drive/MyDrive/colab data/\"\n",
        "\n",
        "enTrainingFileName = folder + \"enTraining\"\n",
        "deTrainingFileName = folder + \"deTraining\"\n",
        "enTestFileName = folder + \"enTest\"\n",
        "deTestFileName = folder + \"deTest\"\n",
        "enValFileName = folder + \"enValidation\"\n",
        "deValFileName = folder + \"deValidation\"\n",
        "\n",
        "enCombinedFileName = folder + \"enCombined\"\n",
        "deCombinedFileName = folder + \"deCombined\""
      ],
      "metadata": {
        "id": "NqAcQrmPg4y0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc6Y11QfhEAv",
        "outputId": "4dd4fa20-daa6-4685-f963-b02450c8bd60"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class SentenceDataset(Dataset):\n",
        "\n",
        "#     TOKENIZER_SUFFIX = \"_tokenizer\"\n",
        "#     BOS_TOKEN = \"[SOS]\"\n",
        "#     EOS_TOKEN = \"[EOS]\"\n",
        "#     PAD_TOKEN = \"[PAD]\"\n",
        "#     UNK_TOKEN = \"[UNK]\"\n",
        "\n",
        "#     def __init__(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, sequence_start_index, sequence_end_index):\n",
        "#         src_sequences = self.to_sequences(self.load_doc(src_filename), sequence_start_index, sequence_end_index)\n",
        "#         tgt_sequences = self.to_sequences(self.load_doc(tgt_filename), sequence_start_index, sequence_end_index)\n",
        "#         src_sequences = [self.add_special_tokens(sequence) for sequence in src_sequences]\n",
        "#         tgt_sequences = [self.add_special_tokens(sequence) for sequence in tgt_sequences]\n",
        "#         self.src_tokenizer, self.tgt_tokenizer = self.setup_tokenizers(src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, src_filename + SentenceDataset.TOKENIZER_SUFFIX, tgt_filename + SentenceDataset.TOKENIZER_SUFFIX)\n",
        "#         # src_tokenized = self.src_tokenizer.encode_batch(src_sequences)\n",
        "#         # tgt_tokenized = self.tgt_tokenizer.encode_batch(tgt_sequences)\n",
        "#         # src_tensors = [torch.IntTensor(sequence.ids) for sequence in src_tokenized]\n",
        "#         # tgt_tensor = [torch.IntTensor(sequence.ids) for sequence in tgt_tokenized]\n",
        "#         self.pairs = self.pair_sequences(src_sequences, tgt_sequences)\n",
        "#         #print(\"pairs\", self.pairs)\n",
        "\n",
        "#     # load doc into memory\n",
        "#     def load_doc(self, filename):\n",
        "#         # open the file as read only\n",
        "#         file = open(filename, mode='rt')\n",
        "#         # read all text\n",
        "#         text = file.read()\n",
        "#         # close the file\n",
        "#         file.close()\n",
        "#         return text\n",
        "\n",
        "#     def add_special_tokens(self, sequence):\n",
        "#         sequence = self.BOS_TOKEN + \" \" + sequence + \" \" + self.EOS_TOKEN\n",
        "#         return sequence\n",
        "\n",
        "#     def pair_sequences(self, src_sequences, tgt_sequences):\n",
        "#         paired_sequences = list(zip(src_sequences, tgt_sequences))\n",
        "#         sorted_pairs = sorted(paired_sequences, key=lambda x: len(x[0]))\n",
        "#         return sorted_pairs\n",
        "\n",
        "#     # split a loaded document into sequences\n",
        "#     def to_sequences(self, doc, sequence_start_index, sequence_end_index):\n",
        "#         sequences = doc.strip().split('\\n')\n",
        "#         return sequences[sequence_start_index:sequence_end_index]\n",
        "\n",
        "#     def setup_tokenizers(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, src_tokenizer_name, tgt_tokenizer_name):\n",
        "#         print(\"creating tokenizer for \" + src_filename)\n",
        "#         src_tokenizer = Tokenizer(BPE(unk_token=SentenceDataset.UNK_TOKEN))\n",
        "#         src_tokenizer.pre_tokenizer = Whitespace()\n",
        "#         # src_tokenizer.post_processor = TemplateProcessing(\n",
        "#         #     single=\"[BOS] $A [EOS]\",\n",
        "#         #     special_tokens=[(\"[BOS]\", 0), (\"[EOS]\", 1)],\n",
        "#         # )\n",
        "#         trainer = BpeTrainer(vocab_size = src_vocab_size, special_tokens=[SentenceDataset.BOS_TOKEN, SentenceDataset.EOS_TOKEN, SentenceDataset.PAD_TOKEN, SentenceDataset.UNK_TOKEN])\n",
        "#         src_tokenizer.train([src_filename], trainer=trainer)\n",
        "#         pickle.dump(src_tokenizer, open(src_tokenizer_name, \"wb\"))\n",
        "\n",
        "#         print(\"creating tokenizer for \" + tgt_filename)\n",
        "#         tgt_tokenizer = Tokenizer(BPE(unk_token=SentenceDataset.UNK_TOKEN))\n",
        "#         tgt_tokenizer.pre_tokenizer = Whitespace()\n",
        "#         trainer = BpeTrainer(vocab_size = tgt_vocab_size, special_tokens=[SentenceDataset.BOS_TOKEN, SentenceDataset.EOS_TOKEN, SentenceDataset.PAD_TOKEN, SentenceDataset.UNK_TOKEN])\n",
        "#         tgt_tokenizer.train([tgt_filename], trainer=trainer)\n",
        "#         pickle.dump(tgt_tokenizer, open(tgt_tokenizer_name, \"wb\"))\n",
        "#         return src_tokenizer, tgt_tokenizer\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.pairs)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         src_seq, tgt_seq = self.pairs[index]\n",
        "#         return src_seq, tgt_seq\n"
      ],
      "metadata": {
        "id": "PkCWRs4-khoT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SequencePairDataset(Dataset):\n",
        "    BOS_TOKEN = \"[SOS]\"\n",
        "    EOS_TOKEN = \"[EOS]\"\n",
        "    PAD_TOKEN = \"[PAD]\"\n",
        "    UNK_TOKEN = \"[UNK]\"\n",
        "    PAD_ID = 2\n",
        "\n",
        "    def __init__(self, src_text, tgt_text, start_index, end_index):\n",
        "        src_sequences = self.to_sequences(src_text, start_index, end_index)\n",
        "        #tgt_sequences = self.to_sequences(tgt_text, start_index, end_index)\n",
        "        tgt_sequences = self.to_sequences(tgt_text, start_index, end_index)\n",
        "        #src_sequences = [self.add_special_tokens(sequence) for sequence in src_sequences]\n",
        "        #tgt_sequences = [self.add_special_tokens(sequence) for sequence in tgt_sequences]\n",
        "        self.pairs = self.pair_sequences(src_sequences, tgt_sequences)\n",
        "\n",
        "    def pair_sequences(self, src_sequences, tgt_sequences):\n",
        "        paired_sequences = list(zip(src_sequences, tgt_sequences))\n",
        "        sorted_pairs = sorted(paired_sequences, key=lambda x: len(x[0]))\n",
        "        return sorted_pairs\n",
        "\n",
        "    # split a loaded document into sequences\n",
        "    def to_sequences(self, doc, sequence_start_index, sequence_end_index):\n",
        "        sequences = doc.strip().split('\\n')\n",
        "        return sequences[sequence_start_index : sequence_end_index]\n",
        "\n",
        "    def add_special_tokens(self, sequence):\n",
        "        sequence = self.BOS_TOKEN + \" \" + sequence + \" \" + self.EOS_TOKEN\n",
        "        return sequence\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        src_seq, tgt_seq = self.pairs[index]\n",
        "        return src_seq, tgt_seq"
      ],
      "metadata": {
        "id": "vVeybRtnKtJX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainAndValidationSequenceDatasets():\n",
        "    def __init__(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, train_start_index, train_end_index, val_start_index, val_end_index):\n",
        "        src_text = self.load_doc(src_filename)\n",
        "        tgt_text = self.load_doc(tgt_filename)\n",
        "        self.train_dataset = SequencePairDataset(src_text, tgt_text, train_start_index, train_end_index)\n",
        "        self.val_dataset = SequencePairDataset(src_text, tgt_text, val_start_index, val_end_index)\n",
        "\n",
        "        # load doc into memory\n",
        "    def load_doc(self, filename):\n",
        "        # open the file as read only\n",
        "        file = open(filename, mode='rt')\n",
        "        # read all text\n",
        "        text = file.read()\n",
        "        # close the file\n",
        "        file.close()\n",
        "        return text"
      ],
      "metadata": {
        "id": "IJciEqbpJajo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "class PadCollate:\n",
        "    TOKENIZER_SUFFIX = \"_tokenizer\"\n",
        "\n",
        "    def __init__(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size):\n",
        "        self.src_tokenizer, self.tgt_tokenizer = self.setup_tokenizers(src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, src_filename + self.TOKENIZER_SUFFIX, tgt_filename + self.TOKENIZER_SUFFIX)\n",
        "\n",
        "    def setup_tokenizers(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, src_tokenizer_name, tgt_tokenizer_name):\n",
        "        print(\"creating tokenizer for \" + src_filename)\n",
        "        src_tokenizer = Tokenizer(BPE(unk_token=SequencePairDataset.UNK_TOKEN))\n",
        "        src_tokenizer.pre_tokenizer = Whitespace()\n",
        "        # src_tokenizer.post_processor = TemplateProcessing(\n",
        "        #     single=\"[BOS] $A [EOS]\",\n",
        "        #     special_tokens=[(\"[BOS]\", 0), (\"[EOS]\", 1)],\n",
        "        # )\n",
        "        trainer = BpeTrainer(vocab_size = src_vocab_size, special_tokens=[SequencePairDataset.BOS_TOKEN, SequencePairDataset.EOS_TOKEN, SequencePairDataset.PAD_TOKEN, SequencePairDataset.UNK_TOKEN])\n",
        "        src_tokenizer.train([src_filename], trainer=trainer)\n",
        "        pickle.dump(src_tokenizer, open(src_tokenizer_name, \"wb\"))\n",
        "\n",
        "        print(\"creating tokenizer for \" + tgt_filename)\n",
        "        tgt_tokenizer = Tokenizer(BPE(unk_token=SequencePairDataset.UNK_TOKEN))\n",
        "        tgt_tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = BpeTrainer(vocab_size = tgt_vocab_size, special_tokens=[SequencePairDataset.BOS_TOKEN, SequencePairDataset.EOS_TOKEN, SequencePairDataset.PAD_TOKEN, SequencePairDataset.UNK_TOKEN])\n",
        "        tgt_tokenizer.train([tgt_filename], trainer=trainer)\n",
        "        tgt_tokenizer.post_processor = TemplateProcessing(\n",
        "            single=\"[BOS] $A [EOS]\",\n",
        "            special_tokens=[(\"[BOS]\", 0), (\"[EOS]\", 1)],\n",
        "        )\n",
        "        pickle.dump(tgt_tokenizer, open(tgt_tokenizer_name, \"wb\"))\n",
        "        return src_tokenizer, tgt_tokenizer\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # max_len_src = max([len(pair[0].split()) for pair in batch])\n",
        "        # max_len_tgt = max([len(pair[1].split()) for pair in batch])\n",
        "\n",
        "        #tgt_sequence_lengths\n",
        "\n",
        "        self.src_tokenizer.no_padding()\n",
        "        self.tgt_tokenizer.no_padding()\n",
        "\n",
        "        self.src_tokenizer.no_truncation()\n",
        "        self.tgt_tokenizer.no_truncation()\n",
        "\n",
        "        src_tokenized = self.src_tokenizer.encode_batch([pair[0] for pair in batch])\n",
        "        tgt_tokenized = self.tgt_tokenizer.encode_batch([pair[1] for pair in batch])\n",
        "\n",
        "        max_len_src = max([len(sequence) for sequence in src_tokenized])\n",
        "        max_len_tgt = max([len(sequence) for sequence in tgt_tokenized])\n",
        "\n",
        "        # print(\"max len src:\", max_len_src)\n",
        "        # print(\"max len tgt:\", max_len_tgt)\n",
        "\n",
        "        self.src_tokenizer.enable_padding(pad_id = SequencePairDataset.PAD_ID, pad_token = SequencePairDataset.PAD_TOKEN)\n",
        "        self.src_tokenizer.enable_truncation(max_length=max_len_src)\n",
        "        self.tgt_tokenizer.enable_padding(pad_id = SequencePairDataset.PAD_ID, pad_token = SequencePairDataset.PAD_TOKEN)\n",
        "        self.tgt_tokenizer.enable_truncation(max_length=max_len_tgt)\n",
        "\n",
        "        # print(\"src batch:\", [pair[0] for pair in batch])\n",
        "        # print(\"tgt batch:\", [pair[1] for pair in batch])\n",
        "\n",
        "        src_tokenized = self.src_tokenizer.encode_batch([pair[0] for pair in batch])\n",
        "        tgt_tokenized = self.tgt_tokenizer.encode_batch([pair[1] for pair in batch])\n",
        "        # src_tokenized = [sequence.ids for sequence in src_tokenized]\n",
        "        # tgt_tokenized = [sequence.ids for sequence in tgt_tokenized]\n",
        "        # src_tensors = torch.IntTensor(src_tokenized)\n",
        "        # tgt_tensor = torch.IntTensor(tgt_tokenized)\n",
        "\n",
        "        return src_tokenized, tgt_tokenized"
      ],
      "metadata": {
        "id": "P1wzP-DDLZ1X"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "train_and_validation_sequence_datasets = TrainAndValidationSequenceDatasets(enRawName, deRawName, vocab_size, vocab_size, 0, 28250, 28250, 29000)\n",
        "train_dataset = train_and_validation_sequence_datasets.train_dataset\n",
        "val_dataset = train_and_validation_sequence_datasets.val_dataset"
      ],
      "metadata": {
        "id": "uDnN_IgsNIJD"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.__getitem__(0))"
      ],
      "metadata": {
        "id": "L3WwzUFdRbk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d95c88a9-01f2-44ac-a1c0-861104bb907d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('A dog in a car.', 'Ein Hund in einem Auto.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pad_collate = PadCollate(enRawName, deRawName, vocab_size, vocab_size)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128, collate_fn = pad_collate)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=128, collate_fn = pad_collate)"
      ],
      "metadata": {
        "id": "zLayKBKBLAYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eabff132-3923-4a56-c271-c20d671ca1c9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating tokenizer for drive/MyDrive/colab data/multi30kEnTrain.txt\n",
            "creating tokenizer for drive/MyDrive/colab data/multi30kDeTrain.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for src, tgt in train_dataloader:\n",
        "    print(src[0].ids, tgt[0].ids)\n",
        "    # print(\"decoded\", sequenceDataset.src_tokenizer.decode_batch([sequence.ids for sequence in src]))\n",
        "    # print(\"tgt\", tgt)\n",
        "\n",
        "    # print(\"mask:\", src[0].attention_mask)\n",
        "    break"
      ],
      "metadata": {
        "id": "7mnqolodQCYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2163ae5b-34e8-4ec4-e34b-e6c1caa3e7ca"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30, 177, 83, 57, 238, 15, 2, 2] [0, 109, 200, 100, 111, 786, 14, 1, 2, 2, 2, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(x, tokenizer):\n",
        "    x = torch.softmax(x, -1)\n",
        "    #print(\"x softmax:\", x)\n",
        "    x = torch.argmax(x, dim=-1)\n",
        "    x = x.tolist()\n",
        "    print(\"argmax x:\", x)\n",
        "    return tokenizer.decode(x)"
      ],
      "metadata": {
        "id": "ERDbLlQVzJUx"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_decode(tokenizer):\n",
        "    x = torch.tensor([[0, 5], [10, 20]], dtype=torch.float32)\n",
        "    words = decode(x, tokenizer)\n",
        "    print(words)\n",
        "\n",
        "test_decode(pad_collate.tgt_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSetg38X8TAo",
        "outputId": "e021c55e-f264-4982-c37c-1e4ca5bcd00f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax x: [1, 1]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_encoder_layers = 4\n",
        "num_decoder_layers = 4\n",
        "num_heads = 8\n",
        "d_attn = 256\n",
        "d_x = 256\n",
        "d_z = 256\n",
        "d_out = 256\n",
        "d_mid = 256\n",
        "d_mlp = 512\n",
        "d_e = 256\n",
        "max_sequence_length = 100\n",
        "p_dropout = 0.1"
      ],
      "metadata": {
        "id": "vFPxq_hkffx3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_decoder_transformer = EncoderDecoderTransformer(num_encoder_layers, num_decoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, d_e, vocab_size, max_sequence_length, 0.1, False).to(device)\n",
        "\n",
        "def train_model(encoder_decoder_transformer, train_dataloader, val_dataloader, src_tokenizer, tgt_tokenizer):\n",
        "    torch.manual_seed(25)\n",
        "    epochs = 1000\n",
        "    print(encoder_decoder_transformer.parameters())\n",
        "    nameSuffix = \"-maxData\"\n",
        "    state_dict_filename = folder + \"encoder_decoder_transformer_state_dict_\" + datetime.today().strftime('%Y-%m-%d %H') + nameSuffix\n",
        "    opt = optim.AdamW(encoder_decoder_transformer.parameters(), lr=0.0001, weight_decay=0.0001)\n",
        "    loss_function = nn.CrossEntropyLoss(label_smoothing=0.1, ignore_index=2)\n",
        "    #labelSmoothing = LabelSmoothing(2000, PADDING_IDX, 0.1)\n",
        "    training_step = 0\n",
        "    validation_step = 0\n",
        "    best_val_loss = 100\n",
        "    num_fails = 0\n",
        "    # Large models need this to actually train\n",
        "    for p in encoder_decoder_transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    for i in range(epochs):\n",
        "        epoch_time_start = time.time()\n",
        "        dataloader_iter = iter(train_dataloader)\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        for src_batch, tgt_batch in dataloader_iter:\n",
        "            # print(\"x:\", sequence_x)\n",
        "            # print(\"z:\", sequence_z)\n",
        "            # sequence_x, sequence_z = sequenceDataset.__getitem__(i)\n",
        "            src_tokens = torch.IntTensor([sequence.ids for sequence in src_batch]).to(device)\n",
        "            encoder_input = src_tokens\n",
        "            train_tgt_tokens = torch.IntTensor([sequence.ids for sequence in tgt_batch]).to(device)\n",
        "            decoder_input = train_tgt_tokens[:, :-1]\n",
        "            decoder_desired_output_train = train_tgt_tokens[:, 1:]\n",
        "            src_masks = torch.IntTensor([sequence.attention_mask for sequence in src_batch]).to(device)\n",
        "            tgt_masks = torch.IntTensor([sequence.attention_mask for sequence in tgt_batch])[:, :-1].to(device)\n",
        "            # print(\"src masks\", src_masks)\n",
        "            # print(\"tgt masks\", tgt_masks)\n",
        "            train_output = encoder_decoder_transformer(encoder_input, decoder_input, src_masks, tgt_masks)\n",
        "            #print(\"output\", train_output)\n",
        "            output_transpose = train_output.transpose(-1, -2) # output needs to be N, C, other dimension for torch cross entropy\n",
        "            loss = loss_function(output_transpose, decoder_desired_output_train.long())\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            train_losses.append(loss.item())\n",
        "            if (training_step % 20 == 0):\n",
        "                print(\"Completed training step\", training_step)\n",
        "            training_step += 1\n",
        "\n",
        "        for src_batch, tgt_batch in val_dataloader:\n",
        "            src_tokens = torch.IntTensor([sequence.ids for sequence in src_batch]).to(device)\n",
        "            encoder_input = src_tokens\n",
        "            val_tgt_tokens = torch.IntTensor([sequence.ids for sequence in tgt_batch]).to(device)\n",
        "            decoder_input = val_tgt_tokens[:, :-1]\n",
        "            decoder_desired_output_val = val_tgt_tokens[:, 1:]\n",
        "            src_masks = torch.IntTensor([sequence.attention_mask for sequence in src_batch]).to(device)\n",
        "            tgt_masks = torch.IntTensor([sequence.attention_mask for sequence in tgt_batch])[:, :-1].to(device)\n",
        "            val_output = encoder_decoder_transformer(encoder_input, decoder_input, src_masks, tgt_masks)\n",
        "            output_transpose = val_output.transpose(-1, -2) # output needs to be N, C, other dimension for torch cross entropy\n",
        "            loss = loss_function(output_transpose, decoder_desired_output_val.long())\n",
        "            val_losses.append(loss.item())\n",
        "            if (validation_step % 20 == 0):\n",
        "                print(\"Completed validation step\", validation_step)\n",
        "            validation_step += 1\n",
        "\n",
        "        print(\"epoch\", i, \"took\", time.time() - epoch_time_start)\n",
        "        print(\"avg training loss:\", sum(train_losses) / len(train_losses))\n",
        "        avg_val_loss = sum(val_losses)/ len(val_losses)\n",
        "        print(\"avg validation loss:\", avg_val_loss)\n",
        "        expected_train_output = tgt_tokenizer.decode(decoder_desired_output_train[0].tolist())\n",
        "        print(\"expected train output\", expected_train_output)\n",
        "        decoded_output = decode(train_output[0], tgt_tokenizer)\n",
        "        print(\"decoded train output:\", decoded_output)\n",
        "        expected_val_output = tgt_tokenizer.decode(decoder_desired_output_val[0].tolist())\n",
        "        print(\"expected validation output\", expected_val_output)\n",
        "        decoded_output = decode(val_output[0], tgt_tokenizer)\n",
        "        print(\"decoded validation output:\", decoded_output)\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(encoder_decoder_transformer.state_dict(), state_dict_filename)\n",
        "            print(\"Saved model state dict to\", state_dict_filename)\n",
        "            num_fails = 0\n",
        "        else:\n",
        "            print(\"Average validation loss did not decrease from \", best_val_loss)\n",
        "            num_fails += 1\n",
        "            print(\"Failed to decrease the average validation loss\", num_fails, \"times.\")\n",
        "            if num_fails >= 2:\n",
        "                print(\"Stopping training\")\n",
        "                break\n",
        "        print()\n",
        "        print()"
      ],
      "metadata": {
        "id": "NCHMIluxq6Su"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_encoder_decoder_transformer = EncoderDecoderTransformer(num_encoder_layers, num_decoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, d_e, vocab_size, max_sequence_length, p_dropout, False).to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "VLT6fo1IjhG3"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For some reason the Pytorch transformer doesn't have its own embedding layers. Adding them here.\n",
        "class ExtendedPytorchTransformer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, num_encoder_layers, num_decoder_layers, d_mlp, p_dropout, batch_first = True, norm_first = True):\n",
        "        super().__init__()\n",
        "        self.src_embedding = Embedding(vocab_size, d_model)\n",
        "        self.tgt_embedding = Embedding(vocab_size, d_model)\n",
        "        self.unembedding = Unembedding(vocab_size, d_model)\n",
        "        self.transformer = nn.Transformer(d_model, num_heads, num_encoder_layers, num_decoder_layers, d_mlp, p_dropout, batch_first = batch_first, norm_first = norm_first)\n",
        "        self.embedding_dropout = nn.Dropout(p_dropout)\n",
        "        self.positionalEmbedding = PositionalEmbedding(d_model, max_sequence_length)\n",
        "\n",
        "    def forward(self, src_sequence, tgt_sequence, src_mask, tgt_key_padding_mask):\n",
        "        src_sequence = self.src_embedding(src_sequence) + self.positionalEmbedding(src_sequence)\n",
        "        src_sequence = self.embedding_dropout(src_sequence)\n",
        "        tgt_sequence = self.tgt_embedding(tgt_sequence) + self.positionalEmbedding(tgt_sequence)\n",
        "        tgt_sequence = self.embedding_dropout(tgt_sequence)\n",
        "        tgt_mask = self.get_tgt_mask(tgt_sequence.shape[1], tgt_sequence.shape[1])\n",
        "        #print(src_sequence.shape, tgt_sequence.shape)\n",
        "        src_mask = ~src_mask.bool()\n",
        "        tgt_mask = ~tgt_mask.bool()\n",
        "        tgt_key_padding_mask = ~tgt_key_padding_mask.bool()\n",
        "        #print(src_mask.shape, tgt_mask.shape)\n",
        "        transformer_out = self.transformer(src_sequence, tgt_sequence, src_key_padding_mask = src_mask, tgt_key_padding_mask = tgt_key_padding_mask, tgt_mask = tgt_mask)\n",
        "        #print(transformer_out)\n",
        "        return self.unembedding(transformer_out)\n",
        "\n",
        "\n",
        "    def get_tgt_mask(self, length_x, length_z):\n",
        "        mask = torch.tril(torch.ones(length_x, length_z) == 1).to(device)\n",
        "        #print(mask.shape)\n",
        "        return mask\n"
      ],
      "metadata": {
        "id": "ClmyrsdyPwPP"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_encoder_decoder_transformer = ExtendedPytorchTransformer(d_e, num_heads, num_encoder_layers, num_decoder_layers, d_mlp, p_dropout, batch_first = True, norm_first = True).to(device)"
      ],
      "metadata": {
        "id": "3fnNWwETNGXi"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(pytorch_encoder_decoder_transformer, train_dataloader, val_dataloader, pad_collate.src_tokenizer, pad_collate.tgt_tokenizer)"
      ],
      "metadata": {
        "id": "vyDhyMksMfRm",
        "outputId": "1289346d-dbc6-405c-ea2c-d11202e0c129",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object Module.parameters at 0x7f70c30c52a0>\n",
            "Completed training step 0\n",
            "Completed training step 20\n",
            "Completed training step 40\n",
            "Completed training step 60\n",
            "Completed training step 80\n",
            "Completed training step 100\n",
            "Completed training step 120\n",
            "Completed training step 140\n",
            "Completed training step 160\n",
            "Completed training step 180\n",
            "Completed training step 200\n",
            "Completed training step 220\n",
            "Completed validation step 0\n",
            "epoch 0 took 21.481504440307617\n",
            "avg training loss: 6.997570639821738\n",
            "avg validation loss: 6.18852702776591\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 124, 100, 100, 12, 12, 12, 12, 114, 12, 12, 111, 12, 12, 12, 111, 12, 111, 12, 14, 114, 14, 14, 14, 1, 14, 14, 14, 14, 14, 1, 14, 14, 14, 14, 111, 14, 14, 14, 14, 14, 114, 14, 111, 1, 14, 111, 14, 12]\n",
            "decoded train output: Ein Mann in in , , , , und , , einem , , , einem , einem , . und . . . . . . . . . . . . einem . . . . . und . einem . einem . ,\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 124, 124, 100, 100, 111, 111, 12, 111, 12, 12, 111, 12, 12, 12, 12, 12, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1, 111, 14]\n",
            "decoded validation output: Ein Mann Mann in in einem einem , einem , , einem , , , , , . . . . . . . . . . . . . . . . einem .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 240\n",
            "Completed training step 260\n",
            "Completed training step 280\n",
            "Completed training step 300\n",
            "Completed training step 320\n",
            "Completed training step 340\n",
            "Completed training step 360\n",
            "Completed training step 380\n",
            "Completed training step 400\n",
            "Completed training step 420\n",
            "Completed training step 440\n",
            "epoch 1 took 21.163043975830078\n",
            "avg training loss: 5.68054419081675\n",
            "avg validation loss: 5.69205888112386\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 124, 100, 111, 100, 111, 111, 114, 114, 265, 114, 111, 138, 265, 114, 111, 138, 111, 114, 14, 103, 111, 14, 14, 138, 14, 14, 14, 14, 14, 1, 14, 14, 1, 14, 111, 14, 14, 14, 14, 14, 111, 14, 111, 14, 14, 14, 14, 14]\n",
            "decoded train output: Ein Mann in einem in einem einem und und Hemd und einem Frau Hemd und einem Frau einem und . ein einem . . Frau . . . . . . . . einem . . . . . einem . einem . . . . .\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 124, 124, 100, 100, 111, 111, 12, 111, 103, 12, 152, 14, 14, 12, 14, 12, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
            "decoded validation output: Ein Mann Mann in in einem einem , einem ein , die . . , . , . . . . . . . . . . . . . . . . . . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 460\n",
            "Completed training step 480\n",
            "Completed training step 500\n",
            "Completed training step 520\n",
            "Completed training step 540\n",
            "Completed training step 560\n",
            "Completed training step 580\n",
            "Completed training step 600\n",
            "Completed training step 620\n",
            "Completed training step 640\n",
            "Completed training step 660\n",
            "epoch 2 took 20.724610090255737\n",
            "avg training loss: 5.261209960436929\n",
            "avg validation loss: 5.421294768651326\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 124, 100, 111, 114, 111, 265, 100, 100, 276, 114, 111, 138, 265, 100, 111, 138, 265, 114, 138, 103, 111, 14, 14, 138, 14, 14, 14, 14, 14, 1, 14, 14, 14, 14, 114, 14, 1, 14, 14, 14, 14, 1, 14, 14, 14, 138, 14, 14]\n",
            "decoded train output: Ein Mann in einem und einem Hemd in in blauen und einem Frau Hemd in einem Frau Hemd und Frau ein einem . . Frau . . . . . . . . . und . . . . . . . . Frau . .\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 124, 124, 100, 100, 111, 294, 100, 103, 103, 12, 250, 138, 245, 114, 14, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1, 14, 14, 1, 1, 14, 138]\n",
            "decoded validation output: Ein Mann Mann in in einem roten in ein ein , während Frau Straße und . . . . . . . . . . . . . . . . . Frau\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 680\n",
            "Completed training step 700\n",
            "Completed training step 720\n",
            "Completed training step 740\n",
            "Completed training step 760\n",
            "Completed training step 780\n",
            "Completed training step 800\n",
            "Completed training step 820\n",
            "Completed training step 840\n",
            "Completed training step 860\n",
            "Completed training step 880\n",
            "Completed validation step 20\n",
            "epoch 3 took 21.02577304840088\n",
            "avg training loss: 4.981435875008009\n",
            "avg validation loss: 5.1796159744262695\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 124, 13, 124, 13, 124, 265, 12, 100, 448, 12, 111, 138, 13, 12, 111, 138, 281, 12, 13, 250, 448, 14, 14, 138, 14, 14, 14, 14, 14, 1, 14, 14, 14, 14, 14, 14, 1, 14, 14, 14, 14, 14, 13, 14, 14, 14, 14, 14]\n",
            "decoded train output: Ein Mann - Mann - Mann Hemd , in Hintergrund , einem Frau - , einem Frau weißen , - während Hintergrund . . Frau . . . . . . . . . . . . . . . . - . . . . .\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 235, 196, 100, 117, 111, 281, 119, 107, 107, 114, 250, 138, 124, 114, 13, 117, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1, 14, 14, 14, 14, 14, 14, 14, 13, 14]\n",
            "decoded validation output: Ein Junge Mädchen in auf einem weißen mit eine eine und während Frau Mann und - auf . . . . . . . . . . . . . . . . - .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 900\n",
            "Completed training step 920\n",
            "Completed training step 940\n",
            "Completed training step 960\n",
            "Completed training step 980\n",
            "Completed training step 1000\n",
            "Completed training step 1020\n",
            "Completed training step 1040\n",
            "Completed training step 1060\n",
            "Completed training step 1080\n",
            "Completed training step 1100\n",
            "epoch 4 took 20.97369146347046\n",
            "avg training loss: 4.7329241269314455\n",
            "avg validation loss: 4.980093399683635\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 124, 13, 503, 13, 503, 265, 12, 12, 448, 12, 111, 281, 13, 117, 111, 401, 200, 12, 200, 121, 448, 12, 14, 461, 13, 14, 14, 111, 14, 1, 14, 14, 14, 14, 286, 14, 14, 14, 14, 200, 14, 14, 14, 14, 14, 14, 14, 14]\n",
            "decoded train output: Ein Mann - schwarzer - schwarzer Hemd , , Hintergrund , einem weißen - auf einem großen Hund , Hund der Hintergrund , . anderen - . . einem . . . . . Wasser . . . . Hund . . . . . . . .\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 196, 100, 176, 111, 294, 114, 111, 107, 114, 250, 138, 265, 114, 13, 12, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 138, 14, 14]\n",
            "decoded validation output: Ein kleines Mädchen in vor einem roten und einem eine und während Frau Hemd und - , . . . . . . . . . . . . . . . . Frau . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 1120\n",
            "Completed training step 1140\n",
            "Completed training step 1160\n",
            "Completed training step 1180\n",
            "Completed training step 1200\n",
            "Completed training step 1220\n",
            "Completed training step 1240\n",
            "Completed training step 1260\n",
            "Completed training step 1280\n",
            "Completed training step 1300\n",
            "Completed training step 1320\n",
            "epoch 5 took 21.195815563201904\n",
            "avg training loss: 4.512769525407126\n",
            "avg validation loss: 4.793641567230225\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 503, 13, 532, 13, 532, 200, 12, 117, 448, 12, 111, 281, 456, 12, 111, 281, 448, 12, 200, 250, 448, 12, 12, 138, 14, 14, 12, 14, 14, 1, 14, 14, 12, 14, 12, 14, 14, 14, 14, 14, 14, 14, 111, 14, 14, 13, 14, 12]\n",
            "decoded train output: Ein schwarzer - weißer - weißer Hund , auf Hintergrund , einem weißen Kleidung , einem weißen Hintergrund , Hund während Hintergrund , , Frau . . , . . . . , . , . . . . . . . einem . . - . ,\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 474, 114, 244, 107, 119, 250, 138, 103, 119, 13, 14, 1, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 13, 12, 1]\n",
            "decoded validation output: Ein kleines Kind steht vor einem Tisch und hält eine mit während Frau ein mit - . - . . . . . . . . . . . . . . . - ,\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 1340\n",
            "Completed training step 1360\n",
            "Completed training step 1380\n",
            "Completed training step 1400\n",
            "Completed training step 1420\n",
            "Completed training step 1440\n",
            "Completed training step 1460\n",
            "Completed training step 1480\n",
            "Completed training step 1500\n",
            "Completed training step 1520\n",
            "Completed training step 1540\n",
            "Completed validation step 40\n",
            "epoch 6 took 21.01984977722168\n",
            "avg training loss: 4.320391273067008\n",
            "avg validation loss: 4.639123360315959\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 124, 13, 532, 13, 532, 200, 12, 117, 457, 12, 111, 281, 281, 12, 111, 281, 456, 13, 13, 12, 448, 14, 14, 138, 14, 14, 14, 111, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
            "decoded train output: Ein Mann - weißer - weißer Hund , auf Freien , einem weißen weißen , einem weißen Kleidung - - , Hintergrund . . Frau . . . einem . . . . . . . . . . . . . . . . . . .\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 294, 114, 244, 107, 119, 250, 138, 731, 119, 13, 119, 1, 119, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 75, 1, 12, 14]\n",
            "decoded validation output: Ein kleines Kind steht vor einem roten und hält eine mit während Frau Gesicht mit - mit mit . . . . . . . . . . . . . . s , .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 1560\n",
            "Completed training step 1580\n",
            "Completed training step 1600\n",
            "Completed training step 1620\n",
            "Completed training step 1640\n",
            "Completed training step 1660\n",
            "Completed training step 1680\n",
            "Completed training step 1700\n",
            "Completed training step 1720\n",
            "Completed training step 1740\n",
            "Completed training step 1760\n",
            "epoch 7 took 21.398507595062256\n",
            "avg training loss: 4.156078202692092\n",
            "avg validation loss: 4.506513675053914\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 214, 13, 532, 13, 532, 200, 12, 12, 448, 12, 111, 276, 454, 12, 111, 281, 454, 12, 138, 250, 448, 254, 254, 138, 13, 100, 12, 14, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 254, 14, 14, 14, 254]\n",
            "decoded train output: Ein schwarz - weißer - weißer Hund , , Hintergrund , einem blauen Jacke , einem weißen Jacke , Frau während Hintergrund ist ist Frau - in , . . . . . . . . . . . . . . . ist . . . ist\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 474, 114, 244, 107, 114, 250, 138, 314, 119, 13, 119, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 319, 314, 14]\n",
            "decoded validation output: Ein kleines Kind steht vor einem Tisch und hält eine und während Frau Hand mit - mit . . . . . . . . . . . . . . . . trägt Hand .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 1780\n",
            "Completed training step 1800\n",
            "Completed training step 1820\n",
            "Completed training step 1840\n",
            "Completed training step 1860\n",
            "Completed training step 1880\n",
            "Completed training step 1900\n",
            "Completed training step 1920\n",
            "Completed training step 1940\n",
            "Completed training step 1960\n",
            "Completed training step 1980\n",
            "epoch 8 took 21.05023503303528\n",
            "avg training loss: 4.012566058344431\n",
            "avg validation loss: 4.411529302597046\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 214, 13, 532, 13, 532, 200, 12, 117, 448, 12, 111, 281, 13, 12, 111, 281, 786, 13, 276, 152, 448, 254, 14, 138, 153, 14, 14, 111, 14, 1, 14, 14, 14, 14, 100, 14, 14, 14, 14, 14, 14, 14, 14, 100, 14, 14, 14, 14]\n",
            "decoded train output: Ein schwarz - weißer - weißer Hund , auf Hintergrund , einem weißen - , einem weißen Auto - blauen die Hintergrund ist . Frau zu . . einem . . . . . in . . . . . . . . in . . . .\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 474, 114, 244, 107, 119, 121, 314, 731, 119, 319, 119, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1, 244, 102, 319]\n",
            "decoded validation output: Ein kleines Kind steht vor einem Tisch und hält eine mit der Hand Gesicht mit trägt mit . . . . . . . . . . . . . . . hält er trägt\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 2000\n",
            "Completed training step 2020\n",
            "Completed training step 2040\n",
            "Completed training step 2060\n",
            "Completed training step 2080\n",
            "Completed training step 2100\n",
            "Completed training step 2120\n",
            "Completed training step 2140\n",
            "Completed training step 2160\n",
            "Completed training step 2180\n",
            "Completed training step 2200\n",
            "epoch 9 took 21.100396156311035\n",
            "avg training loss: 3.8801925484411317\n",
            "avg validation loss: 4.300066391626994\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 214, 13, 532, 13, 532, 200, 12, 117, 448, 12, 111, 281, 493, 12, 111, 281, 493, 153, 13, 100, 448, 254, 14, 138, 14, 14, 14, 14, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 153, 14, 14, 14, 153]\n",
            "decoded train output: Ein schwarz - weißer - weißer Hund , auf Hintergrund , einem weißen Gebäude , einem weißen Gebäude zu - in Hintergrund ist . Frau . . . . . . . . . . . . . . . . . . zu . . . zu\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 694, 114, 244, 107, 114, 121, 209, 731, 114, 319, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1]\n",
            "decoded validation output: Ein kleines Kind steht vor einem Baum und hält eine und der Person Gesicht und trägt . . . . . . . . . . . . . . . . . . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 2220\n",
            "Completed training step 2240\n",
            "Completed training step 2260\n",
            "Completed training step 2280\n",
            "Completed training step 2300\n",
            "Completed training step 2320\n",
            "Completed training step 2340\n",
            "Completed training step 2360\n",
            "Completed training step 2380\n",
            "Completed training step 2400\n",
            "Completed training step 2420\n",
            "Completed validation step 60\n",
            "epoch 10 took 21.18507957458496\n",
            "avg training loss: 3.762674061960764\n",
            "avg validation loss: 4.2168184121449785\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 214, 13, 532, 13, 532, 200, 117, 117, 448, 12, 111, 401, 401, 100, 111, 276, 920, 100, 534, 100, 448, 254, 14, 461, 153, 14, 14, 14, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
            "decoded train output: Ein schwarz - weißer - weißer Hund auf auf Hintergrund , einem großen großen in einem blauen Raum in Menschenmenge in Hintergrund ist . anderen zu . . . . . . . . . . . . . . . . . . . . . .\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 694, 114, 244, 107, 225, 250, 294, 294, 319, 319, 152, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1, 14, 14, 1]\n",
            "decoded validation output: Ein kleines Kind steht vor einem Baum und hält eine aus während roten roten trägt trägt die . . . . . . . . . . . . . . . . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 2440\n",
            "Completed training step 2460\n",
            "Completed training step 2480\n",
            "Completed training step 2500\n",
            "Completed training step 2520\n",
            "Completed training step 2540\n",
            "Completed training step 2560\n",
            "Completed training step 2580\n",
            "Completed training step 2600\n",
            "Completed training step 2620\n",
            "Completed training step 2640\n",
            "epoch 11 took 20.805487871170044\n",
            "avg training loss: 3.655448257653422\n",
            "avg validation loss: 4.140235026677449\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 214, 13, 532, 13, 532, 200, 117, 117, 440, 12, 125, 401, 570, 117, 125, 401, 281, 13, 209, 152, 448, 107, 107, 601, 100, 14, 14, 450, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 281, 14, 14, 14, 153, 14, 14, 14, 100]\n",
            "decoded train output: Ein schwarz - weißer - weißer Hund auf auf Strand , einer großen Stadt auf einer großen weißen - Person die Hintergrund eine eine große in . . sehen . . . . . . . . . . weißen . . . zu . . . in\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 694, 114, 244, 144, 225, 121, 184, 294, 184, 75, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
            "decoded validation output: Ein kleines Kind steht vor einem Baum und hält einen aus der von roten von s . . . . . . . . . . . . . . . . . . . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 2660\n",
            "Completed training step 2680\n",
            "Completed training step 2700\n",
            "Completed training step 2720\n",
            "Completed training step 2740\n",
            "Completed training step 2760\n",
            "Completed training step 2780\n",
            "Completed training step 2800\n",
            "Completed training step 2820\n",
            "Completed training step 2840\n",
            "Completed training step 2860\n",
            "epoch 12 took 21.24514651298523\n",
            "avg training loss: 3.56111838159518\n",
            "avg validation loss: 4.074360529581706\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 214, 13, 532, 13, 532, 200, 12, 117, 448, 12, 125, 401, 625, 119, 111, 281, 456, 12, 209, 250, 448, 153, 107, 534, 100, 153, 153, 450, 14, 1, 14, 14, 153, 14, 153, 153, 14, 14, 14, 14, 14, 14, 14, 153, 14, 14, 153, 153]\n",
            "decoded train output: Ein schwarz - weißer - weißer Hund , auf Hintergrund , einer großen Bühne mit einem weißen Kleidung , Person während Hintergrund zu eine Menschenmenge in zu zu sehen . . . zu . zu zu . . . . . . . zu . . zu zu\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 2066, 114, 441, 107, 114, 250, 119, 454, 100, 319, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1]\n",
            "decoded validation output: Ein kleines Kind steht vor einem Spiegel und hat eine und während mit Jacke in trägt . . . . . . . . . . . . . . . . . . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 2880\n",
            "Completed training step 2900\n",
            "Completed training step 2920\n",
            "Completed training step 2940\n",
            "Completed training step 2960\n",
            "Completed training step 2980\n",
            "Completed training step 3000\n",
            "Completed training step 3020\n",
            "Completed training step 3040\n",
            "Completed training step 3060\n",
            "Completed training step 3080\n",
            "Completed validation step 80\n",
            "epoch 13 took 20.957047700881958\n",
            "avg training loss: 3.4736998523522287\n",
            "avg validation loss: 4.036106427510579\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 214, 13, 532, 13, 532, 200, 12, 148, 448, 12, 125, 401, 1889, 119, 111, 281, 281, 148, 281, 148, 448, 254, 107, 281, 153, 14, 153, 450, 14, 1, 14, 14, 14, 14, 153, 14, 14, 14, 14, 448, 153, 14, 14, 153, 14, 14, 14, 14]\n",
            "decoded train output: Ein schwarz - weißer - weißer Hund , im Hintergrund , einer großen Umgebung mit einem weißen weißen im weißen im Hintergrund ist eine weißen zu . zu sehen . . . . . zu . . . . Hintergrund zu . . zu . . . .\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 694, 114, 441, 636, 12, 121, 314, 102, 100, 319, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
            "decoded validation output: Ein kleines Kind steht vor einem Baum und hat seine , der Hand er in trägt . . . . . . . . . . . . . . . . . . . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 3100\n",
            "Completed training step 3120\n",
            "Completed training step 3140\n",
            "Completed training step 3160\n",
            "Completed training step 3180\n",
            "Completed training step 3200\n",
            "Completed training step 3220\n",
            "Completed training step 3240\n",
            "Completed training step 3260\n",
            "Completed training step 3280\n",
            "Completed training step 3300\n",
            "epoch 14 took 21.125434160232544\n",
            "avg training loss: 3.3972623995526345\n",
            "avg validation loss: 3.9800358613332114\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 214, 13, 532, 13, 532, 200, 313, 117, 457, 12, 125, 276, 1446, 12, 111, 276, 1446, 1446, 276, 100, 448, 254, 107, 100, 148, 254, 153, 450, 14, 1, 14, 14, 14, 254, 14, 14, 14, 153, 14, 138, 14, 14, 14, 14, 254, 14, 14, 153]\n",
            "decoded train output: Ein schwarz - weißer - weißer Hund fährt auf Freien , einer blauen Decke , einem blauen Decke Decke blauen in Hintergrund ist eine in im ist zu sehen . . . . ist . . . zu . Frau . . . . ist . . zu\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 2066, 114, 244, 144, 114, 121, 107, 294, 119, 319, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
            "decoded validation output: Ein kleines Kind steht vor einem Spiegel und hält einen und der eine roten mit trägt . . . . . . . . . . . . . . . . . . . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 3320\n",
            "Completed training step 3340\n",
            "Completed training step 3360\n",
            "Completed training step 3380\n",
            "Completed training step 3400\n",
            "Completed training step 3420\n",
            "Completed training step 3440\n",
            "Completed training step 3460\n",
            "Completed training step 3480\n",
            "Completed training step 3500\n",
            "Completed training step 3520\n",
            "epoch 15 took 20.96242618560791\n",
            "avg training loss: 3.325484955472644\n",
            "avg validation loss: 3.952025294303894\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 214, 13, 532, 13, 532, 786, 117, 117, 448, 212, 125, 276, 1889, 100, 111, 281, 1354, 13, 209, 152, 448, 254, 107, 601, 137, 100, 14, 450, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 461, 14, 14, 14, 14]\n",
            "decoded train output: Ein schwarz - weißer - weißer Auto auf auf Hintergrund eines einer blauen Umgebung in einem weißen Tür - Person die Hintergrund ist eine große te in . sehen . . . . . . . . . . . . . . anderen . . . .\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 2066, 114, 441, 177, 12, 273, 107, 454, 114, 75, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
            "decoded validation output: Ein kleines Kind steht vor einem Spiegel und hat sich , das eine Jacke und s . . . . . . . . . . . . . . . . . . . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 3540\n",
            "Completed training step 3560\n",
            "Completed training step 3580\n",
            "Completed training step 3600\n",
            "Completed training step 3620\n",
            "Completed training step 3640\n",
            "Completed training step 3660\n",
            "Completed training step 3680\n",
            "Completed training step 3700\n",
            "Completed training step 3720\n",
            "Completed training step 3740\n",
            "Completed validation step 100\n",
            "epoch 16 took 21.17332911491394\n",
            "avg training loss: 3.2588328894446876\n",
            "avg validation loss: 3.8988933165868125\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 214, 13, 532, 13, 532, 786, 117, 117, 448, 117, 111, 276, 1446, 148, 111, 276, 1446, 12, 276, 152, 448, 107, 107, 1377, 100, 100, 100, 450, 14, 1, 14, 14, 153, 14, 14, 14, 153, 153, 14, 461, 14, 14, 14, 153, 14, 100, 14, 100]\n",
            "decoded train output: Ein schwarz - weißer - weißer Auto auf auf Hintergrund auf einem blauen Decke im einem blauen Decke , blauen die Hintergrund eine eine blaue in in in sehen . . . zu . . . zu zu . anderen . . . zu . in . in\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 2066, 114, 441, 636, 12, 121, 454, 454, 119, 14, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
            "decoded validation output: Ein kleines Kind steht vor einem Spiegel und hat seine , der Jacke Jacke mit . . . . . . . . . . . . . . . . . . . . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 3760\n",
            "Completed training step 3780\n",
            "Completed training step 3800\n",
            "Completed training step 3820\n",
            "Completed training step 3840\n",
            "Completed training step 3860\n",
            "Completed training step 3880\n",
            "Completed training step 3900\n",
            "Completed training step 3920\n",
            "Completed training step 3940\n",
            "Completed training step 3960\n",
            "epoch 17 took 21.171783924102783\n",
            "avg training loss: 3.194526538589961\n",
            "avg validation loss: 3.878715912501017\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 214, 13, 532, 13, 532, 786, 12, 148, 457, 12, 125, 401, 1889, 12, 111, 281, 1889, 12, 100, 100, 448, 100, 107, 601, 1889, 14, 100, 450, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 448, 14, 14, 14, 153, 14, 450, 14, 100]\n",
            "decoded train output: Ein schwarz - weißer - weißer Auto , im Freien , einer großen Umgebung , einem weißen Umgebung , in in Hintergrund in eine große Umgebung . in sehen . . . . . . . . . . Hintergrund . . . zu . sehen . in\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 2066, 176, 441, 144, 225, 121, 107, 454, 225, 75, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
            "decoded validation output: Ein kleines Kind steht vor einem Spiegel vor hat einen aus der eine Jacke aus s . . . . . . . . . . . . . . . . . . . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 3980\n",
            "Completed training step 4000\n",
            "Completed training step 4020\n",
            "Completed training step 4040\n",
            "Completed training step 4060\n",
            "Completed training step 4080\n",
            "Completed training step 4100\n",
            "Completed training step 4120\n",
            "Completed training step 4140\n",
            "Completed training step 4160\n",
            "Completed training step 4180\n",
            "epoch 18 took 20.95351219177246\n",
            "avg training loss: 3.136692084877739\n",
            "avg validation loss: 3.8363537788391113\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 214, 13, 532, 13, 532, 3668, 117, 117, 457, 12, 125, 798, 1962, 119, 125, 281, 1889, 12, 209, 250, 448, 107, 107, 601, 153, 14, 153, 14, 14, 1, 14, 14, 14, 14, 14, 14, 153, 14, 14, 100, 14, 14, 14, 14, 14, 14, 153, 153]\n",
            "decoded train output: Ein schwarz - weißer - weißer Rennauto auf auf Freien , einer grauen Gegend mit einer weißen Umgebung , Person während Hintergrund eine eine große zu . zu . . . . . . . . zu . . in . . . . . . zu zu\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 2066, 114, 441, 636, 114, 273, 454, 454, 14, 319, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1]\n",
            "decoded validation output: Ein kleines Kind steht vor einem Spiegel und hat seine und das Jacke Jacke . trägt . . . . . . . . . . . . . . . . . . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 4200\n",
            "Completed training step 4220\n",
            "Completed training step 4240\n",
            "Completed training step 4260\n",
            "Completed training step 4280\n",
            "Completed training step 4300\n",
            "Completed training step 4320\n",
            "Completed training step 4340\n",
            "Completed training step 4360\n",
            "Completed training step 4380\n",
            "Completed training step 4400\n",
            "epoch 19 took 21.166724681854248\n",
            "avg training loss: 3.0814625526445485\n",
            "avg validation loss: 3.7743129332860312\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 214, 13, 532, 13, 532, 3338, 254, 148, 448, 12, 111, 798, 1158, 12, 111, 276, 1354, 13, 13, 152, 448, 254, 107, 649, 153, 153, 153, 450, 14, 1, 14, 14, 153, 14, 14, 14, 153, 14, 14, 461, 14, 14, 14, 14, 14, 450, 14, 153]\n",
            "decoded train output: Ein schwarz - weißer - weißer Rennwagen ist im Hintergrund , einem grauen Welle , einem blauen Tür - - die Hintergrund ist eine weiße zu zu zu sehen . . . zu . . . zu . . anderen . . . . . sehen . zu\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 2066, 114, 441, 150, 12, 121, 1039, 454, 441, 441, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
            "decoded validation output: Ein kleines Kind steht vor einem Spiegel und hat den , der rote Jacke hat hat . . . . . . . . . . . . . . . . . . . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 4420\n",
            "Completed training step 4440\n",
            "Completed training step 4460\n",
            "Completed training step 4480\n",
            "Completed training step 4500\n",
            "Completed training step 4520\n",
            "Completed training step 4540\n",
            "Completed training step 4560\n",
            "Completed training step 4580\n",
            "Completed training step 4600\n",
            "Completed training step 4620\n",
            "Completed training step 4640\n",
            "Completed validation step 120\n",
            "epoch 20 took 20.87893533706665\n",
            "avg training loss: 3.027850044259119\n",
            "avg validation loss: 3.7286214033762612\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 214, 13, 322, 907, 532, 200, 148, 148, 511, 117, 111, 798, 12, 119, 111, 281, 1158, 12, 12, 152, 448, 153, 107, 601, 153, 100, 153, 450, 14, 1, 14, 14, 153, 14, 14, 14, 14, 14, 153, 448, 14, 14, 14, 14, 14, 14, 14, 153]\n",
            "decoded train output: Ein schwarz - weiß gekleideter weißer Hund im im Gras auf einem grauen , mit einem weißen Welle , , die Hintergrund zu eine große zu in zu sehen . . . zu . . . . . zu Hintergrund . . . . . . . zu\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 2066, 114, 441, 150, 12, 273, 294, 454, 319, 319, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
            "decoded validation output: Ein kleines Kind steht vor einem Spiegel und hat den , das roten Jacke trägt trägt . . . . . . . . . . . . . . . . . . . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 4660\n",
            "Completed training step 4680\n",
            "Completed training step 4700\n",
            "Completed training step 4720\n",
            "Completed training step 4740\n",
            "Completed training step 4760\n",
            "Completed training step 4780\n",
            "Completed training step 4800\n",
            "Completed training step 4820\n",
            "Completed training step 4840\n",
            "Completed training step 4860\n",
            "epoch 21 took 21.473821878433228\n",
            "avg training loss: 2.9774881412540624\n",
            "avg validation loss: 3.6982946395874023\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 214, 13, 532, 13, 532, 786, 148, 148, 448, 117, 111, 798, 1962, 119, 111, 798, 1158, 12, 100, 100, 448, 254, 107, 601, 13, 14, 153, 450, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 153, 1158, 14, 14, 14, 153, 100, 450, 14, 14]\n",
            "decoded train output: Ein schwarz - weißer - weißer Auto im im Hintergrund auf einem grauen Gegend mit einem grauen Welle , in in Hintergrund ist eine große - . zu sehen . . . . . . . . . zu Welle . . . zu in sehen . .\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 2066, 12, 441, 654, 12, 273, 107, 454, 339, 339, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1, 14, 14, 14, 14]\n",
            "decoded validation output: Ein kleines Kind steht vor einem Spiegel , hat dabei , das eine Jacke macht macht . . . . . . . . . . . . . . . . . . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 4880\n",
            "Completed training step 4900\n",
            "Completed training step 4920\n",
            "Completed training step 4940\n",
            "Completed training step 4960\n",
            "Completed training step 4980\n",
            "Completed training step 5000\n",
            "Completed training step 5020\n",
            "Completed training step 5040\n",
            "Completed training step 5060\n",
            "Completed training step 5080\n",
            "epoch 22 took 21.506802797317505\n",
            "avg training loss: 2.9273644799021037\n",
            "avg validation loss: 3.6953173081080117\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 214, 13, 532, 13, 532, 3338, 148, 148, 2076, 117, 111, 798, 1889, 119, 111, 281, 1354, 13, 12, 250, 448, 254, 107, 601, 1158, 184, 153, 450, 14, 1, 14, 153, 153, 14, 14, 153, 14, 14, 153, 461, 254, 14, 14, 14, 153, 12, 14, 153]\n",
            "decoded train output: Ein schwarz - weißer - weißer Rennwagen im im Vordergrund auf einem grauen Umgebung mit einem weißen Tür - , während Hintergrund ist eine große Welle von zu sehen . . zu zu . . zu . . zu anderen ist . . . zu , . zu\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 2066, 114, 441, 150, 12, 273, 454, 454, 441, 441, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
            "decoded validation output: Ein kleines Kind steht vor einem Spiegel und hat den , das Jacke Jacke hat hat . . . . . . . . . . . . . . . . . . . .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-07 15-maxData\n",
            "\n",
            "\n",
            "Completed training step 5100\n",
            "Completed training step 5120\n",
            "Completed training step 5140\n",
            "Completed training step 5160\n",
            "Completed training step 5180\n",
            "Completed training step 5200\n",
            "Completed training step 5220\n",
            "Completed training step 5240\n",
            "Completed training step 5260\n",
            "Completed training step 5280\n",
            "Completed training step 5300\n",
            "Completed validation step 140\n",
            "epoch 23 took 21.44963812828064\n",
            "avg training loss: 2.882179191209612\n",
            "avg validation loss: 3.700756033261617\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 214, 13, 532, 907, 532, 200, 148, 148, 457, 117, 111, 798, 245, 119, 111, 798, 1158, 12, 13, 100, 448, 107, 107, 601, 61, 100, 153, 450, 14, 1, 153, 14, 14, 14, 448, 14, 14, 153, 14, 461, 100, 14, 153, 153, 1, 100, 14, 100]\n",
            "decoded train output: Ein schwarz - weißer gekleideter weißer Hund im im Freien auf einem grauen Straße mit einem grauen Welle , - in Hintergrund eine eine große e in zu sehen . zu . . . Hintergrund . . zu . anderen in . zu zu in . in\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 2066, 114, 441, 150, 12, 273, 107, 454, 317, 441, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
            "decoded validation output: Ein kleines Kind steht vor einem Spiegel und hat den , das eine Jacke ab hat . . . . . . . . . . . . . . . . . . . .\n",
            "Average validation loss did not decrease from  3.6953173081080117\n",
            "Failed to decrease the average validation loss 1 times.\n",
            "\n",
            "\n",
            "Completed training step 5320\n",
            "Completed training step 5340\n",
            "Completed training step 5360\n",
            "Completed training step 5380\n",
            "Completed training step 5400\n",
            "Completed training step 5420\n",
            "Completed training step 5440\n",
            "Completed training step 5460\n",
            "Completed training step 5480\n",
            "Completed training step 5500\n",
            "Completed training step 5520\n",
            "epoch 24 took 21.11242175102234\n",
            "avg training loss: 2.8376148038320412\n",
            "avg validation loss: 3.7106557289759317\n",
            "expected train output Ein schwarz - rot - weißes Rennwagen saust im Vordergrund auf einer grauen Strecke mit einer blauen Ban de , im Hintergrund ist eine verschwommen e Menschenmenge zu sehen .\n",
            "argmax x: [109, 214, 13, 611, 13, 532, 200, 117, 148, 2076, 117, 111, 798, 245, 100, 111, 281, 12, 12, 12, 250, 448, 254, 107, 209, 61, 100, 153, 450, 14, 1, 100, 100, 153, 153, 153, 153, 153, 153, 153, 100, 450, 153, 100, 153, 153, 100, 153, 100]\n",
            "decoded train output: Ein schwarz - rot - weißer Hund auf im Vordergrund auf einem grauen Straße in einem weißen , , , während Hintergrund ist eine Person e in zu sehen . in in zu zu zu zu zu zu zu in sehen zu in zu zu in zu in\n",
            "expected validation output Ein kleines Kind steht vor einem Spiegel und hat Schwierigkeiten , seine rote Jacke auszu ziehen .\n",
            "argmax x: [109, 340, 299, 220, 176, 111, 2066, 114, 441, 144, 12, 273, 454, 454, 184, 441, 14, 1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
            "decoded validation output: Ein kleines Kind steht vor einem Spiegel und hat einen , das Jacke Jacke von hat . . . . . . . . . . . . . . . . . . . .\n",
            "Average validation loss did not decrease from  3.6953173081080117\n",
            "Failed to decrease the average validation loss 2 times.\n",
            "Stopping training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#state_dict_filename = folder + \"encoder_decoder_transformer_state_dict_\" + \"2024-08-02 15\"\n",
        "state_dict = torch.load(state_dict_filename, map_location = device)\n",
        "print(state_dict.keys())\n",
        "encoder_decoder_transformer.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGBx0brYMJE6",
        "outputId": "4aa324fd-afa6-493a-c7e0-b697011f14ab"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['src_embedding.table.weight', 'tgt_embedding.table.weight', 'unembedding.weight.weight', 'unembedding.weight.bias', 'positionalEmbedding.table.weight', 'encoder.layers.0.multi_head_attention.weight_query.weight', 'encoder.layers.0.multi_head_attention.weight_query.bias', 'encoder.layers.0.multi_head_attention.weight_key.weight', 'encoder.layers.0.multi_head_attention.weight_key.bias', 'encoder.layers.0.multi_head_attention.weight_value.weight', 'encoder.layers.0.multi_head_attention.weight_value.bias', 'encoder.layers.0.multi_head_attention.weight_out.weight', 'encoder.layers.0.multi_head_attention.weight_out.bias', 'encoder.layers.0.layer_norm1.scale', 'encoder.layers.0.layer_norm1.offset', 'encoder.layers.0.feed_forward.mlp1', 'encoder.layers.0.feed_forward.mlp2', 'encoder.layers.0.feed_forward.mlp1_bias', 'encoder.layers.0.feed_forward.mlp2_bias', 'encoder.layers.0.layer_norm2.scale', 'encoder.layers.0.layer_norm2.offset', 'encoder.layers.1.multi_head_attention.weight_query.weight', 'encoder.layers.1.multi_head_attention.weight_query.bias', 'encoder.layers.1.multi_head_attention.weight_key.weight', 'encoder.layers.1.multi_head_attention.weight_key.bias', 'encoder.layers.1.multi_head_attention.weight_value.weight', 'encoder.layers.1.multi_head_attention.weight_value.bias', 'encoder.layers.1.multi_head_attention.weight_out.weight', 'encoder.layers.1.multi_head_attention.weight_out.bias', 'encoder.layers.1.layer_norm1.scale', 'encoder.layers.1.layer_norm1.offset', 'encoder.layers.1.feed_forward.mlp1', 'encoder.layers.1.feed_forward.mlp2', 'encoder.layers.1.feed_forward.mlp1_bias', 'encoder.layers.1.feed_forward.mlp2_bias', 'encoder.layers.1.layer_norm2.scale', 'encoder.layers.1.layer_norm2.offset', 'encoder.layers.2.multi_head_attention.weight_query.weight', 'encoder.layers.2.multi_head_attention.weight_query.bias', 'encoder.layers.2.multi_head_attention.weight_key.weight', 'encoder.layers.2.multi_head_attention.weight_key.bias', 'encoder.layers.2.multi_head_attention.weight_value.weight', 'encoder.layers.2.multi_head_attention.weight_value.bias', 'encoder.layers.2.multi_head_attention.weight_out.weight', 'encoder.layers.2.multi_head_attention.weight_out.bias', 'encoder.layers.2.layer_norm1.scale', 'encoder.layers.2.layer_norm1.offset', 'encoder.layers.2.feed_forward.mlp1', 'encoder.layers.2.feed_forward.mlp2', 'encoder.layers.2.feed_forward.mlp1_bias', 'encoder.layers.2.feed_forward.mlp2_bias', 'encoder.layers.2.layer_norm2.scale', 'encoder.layers.2.layer_norm2.offset', 'encoder.layers.3.multi_head_attention.weight_query.weight', 'encoder.layers.3.multi_head_attention.weight_query.bias', 'encoder.layers.3.multi_head_attention.weight_key.weight', 'encoder.layers.3.multi_head_attention.weight_key.bias', 'encoder.layers.3.multi_head_attention.weight_value.weight', 'encoder.layers.3.multi_head_attention.weight_value.bias', 'encoder.layers.3.multi_head_attention.weight_out.weight', 'encoder.layers.3.multi_head_attention.weight_out.bias', 'encoder.layers.3.layer_norm1.scale', 'encoder.layers.3.layer_norm1.offset', 'encoder.layers.3.feed_forward.mlp1', 'encoder.layers.3.feed_forward.mlp2', 'encoder.layers.3.feed_forward.mlp1_bias', 'encoder.layers.3.feed_forward.mlp2_bias', 'encoder.layers.3.layer_norm2.scale', 'encoder.layers.3.layer_norm2.offset', 'encoder.final_norm.scale', 'encoder.final_norm.offset', 'decoder.layers.0.multi_head_self_attention.weight_query.weight', 'decoder.layers.0.multi_head_self_attention.weight_query.bias', 'decoder.layers.0.multi_head_self_attention.weight_key.weight', 'decoder.layers.0.multi_head_self_attention.weight_key.bias', 'decoder.layers.0.multi_head_self_attention.weight_value.weight', 'decoder.layers.0.multi_head_self_attention.weight_value.bias', 'decoder.layers.0.multi_head_self_attention.weight_out.weight', 'decoder.layers.0.multi_head_self_attention.weight_out.bias', 'decoder.layers.0.layer_norm1.scale', 'decoder.layers.0.layer_norm1.offset', 'decoder.layers.0.multi_head_global_attention.weight_query.weight', 'decoder.layers.0.multi_head_global_attention.weight_query.bias', 'decoder.layers.0.multi_head_global_attention.weight_key.weight', 'decoder.layers.0.multi_head_global_attention.weight_key.bias', 'decoder.layers.0.multi_head_global_attention.weight_value.weight', 'decoder.layers.0.multi_head_global_attention.weight_value.bias', 'decoder.layers.0.multi_head_global_attention.weight_out.weight', 'decoder.layers.0.multi_head_global_attention.weight_out.bias', 'decoder.layers.0.layer_norm2.scale', 'decoder.layers.0.layer_norm2.offset', 'decoder.layers.0.feed_forward.mlp1', 'decoder.layers.0.feed_forward.mlp2', 'decoder.layers.0.feed_forward.mlp1_bias', 'decoder.layers.0.feed_forward.mlp2_bias', 'decoder.layers.0.layer_norm3.scale', 'decoder.layers.0.layer_norm3.offset', 'decoder.layers.1.multi_head_self_attention.weight_query.weight', 'decoder.layers.1.multi_head_self_attention.weight_query.bias', 'decoder.layers.1.multi_head_self_attention.weight_key.weight', 'decoder.layers.1.multi_head_self_attention.weight_key.bias', 'decoder.layers.1.multi_head_self_attention.weight_value.weight', 'decoder.layers.1.multi_head_self_attention.weight_value.bias', 'decoder.layers.1.multi_head_self_attention.weight_out.weight', 'decoder.layers.1.multi_head_self_attention.weight_out.bias', 'decoder.layers.1.layer_norm1.scale', 'decoder.layers.1.layer_norm1.offset', 'decoder.layers.1.multi_head_global_attention.weight_query.weight', 'decoder.layers.1.multi_head_global_attention.weight_query.bias', 'decoder.layers.1.multi_head_global_attention.weight_key.weight', 'decoder.layers.1.multi_head_global_attention.weight_key.bias', 'decoder.layers.1.multi_head_global_attention.weight_value.weight', 'decoder.layers.1.multi_head_global_attention.weight_value.bias', 'decoder.layers.1.multi_head_global_attention.weight_out.weight', 'decoder.layers.1.multi_head_global_attention.weight_out.bias', 'decoder.layers.1.layer_norm2.scale', 'decoder.layers.1.layer_norm2.offset', 'decoder.layers.1.feed_forward.mlp1', 'decoder.layers.1.feed_forward.mlp2', 'decoder.layers.1.feed_forward.mlp1_bias', 'decoder.layers.1.feed_forward.mlp2_bias', 'decoder.layers.1.layer_norm3.scale', 'decoder.layers.1.layer_norm3.offset', 'decoder.layers.2.multi_head_self_attention.weight_query.weight', 'decoder.layers.2.multi_head_self_attention.weight_query.bias', 'decoder.layers.2.multi_head_self_attention.weight_key.weight', 'decoder.layers.2.multi_head_self_attention.weight_key.bias', 'decoder.layers.2.multi_head_self_attention.weight_value.weight', 'decoder.layers.2.multi_head_self_attention.weight_value.bias', 'decoder.layers.2.multi_head_self_attention.weight_out.weight', 'decoder.layers.2.multi_head_self_attention.weight_out.bias', 'decoder.layers.2.layer_norm1.scale', 'decoder.layers.2.layer_norm1.offset', 'decoder.layers.2.multi_head_global_attention.weight_query.weight', 'decoder.layers.2.multi_head_global_attention.weight_query.bias', 'decoder.layers.2.multi_head_global_attention.weight_key.weight', 'decoder.layers.2.multi_head_global_attention.weight_key.bias', 'decoder.layers.2.multi_head_global_attention.weight_value.weight', 'decoder.layers.2.multi_head_global_attention.weight_value.bias', 'decoder.layers.2.multi_head_global_attention.weight_out.weight', 'decoder.layers.2.multi_head_global_attention.weight_out.bias', 'decoder.layers.2.layer_norm2.scale', 'decoder.layers.2.layer_norm2.offset', 'decoder.layers.2.feed_forward.mlp1', 'decoder.layers.2.feed_forward.mlp2', 'decoder.layers.2.feed_forward.mlp1_bias', 'decoder.layers.2.feed_forward.mlp2_bias', 'decoder.layers.2.layer_norm3.scale', 'decoder.layers.2.layer_norm3.offset', 'decoder.layers.3.multi_head_self_attention.weight_query.weight', 'decoder.layers.3.multi_head_self_attention.weight_query.bias', 'decoder.layers.3.multi_head_self_attention.weight_key.weight', 'decoder.layers.3.multi_head_self_attention.weight_key.bias', 'decoder.layers.3.multi_head_self_attention.weight_value.weight', 'decoder.layers.3.multi_head_self_attention.weight_value.bias', 'decoder.layers.3.multi_head_self_attention.weight_out.weight', 'decoder.layers.3.multi_head_self_attention.weight_out.bias', 'decoder.layers.3.layer_norm1.scale', 'decoder.layers.3.layer_norm1.offset', 'decoder.layers.3.multi_head_global_attention.weight_query.weight', 'decoder.layers.3.multi_head_global_attention.weight_query.bias', 'decoder.layers.3.multi_head_global_attention.weight_key.weight', 'decoder.layers.3.multi_head_global_attention.weight_key.bias', 'decoder.layers.3.multi_head_global_attention.weight_value.weight', 'decoder.layers.3.multi_head_global_attention.weight_value.bias', 'decoder.layers.3.multi_head_global_attention.weight_out.weight', 'decoder.layers.3.multi_head_global_attention.weight_out.bias', 'decoder.layers.3.layer_norm2.scale', 'decoder.layers.3.layer_norm2.offset', 'decoder.layers.3.feed_forward.mlp1', 'decoder.layers.3.feed_forward.mlp2', 'decoder.layers.3.feed_forward.mlp1_bias', 'decoder.layers.3.feed_forward.mlp2_bias', 'decoder.layers.3.layer_norm3.scale', 'decoder.layers.3.layer_norm3.offset', 'decoder.final_norm.scale', 'decoder.final_norm.offset'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_with_one_sample(model):\n",
        "    dataloader_iter = iter(train_dataloader)\n",
        "    for src_batch, tgt_batch in dataloader_iter:\n",
        "        # print(\"x:\", sequence_x)\n",
        "        # print(\"z:\", sequence_z)\n",
        "        # sequence_x, sequence_z = sequenceDataset.__getitem__(i)\n",
        "        src_batch = [src_batch[0]]\n",
        "        tgt_batch = [tgt_batch[0]]\n",
        "        src_tokens = torch.IntTensor([sequence.ids for sequence in src_batch]).to(device)\n",
        "        encoder_input = src_tokens\n",
        "        print(\"encoder input\", encoder_input)\n",
        "        decoded_input = pad_collate.src_tokenizer.decode(encoder_input[0].tolist())\n",
        "        print(\"decoded input\", decoded_input)\n",
        "        train_tgt_tokens = torch.IntTensor([sequence.ids for sequence in tgt_batch]).to(device)\n",
        "        decoder_input = train_tgt_tokens[:, :-1]\n",
        "        print(\"decoder input\", decoder_input)\n",
        "        decoder_desired_output_train = train_tgt_tokens[:, 1:]\n",
        "        src_masks = torch.IntTensor([sequence.attention_mask for sequence in src_batch]).to(device)\n",
        "        tgt_masks = torch.IntTensor([sequence.attention_mask for sequence in tgt_batch])[:, :-1].to(device)\n",
        "        # print(\"src masks\", src_masks)\n",
        "        # print(\"tgt masks\", tgt_masks)\n",
        "        train_output = encoder_decoder_transformer(encoder_input, decoder_input, src_masks, tgt_masks)\n",
        "        print(train_output.shape)\n",
        "        decoded_output = decode(train_output[0], pad_collate.tgt_tokenizer)\n",
        "        print(\"decoded output\", decoded_output)\n",
        "        break\n",
        "\n",
        "test_model_with_one_sample(encoder_decoder_transformer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bertgYLZzZ7m",
        "outputId": "3d579f81-a422-4eb2-a04d-cbf8c57805a5"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder input tensor([[  30,   93,   89,   94, 1602,   15,    2,    2]], device='cuda:0',\n",
            "       dtype=torch.int32)\n",
            "decoded input A man on the sea .\n",
            "decoder input tensor([[   0,   30,   93,   89,   94, 1602,   15,    1,    2]],\n",
            "       device='cuda:0', dtype=torch.int32)\n",
            "torch.Size([1, 9, 10000])\n",
            "argmax x: [30, 93, 89, 94, 1602, 15, 1, 2, 2]\n",
            "decoded output A man on the sea .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(encoder_decoder_transformer.state_dict(), state_dict_filename)"
      ],
      "metadata": {
        "id": "kuI-KXWBa1Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_from_tokens(model, input, src_tokenizer, tgt_tokenizer):\n",
        "    #model.disable_subsequent_mask()\n",
        "    src_tokenizer.no_padding()\n",
        "    tgt_tokenizer.no_padding()\n",
        "\n",
        "    src_tokenizer.no_truncation()\n",
        "    tgt_tokenizer.no_truncation()\n",
        "    src_sequence = input\n",
        "    print(src_sequence)\n",
        "    src_sequence = src_tokenizer.encode(src_sequence)\n",
        "    print(src_sequence)\n",
        "    print(src_tokenizer.decode(src_sequence.ids))\n",
        "    src_sequence = torch.IntTensor(src_sequence.ids).unsqueeze(0).to(device)\n",
        "    print(\"src tokens\", src_sequence)\n",
        "    tgt_sequence = torch.IntTensor([0]).unsqueeze(0).to(device)\n",
        "    src_mask = torch.ones(src_sequence.shape, dtype=torch.int32).to(device)\n",
        "    print(\"decoder input\", tgt_sequence)\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        length_gen = 100\n",
        "        for i in range(length_gen):\n",
        "            tgt_mask = torch.ones(tgt_sequence.shape, dtype=torch.int32).to(device)\n",
        "            prediction = model(src_sequence, tgt_sequence, src_mask, tgt_mask)\n",
        "            #print(\"prediction:\", prediction)\n",
        "            prediction = torch.softmax(prediction, -1)\n",
        "            #print(\"softmax prediction:\", prediction.shape)\n",
        "            prediction = torch.argmax(prediction, dim=-1)\n",
        "            print(\"argmax prediction:\", prediction)\n",
        "            print(\"actual prediction:\", tgt_tokenizer.decode(prediction[0].tolist()))\n",
        "            last_token = prediction[0][-1]\n",
        "            tgt_sequence = torch.cat((tgt_sequence, last_token.unsqueeze(0).unsqueeze(0)), dim=-1)\n",
        "            if last_token == 1:\n",
        "                break\n",
        "    return tgt_sequence\n",
        "\n",
        "tgt_sequence = predict_from_tokens(pytorch_encoder_decoder_transformer, \"A man sings a song.\", pad_collate.src_tokenizer, pad_collate.tgt_tokenizer)\n",
        "print(tgt_sequence)\n",
        "print(pad_collate.tgt_tokenizer.decode(tgt_sequence[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVEp1SmrhVuf",
        "outputId": "4012c7d1-54b6-458e-f4a5-3e270be29821"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A man sings a song.\n",
            "Encoding(num_tokens=6, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "A man sings a song .\n",
            "src tokens tensor([[  30,   93, 1561,   57, 2705,   15]], device='cuda:0',\n",
            "       dtype=torch.int32)\n",
            "decoder input tensor([[0]], device='cuda:0', dtype=torch.int32)\n",
            "argmax prediction: tensor([[109]], device='cuda:0')\n",
            "actual prediction: Ein\n",
            "argmax prediction: tensor([[109, 124]], device='cuda:0')\n",
            "actual prediction: Ein Mann\n",
            "argmax prediction: tensor([[109, 124, 829]], device='cuda:0')\n",
            "actual prediction: Ein Mann singt\n",
            "argmax prediction: tensor([[109, 124, 829, 114]], device='cuda:0')\n",
            "actual prediction: Ein Mann singt und\n",
            "argmax prediction: tensor([[109, 124, 829, 114, 829]], device='cuda:0')\n",
            "actual prediction: Ein Mann singt und singt\n",
            "argmax prediction: tensor([[109, 124, 829, 114, 829, 103]], device='cuda:0')\n",
            "actual prediction: Ein Mann singt und singt ein\n",
            "argmax prediction: tensor([[ 109,  124,  829,  114,  829,  103, 3680]], device='cuda:0')\n",
            "actual prediction: Ein Mann singt und singt ein Lied\n",
            "argmax prediction: tensor([[ 109,  124,  829,  114,  829,  103, 3680,   14]], device='cuda:0')\n",
            "actual prediction: Ein Mann singt und singt ein Lied .\n",
            "argmax prediction: tensor([[ 109,  124,  829,  114,  829,  103, 3680,   14,    1]],\n",
            "       device='cuda:0')\n",
            "actual prediction: Ein Mann singt und singt ein Lied .\n",
            "tensor([[   0,  109,  124,  829,  114,  829,  103, 3680,   14,    1]],\n",
            "       device='cuda:0')\n",
            "Ein Mann singt und singt ein Lied .\n"
          ]
        }
      ]
    }
  ]
}