{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM1G0hY0iGmn6ZUXFdGlKVp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JackWittmayer/Transformer-Implementation/blob/main/EDTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37L0J9nog5yz",
        "outputId": "4d3c2fa2-beb3-4edc-e0b4-2a73964184d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VNf0Rr3ogy-4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import os\n",
        "import pickle\n",
        "from unicodedata import normalize\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import log_softmax, pad\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(25)\n",
        "random.seed(25)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "2Hs0kEZYlwc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c94ef30-70c3-466a-9a35-9fbccecfd670"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_X = torch.tensor([[3, 2, 0, 1], [1, 2, 3, 0]], dtype=torch.int32).to(device)\n",
        "SAMPLE_Z = torch.tensor([4, 1, 7, 6], dtype=torch.int32).to(device)"
      ],
      "metadata": {
        "id": "bMXiZFPpEtZa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printIfVerbose(verbose, tag, value):\n",
        "    if verbose:\n",
        "        print(tag, value)"
      ],
      "metadata": {
        "id": "6gsV7Wgv2DKh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        super().__init__()\n",
        "        self.table = nn.Embedding(vocab_size, embedding_size).to(device)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        embeddings = self.table(sequence)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "Pk7xtwam89Hh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_embedding():\n",
        "    torch.manual_seed(25)\n",
        "    vocab_size = 4\n",
        "    embedding = Embedding(vocab_size, 4)\n",
        "    print(\"weight:\", embedding.table.weight)\n",
        "    print(\"SAMPLE_X: \", SAMPLE_X)\n",
        "    output = embedding(SAMPLE_X)\n",
        "    print(\"output:\", output)\n",
        "    for j in range(len(output)):\n",
        "        #print(\"sample:\", sample)\n",
        "        for i in range(vocab_size):\n",
        "            assert output[j, i, :].eq(embedding.table.weight[SAMPLE_X[j, i]]).all()\n",
        "test_embedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StJF4qlIBYVj",
        "outputId": "0d596e11-d1b7-4ab5-8c79-98378d45c252"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight: Parameter containing:\n",
            "tensor([[ 0.0877, -0.6113,  0.3441, -1.2916],\n",
            "        [-0.5874,  0.8060,  1.3200,  0.4826],\n",
            "        [ 1.6671, -0.2342,  0.1074,  1.7852],\n",
            "        [ 0.7874, -0.2466,  0.2384, -0.6746]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "SAMPLE_X:  tensor([[3, 2, 0, 1],\n",
            "        [1, 2, 3, 0]], device='cuda:0', dtype=torch.int32)\n",
            "output: tensor([[[ 0.7874, -0.2466,  0.2384, -0.6746],\n",
            "         [ 1.6671, -0.2342,  0.1074,  1.7852],\n",
            "         [ 0.0877, -0.6113,  0.3441, -1.2916],\n",
            "         [-0.5874,  0.8060,  1.3200,  0.4826]],\n",
            "\n",
            "        [[-0.5874,  0.8060,  1.3200,  0.4826],\n",
            "         [ 1.6671, -0.2342,  0.1074,  1.7852],\n",
            "         [ 0.7874, -0.2466,  0.2384, -0.6746],\n",
            "         [ 0.0877, -0.6113,  0.3441, -1.2916]]], device='cuda:0',\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Unembedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Linear(embedding_size, vocab_size).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.weight(x)"
      ],
      "metadata": {
        "id": "EpoJIpc_CaX6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_unembedding():\n",
        "    torch.manual_seed(25)\n",
        "    vocab_size = 10\n",
        "    embedding_size = 4\n",
        "    sequence_length = 4\n",
        "    batch_size = 2\n",
        "    input = torch.rand(batch_size, sequence_length, embedding_size).to(device)\n",
        "    unembedding = Unembedding(vocab_size, embedding_size)\n",
        "\n",
        "    print(\"weight:\", unembedding.weight)\n",
        "    print(\"input: \", input)\n",
        "    output = unembedding(input)\n",
        "    print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, sequence_length, vocab_size)\n",
        "test_unembedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLZ0-F4xNOMf",
        "outputId": "f83116d1-523c-4981-f99c-f16d0142524a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight: Linear(in_features=4, out_features=10, bias=True)\n",
            "input:  tensor([[[0.7518, 0.1929, 0.0629, 0.9118],\n",
            "         [0.3828, 0.2990, 0.5933, 0.2911],\n",
            "         [0.2416, 0.5582, 0.0481, 0.3497],\n",
            "         [0.3520, 0.9528, 0.0284, 0.8488]],\n",
            "\n",
            "        [[0.3947, 0.5181, 0.9726, 0.8813],\n",
            "         [0.0056, 0.3056, 0.9384, 0.7949],\n",
            "         [0.4399, 0.1766, 0.8739, 0.1425],\n",
            "         [0.4682, 0.6254, 0.3040, 0.7923]]], device='cuda:0')\n",
            "output: tensor([[[-4.9334e-01,  3.9030e-01, -3.4348e-03,  2.0479e-01, -1.7155e-01,\n",
            "           3.0325e-01,  9.5298e-01, -9.2740e-01,  3.5209e-01, -2.1405e-02],\n",
            "         [-6.2972e-02, -6.9980e-02, -5.4304e-02,  1.2675e-01, -5.5075e-01,\n",
            "           1.8844e-01,  8.6408e-01, -5.4956e-01,  4.7789e-01,  7.8078e-02],\n",
            "         [-2.9110e-01,  3.9284e-02,  7.2926e-02,  2.0875e-01, -3.4683e-01,\n",
            "           1.1962e-01,  7.2445e-01, -5.6804e-01,  4.2547e-01,  1.0732e-02],\n",
            "         [-3.4147e-01,  6.4171e-02, -5.8167e-02,  1.8160e-01, -2.1651e-01,\n",
            "          -1.4458e-01,  1.0255e+00, -9.2857e-01,  5.6889e-01, -7.0657e-02]],\n",
            "\n",
            "        [[ 3.2716e-02, -1.6396e-01, -2.2999e-01,  1.5606e-01, -5.7223e-01,\n",
            "          -1.2983e-01,  1.2599e+00, -8.8230e-01,  6.9367e-01,  1.1626e-01],\n",
            "         [ 7.3857e-03, -2.2392e-01, -6.1284e-02,  3.6410e-01, -6.3714e-01,\n",
            "          -1.2969e-01,  1.0509e+00, -6.4156e-01,  6.5060e-01,  2.7214e-01],\n",
            "         [ 8.3425e-02, -1.5596e-01, -1.1466e-01,  6.0901e-02, -6.8152e-01,\n",
            "           2.4368e-01,  8.9560e-01, -4.8380e-01,  4.9643e-01,  1.0562e-01],\n",
            "         [-2.5832e-01,  7.1240e-02, -8.8499e-02,  1.6288e-01, -3.2179e-01,\n",
            "          -1.3083e-04,  1.0437e+00, -8.7687e-01,  5.3716e-01, -1.0231e-02]]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, embedding_size, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.table = nn.Embedding(max_sequence_length, embedding_size).to(device)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        positions = torch.zeros(sequence.shape, dtype=torch.int32)\n",
        "        positions[:, ::] = torch.arange(0, sequence.shape[-1])\n",
        "        #print(\"positions\", positions)\n",
        "        positional_embeddings = self.table(positions.to(device))\n",
        "        return positional_embeddings"
      ],
      "metadata": {
        "id": "5xpYM9Zf_KLw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_positional_embedding():\n",
        "    embedding_size = 8\n",
        "    max_sequence_length = 10\n",
        "    batch_size = 2\n",
        "    positional_embedding = PositionalEmbedding(embedding_size, max_sequence_length)\n",
        "    output = positional_embedding(SAMPLE_X)\n",
        "    print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, SAMPLE_X.shape[-1], embedding_size)\n",
        "test_positional_embedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l1Z_ZH8Pb_P",
        "outputId": "f45acaf3-b2f3-49c8-88fe-521d6785311a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output: tensor([[[ 0.3412, -0.2888, -0.4162, -1.2433,  0.3364, -2.1897, -0.2086,\n",
            "           0.0196],\n",
            "         [ 0.2461, -0.0812, -0.4464, -1.2595,  0.5963, -1.3647, -0.7684,\n",
            "           0.3472],\n",
            "         [-0.0142,  0.1426,  2.0701, -0.1623, -0.4448,  0.8318, -0.2930,\n",
            "          -0.2068],\n",
            "         [-0.8096,  1.2487,  0.5594, -0.3657,  0.5478, -1.4327, -1.4111,\n",
            "          -0.4237]],\n",
            "\n",
            "        [[ 0.3412, -0.2888, -0.4162, -1.2433,  0.3364, -2.1897, -0.2086,\n",
            "           0.0196],\n",
            "         [ 0.2461, -0.0812, -0.4464, -1.2595,  0.5963, -1.3647, -0.7684,\n",
            "           0.3472],\n",
            "         [-0.0142,  0.1426,  2.0701, -0.1623, -0.4448,  0.8318, -0.2930,\n",
            "          -0.2068],\n",
            "         [-0.8096,  1.2487,  0.5594, -0.3657,  0.5478, -1.4327, -1.4111,\n",
            "          -0.4237]]], device='cuda:0', grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(queries, keys, values, mask, dropout, verbose):\n",
        "    printIfVerbose(verbose, \"queries:\", queries)\n",
        "    printIfVerbose(verbose, \"keys:\", keys)\n",
        "    printIfVerbose(verbose, \"values:\", values)\n",
        "    keys_transposed = torch.transpose(keys, -2, -1)\n",
        "    printIfVerbose(verbose, \"keys_transposed:\", keys_transposed)\n",
        "    scores = torch.matmul(queries, keys_transposed)\n",
        "    #assert scores.shape == (keys.shape[0], keys.shape[-1], queries.shape[-1])\n",
        "    printIfVerbose(verbose, \"scores:\", scores)\n",
        "    printIfVerbose(verbose, \"scores:\", scores.shape)\n",
        "    printIfVerbose(verbose, \"masks:\", mask.shape)\n",
        "    scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    printIfVerbose(verbose, \"masked scores:\", scores)\n",
        "    d_attn = keys.shape[-1]\n",
        "    scaled_scores = scores / math.sqrt(d_attn)\n",
        "    printIfVerbose(verbose, \"scaled_scores:\", scaled_scores)\n",
        "    softmax_scores = torch.softmax(scaled_scores, -1)\n",
        "    softmax_scores = dropout(softmax_scores)\n",
        "    printIfVerbose(verbose, \"softmax_scores:\", softmax_scores)\n",
        "    printIfVerbose(verbose, \"softmax_socres shape:\", softmax_scores.shape)\n",
        "    printIfVerbose(verbose, \"values:\", values)\n",
        "    v_out = torch.matmul(softmax_scores, values)\n",
        "    return v_out"
      ],
      "metadata": {
        "id": "DL3t3E_ThGF2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_attention():\n",
        "    d_attn = 4\n",
        "    length_x = 4\n",
        "    length_z = 3\n",
        "    batch_size = 2\n",
        "    d_out = 2\n",
        "\n",
        "    queries = torch.rand(batch_size, length_x, d_attn)\n",
        "    keys = torch.rand(batch_size, length_z, d_attn)\n",
        "    values = torch.rand(batch_size, length_z, d_out)\n",
        "    mask = torch.tril(torch.ones(length_x, length_z) == 1)\n",
        "    padding_mask = torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0]], dtype=torch.int32)\n",
        "\n",
        "    v_out = attention(queries, keys, values, mask, nn.Dropout(0.1), True)\n",
        "    #print(\"output:\", v_out)\n",
        "    assert v_out.shape == (batch_size, length_x, d_out)\n",
        "test_attention()"
      ],
      "metadata": {
        "id": "KhGXLYp6i61z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f10ce257-ce28-48ed-ac4a-d635b67db0e2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "queries: tensor([[[0.4961, 0.6278, 0.3572, 0.5220],\n",
            "         [0.1997, 0.5286, 0.4723, 0.0238],\n",
            "         [0.1838, 0.2010, 0.1765, 0.8587],\n",
            "         [0.7776, 0.1199, 0.8638, 0.1066]],\n",
            "\n",
            "        [[0.1084, 0.8448, 0.7043, 0.9275],\n",
            "         [0.3953, 0.2704, 0.6228, 0.6078],\n",
            "         [0.7686, 0.3296, 0.4959, 0.0065],\n",
            "         [0.9125, 0.8358, 0.6698, 0.4129]]])\n",
            "keys: tensor([[[0.0129, 0.5052, 0.5967, 0.3134],\n",
            "         [0.1648, 0.4834, 0.2368, 0.7654],\n",
            "         [0.9255, 0.3393, 0.5612, 0.0953]],\n",
            "\n",
            "        [[0.5582, 0.5739, 0.5244, 0.6292],\n",
            "         [0.7426, 0.3134, 0.7793, 0.9385],\n",
            "         [0.1588, 0.3427, 0.3863, 0.2306]]])\n",
            "values: tensor([[[0.1533, 0.0876],\n",
            "         [0.9218, 0.8859],\n",
            "         [0.6448, 0.5202]],\n",
            "\n",
            "        [[0.3174, 0.8487],\n",
            "         [0.8658, 0.5804],\n",
            "         [0.1021, 0.1329]]])\n",
            "keys_transposed: tensor([[[0.0129, 0.1648, 0.9255],\n",
            "         [0.5052, 0.4834, 0.3393],\n",
            "         [0.5967, 0.2368, 0.5612],\n",
            "         [0.3134, 0.7654, 0.0953]],\n",
            "\n",
            "        [[0.5582, 0.7426, 0.1588],\n",
            "         [0.5739, 0.3134, 0.3427],\n",
            "         [0.5244, 0.7793, 0.3863],\n",
            "         [0.6292, 0.9385, 0.2306]]])\n",
            "scores: tensor([[[0.7003, 0.8693, 0.9223],\n",
            "         [0.5590, 0.4186, 0.6315],\n",
            "         [0.4784, 0.8265, 0.4192],\n",
            "         [0.6194, 0.4722, 1.2552]],\n",
            "\n",
            "        [[1.4984, 1.7646, 0.7927],\n",
            "         [1.0850, 1.4341, 0.5363],\n",
            "         [0.8825, 1.0667, 0.4282],\n",
            "         [1.6002, 1.8490, 0.7854]]])\n",
            "scores: torch.Size([2, 4, 3])\n",
            "masks: torch.Size([4, 3])\n",
            "masked scores: tensor([[[ 7.0027e-01, -1.0000e+09, -1.0000e+09],\n",
            "         [ 5.5897e-01,  4.1855e-01, -1.0000e+09],\n",
            "         [ 4.7836e-01,  8.2648e-01,  4.1922e-01],\n",
            "         [ 6.1941e-01,  4.7223e-01,  1.2552e+00]],\n",
            "\n",
            "        [[ 1.4984e+00, -1.0000e+09, -1.0000e+09],\n",
            "         [ 1.0850e+00,  1.4341e+00, -1.0000e+09],\n",
            "         [ 8.8246e-01,  1.0667e+00,  4.2816e-01],\n",
            "         [ 1.6002e+00,  1.8490e+00,  7.8537e-01]]])\n",
            "scaled_scores: tensor([[[ 3.5014e-01, -5.0000e+08, -5.0000e+08],\n",
            "         [ 2.7948e-01,  2.0928e-01, -5.0000e+08],\n",
            "         [ 2.3918e-01,  4.1324e-01,  2.0961e-01],\n",
            "         [ 3.0970e-01,  2.3612e-01,  6.2761e-01]],\n",
            "\n",
            "        [[ 7.4918e-01, -5.0000e+08, -5.0000e+08],\n",
            "         [ 5.4248e-01,  7.1705e-01, -5.0000e+08],\n",
            "         [ 4.4123e-01,  5.3335e-01,  2.1408e-01],\n",
            "         [ 8.0008e-01,  9.2452e-01,  3.9269e-01]]])\n",
            "softmax_scores: tensor([[[1.1111, 0.0000, 0.0000],\n",
            "         [0.5751, 0.5361, 0.0000],\n",
            "         [0.3515, 0.4183, 0.3413],\n",
            "         [0.3364, 0.3125, 0.0000]],\n",
            "\n",
            "        [[1.1111, 0.0000, 0.0000],\n",
            "         [0.5072, 0.6039, 0.0000],\n",
            "         [0.0000, 0.4211, 0.3060],\n",
            "         [0.0000, 0.4497, 0.2642]]])\n",
            "softmax_socres shape: torch.Size([2, 4, 3])\n",
            "values: tensor([[[0.1533, 0.0876],\n",
            "         [0.9218, 0.8859],\n",
            "         [0.6448, 0.5202]],\n",
            "\n",
            "        [[0.3174, 0.8487],\n",
            "         [0.8658, 0.5804],\n",
            "         [0.1021, 0.1329]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "class MaskStrategy(Enum):\n",
        "    UNMASKED = 1\n",
        "    MASKED = 2"
      ],
      "metadata": {
        "id": "wUJ-CbaCB9g-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, num_heads, d_attn, d_x, d_z, d_out, d_mid, maskStrategy, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.num_heads = num_heads\n",
        "        self.d_attn = d_attn\n",
        "        self.d_x = d_x\n",
        "        self.d_z = d_z\n",
        "        self.d_out = d_out\n",
        "        self.d_mid = d_mid\n",
        "        self.maskStrategy = maskStrategy\n",
        "        self.weight_query = nn.Linear(d_x, d_attn).to(device)\n",
        "        self.weight_key = nn.Linear(d_z, d_attn).to(device)\n",
        "        self.weight_value = nn.Linear(d_z, d_mid).to(device)\n",
        "        self.weight_out = nn.Linear(d_mid, d_out).to(device)\n",
        "        self.dropout = nn.Dropout(p_dropout)\n",
        "\n",
        "    def forward(self, z, x, padding_mask):\n",
        "        length_z = z.shape[-2]\n",
        "        length_x = x.shape[-2]\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        queries = self.weight_query(x).view(batch_size, length_x, self.num_heads, -1).transpose(1, 2)\n",
        "        keys = self.weight_key(z).view(batch_size, length_z, self.num_heads, -1).transpose(1, 2)\n",
        "        values = self.weight_value(z).view(batch_size, length_z, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        assert queries.shape == (batch_size, self.num_heads, length_x, self.d_attn / self.num_heads)\n",
        "        assert keys.shape == (batch_size, self.num_heads, length_z, self.d_attn / self.num_heads)\n",
        "        assert values.shape == (batch_size, self.num_heads, length_z, self.d_mid / self.num_heads)\n",
        "\n",
        "        if self.maskStrategy == MaskStrategy['UNMASKED']:\n",
        "            mask = padding_mask.unsqueeze(-2)\n",
        "        elif self.maskStrategy == MaskStrategy['MASKED']:\n",
        "            padding_mask = padding_mask.unsqueeze(-2)\n",
        "            mask = torch.tril(torch.ones(length_x, length_z) == 1).to(device)\n",
        "            printIfVerbose(self.verbose, \"padding mask:\", padding_mask.shape)\n",
        "            printIfVerbose(self.verbose, \"mask tril\", mask)\n",
        "            mask = mask & padding_mask\n",
        "            printIfVerbose(self.verbose, \"merged mask:\", mask)\n",
        "        mask = mask.unsqueeze(1)\n",
        "        printIfVerbose(self.verbose, \"mask\", mask)\n",
        "        printIfVerbose(self.verbose, \"mask\", mask.shape)\n",
        "        v_out = attention(queries, keys, values, mask, self.dropout, self.verbose)\n",
        "        printIfVerbose(self.verbose, \"v_out shape\", v_out.shape)\n",
        "        assert v_out.shape == (batch_size, self.num_heads, length_x, self.d_mid / self.num_heads)\n",
        "        printIfVerbose(self.verbose, \"v_out:\", v_out)\n",
        "        printIfVerbose(self.verbose, \"v_out shape before:\", v_out.shape)\n",
        "        v_out = v_out.transpose(1, 2).reshape(batch_size, length_x, -1)\n",
        "        printIfVerbose(self.verbose, \"v_out shape:\", v_out.shape)\n",
        "        printIfVerbose(self.verbose, \"v_out reshaped:\", v_out)\n",
        "        output = self.weight_out(v_out)\n",
        "        printIfVerbose(self.verbose, \"output shape\", output.shape)\n",
        "        assert output.shape == (batch_size, length_x, self.d_out)\n",
        "        return output\n",
        "\n",
        "    def disable_subsequent_mask(self):\n",
        "        self.maskStrategy = MaskStrategy['UNMASKED']\n",
        "\n",
        "    def enable_subsequent_mask(self):\n",
        "        self.maskStrategy = MaskStrategy['MASKED']\n"
      ],
      "metadata": {
        "id": "CX2A1i9Z4lVO"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_headed_attention_encoder_fixed():\n",
        "    num_heads = 1\n",
        "    d_attn = 4\n",
        "    d_x = 4\n",
        "    d_z = 4\n",
        "    d_out = 1\n",
        "    d_mid = 3\n",
        "    length_z = 3\n",
        "    batch_size = 1\n",
        "    padding_mask = torch.tensor([[1, 1, 0]], dtype=torch.int32).to(device)\n",
        "\n",
        "    multi_headed_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], 0.0, True).to(device)\n",
        "    z = torch.tensor([[[1, 0, 1, 0], [0, 2, 0, 2], [1, 1, 1, 1]]], dtype=torch.float32).to(device)\n",
        "    #print(\"z:\", z\n",
        "    output = multi_headed_attention(z, z, padding_mask)\n",
        "    #print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, length_z, d_out)\n",
        "test_multi_headed_attention_encoder_fixed()"
      ],
      "metadata": {
        "id": "kJuRy-3dTMY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e7b26d0-77e4-432b-d970-c23a4cb5ca93"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "queries torch.Size([1, 3, 4])\n",
            "torch.Size([1, 1, 3, 4])\n",
            "torch.Size([1, 1, 3, 4])\n",
            "torch.Size([1, 1, 3, 3])\n",
            "mask tensor([[[[1, 1, 0]]]], device='cuda:0', dtype=torch.int32)\n",
            "mask torch.Size([1, 1, 1, 3])\n",
            "queries: tensor([[[[ 0.1323,  0.5921, -0.6788, -0.2635],\n",
            "          [ 1.6504,  0.6980, -1.5903,  1.1315],\n",
            "          [ 0.9161,  0.6927, -1.4619,  0.5480]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "keys: tensor([[[[ 0.5118,  0.5339, -0.4241, -0.7160],\n",
            "          [ 1.7491,  0.4337, -0.8936, -0.6943],\n",
            "          [ 1.2685,  0.5772, -0.8389, -0.9327]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "values: tensor([[[[-0.4436,  0.3099, -0.4846],\n",
            "          [-0.3242,  0.6657,  0.2512],\n",
            "          [-0.6543,  0.4318, -0.4064]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "keys_transposed: tensor([[[[ 0.5118,  1.7491,  1.2685],\n",
            "          [ 0.5339,  0.4337,  0.5772],\n",
            "          [-0.4241, -0.8936, -0.8389],\n",
            "          [-0.7160, -0.6943, -0.9327]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "scores: tensor([[[[0.8604, 1.2778, 1.3249],\n",
            "          [1.0817, 3.8250, 2.7751],\n",
            "          [1.0663, 2.8287, 2.2772]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "scores: torch.Size([1, 1, 3, 3])\n",
            "masks: torch.Size([1, 1, 1, 3])\n",
            "masked scores: tensor([[[[ 8.6039e-01,  1.2778e+00, -1.0000e+09],\n",
            "          [ 1.0817e+00,  3.8250e+00, -1.0000e+09],\n",
            "          [ 1.0663e+00,  2.8287e+00, -1.0000e+09]]]], device='cuda:0',\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "scaled_scores: tensor([[[[ 4.3019e-01,  6.3890e-01, -5.0000e+08],\n",
            "          [ 5.4084e-01,  1.9125e+00, -5.0000e+08],\n",
            "          [ 5.3317e-01,  1.4144e+00, -5.0000e+08]]]], device='cuda:0',\n",
            "       grad_fn=<DivBackward0>)\n",
            "softmax_scores: tensor([[[[0.4480, 0.5520, 0.0000],\n",
            "          [0.2024, 0.7976, 0.0000],\n",
            "          [0.2929, 0.7071, 0.0000]]]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "softmax_socres shape: torch.Size([1, 1, 3, 3])\n",
            "values: tensor([[[[-0.4436,  0.3099, -0.4846],\n",
            "          [-0.3242,  0.6657,  0.2512],\n",
            "          [-0.6543,  0.4318, -0.4064]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "v_out shape torch.Size([1, 1, 3, 3])\n",
            "v_out: tensor([[[[-0.3777,  0.5063, -0.0785],\n",
            "          [-0.3484,  0.5937,  0.1023],\n",
            "          [-0.3592,  0.5615,  0.0356]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "v_out shape before: torch.Size([1, 1, 3, 3])\n",
            "v_out shape: torch.Size([1, 3, 3])\n",
            "v_out reshaped: tensor([[[-0.3777,  0.5063, -0.0785],\n",
            "         [-0.3484,  0.5937,  0.1023],\n",
            "         [-0.3592,  0.5615,  0.0356]]], device='cuda:0',\n",
            "       grad_fn=<ViewBackward0>)\n",
            "output shape torch.Size([1, 3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_headed_attention_encoder():\n",
        "    num_heads = 4\n",
        "    d_attn = 4\n",
        "    d_x = 4\n",
        "    d_z = 4\n",
        "    d_out = 1\n",
        "    d_mid = 4\n",
        "    length_z = 3\n",
        "    batch_size = 3\n",
        "    padding_mask = torch.tensor([[1, 1, 0], [1, 1, 0], [1, 1, 1]], dtype=torch.int32).to(device)\n",
        "\n",
        "    multi_headed_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], 0.0, True).to(device)\n",
        "    z = torch.tensor([[[1, 0, 1, 0], [0, 2, 0, 2], [1, 1, 1, 1]],\n",
        "                      [[1, 0, 1, 0], [0, 2, 0, 2], [1, 1, 1, 1]],\n",
        "                      [[1, 0, 1, 0], [0, 2, 0, 2], [1, 1, 1, 1]]], dtype=torch.float32).to(device)\n",
        "    #print(\"z:\", z\n",
        "    output = multi_headed_attention(z, z, padding_mask)\n",
        "    #print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, length_z, d_out)\n",
        "test_multi_headed_attention_encoder()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9CYHYJbMObt",
        "outputId": "13d05b80-ed07-4dc6-d88a-93cf3d85e8c5"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "queries torch.Size([3, 3, 4])\n",
            "torch.Size([3, 4, 3, 1])\n",
            "torch.Size([3, 4, 3, 1])\n",
            "torch.Size([3, 4, 3, 1])\n",
            "mask tensor([[[[1, 1, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 1, 1]]]], device='cuda:0', dtype=torch.int32)\n",
            "mask torch.Size([3, 1, 1, 3])\n",
            "queries: tensor([[[[-0.3482],\n",
            "          [-1.1754],\n",
            "          [-0.7185]],\n",
            "\n",
            "         [[ 0.3963],\n",
            "          [ 0.4933],\n",
            "          [ 0.4069]],\n",
            "\n",
            "         [[ 0.4984],\n",
            "          [-0.5069],\n",
            "          [ 0.2341]],\n",
            "\n",
            "         [[ 0.5366],\n",
            "          [ 0.7522],\n",
            "          [ 0.7844]]],\n",
            "\n",
            "\n",
            "        [[[-0.3482],\n",
            "          [-1.1754],\n",
            "          [-0.7185]],\n",
            "\n",
            "         [[ 0.3963],\n",
            "          [ 0.4933],\n",
            "          [ 0.4069]],\n",
            "\n",
            "         [[ 0.4984],\n",
            "          [-0.5069],\n",
            "          [ 0.2341]],\n",
            "\n",
            "         [[ 0.5366],\n",
            "          [ 0.7522],\n",
            "          [ 0.7844]]],\n",
            "\n",
            "\n",
            "        [[[-0.3482],\n",
            "          [-1.1754],\n",
            "          [-0.7185]],\n",
            "\n",
            "         [[ 0.3963],\n",
            "          [ 0.4933],\n",
            "          [ 0.4069]],\n",
            "\n",
            "         [[ 0.4984],\n",
            "          [-0.5069],\n",
            "          [ 0.2341]],\n",
            "\n",
            "         [[ 0.5366],\n",
            "          [ 0.7522],\n",
            "          [ 0.7844]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "keys: tensor([[[[ 0.0478],\n",
            "          [-0.2717],\n",
            "          [-0.1440]],\n",
            "\n",
            "         [[-0.6246],\n",
            "          [ 1.2480],\n",
            "          [-0.0473]],\n",
            "\n",
            "         [[ 0.2522],\n",
            "          [-0.5438],\n",
            "          [-0.1306]],\n",
            "\n",
            "         [[-0.8426],\n",
            "          [ 0.6068],\n",
            "          [-0.4385]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0478],\n",
            "          [-0.2717],\n",
            "          [-0.1440]],\n",
            "\n",
            "         [[-0.6246],\n",
            "          [ 1.2480],\n",
            "          [-0.0473]],\n",
            "\n",
            "         [[ 0.2522],\n",
            "          [-0.5438],\n",
            "          [-0.1306]],\n",
            "\n",
            "         [[-0.8426],\n",
            "          [ 0.6068],\n",
            "          [-0.4385]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0478],\n",
            "          [-0.2717],\n",
            "          [-0.1440]],\n",
            "\n",
            "         [[-0.6246],\n",
            "          [ 1.2480],\n",
            "          [-0.0473]],\n",
            "\n",
            "         [[ 0.2522],\n",
            "          [-0.5438],\n",
            "          [-0.1306]],\n",
            "\n",
            "         [[-0.8426],\n",
            "          [ 0.6068],\n",
            "          [-0.4385]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "values: tensor([[[[ 0.2765],\n",
            "          [-0.0787],\n",
            "          [ 0.3918]],\n",
            "\n",
            "         [[ 0.7885],\n",
            "          [-0.5177],\n",
            "          [ 0.3691]],\n",
            "\n",
            "         [[-0.9013],\n",
            "          [ 0.4129],\n",
            "          [-0.5904]],\n",
            "\n",
            "         [[-0.5238],\n",
            "          [-1.3185],\n",
            "          [-1.2697]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2765],\n",
            "          [-0.0787],\n",
            "          [ 0.3918]],\n",
            "\n",
            "         [[ 0.7885],\n",
            "          [-0.5177],\n",
            "          [ 0.3691]],\n",
            "\n",
            "         [[-0.9013],\n",
            "          [ 0.4129],\n",
            "          [-0.5904]],\n",
            "\n",
            "         [[-0.5238],\n",
            "          [-1.3185],\n",
            "          [-1.2697]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2765],\n",
            "          [-0.0787],\n",
            "          [ 0.3918]],\n",
            "\n",
            "         [[ 0.7885],\n",
            "          [-0.5177],\n",
            "          [ 0.3691]],\n",
            "\n",
            "         [[-0.9013],\n",
            "          [ 0.4129],\n",
            "          [-0.5904]],\n",
            "\n",
            "         [[-0.5238],\n",
            "          [-1.3185],\n",
            "          [-1.2697]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "keys_transposed: tensor([[[[ 0.0478, -0.2717, -0.1440]],\n",
            "\n",
            "         [[-0.6246,  1.2480, -0.0473]],\n",
            "\n",
            "         [[ 0.2522, -0.5438, -0.1306]],\n",
            "\n",
            "         [[-0.8426,  0.6068, -0.4385]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0478, -0.2717, -0.1440]],\n",
            "\n",
            "         [[-0.6246,  1.2480, -0.0473]],\n",
            "\n",
            "         [[ 0.2522, -0.5438, -0.1306]],\n",
            "\n",
            "         [[-0.8426,  0.6068, -0.4385]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0478, -0.2717, -0.1440]],\n",
            "\n",
            "         [[-0.6246,  1.2480, -0.0473]],\n",
            "\n",
            "         [[ 0.2522, -0.5438, -0.1306]],\n",
            "\n",
            "         [[-0.8426,  0.6068, -0.4385]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "scores: tensor([[[[-0.0166,  0.0946,  0.0501],\n",
            "          [-0.0562,  0.3193,  0.1693],\n",
            "          [-0.0343,  0.1952,  0.1035]],\n",
            "\n",
            "         [[-0.2475,  0.4946, -0.0188],\n",
            "          [-0.3081,  0.6157, -0.0233],\n",
            "          [-0.2541,  0.5078, -0.0193]],\n",
            "\n",
            "         [[ 0.1257, -0.2710, -0.0651],\n",
            "          [-0.1278,  0.2757,  0.0662],\n",
            "          [ 0.0590, -0.1273, -0.0306]],\n",
            "\n",
            "         [[-0.4521,  0.3256, -0.2353],\n",
            "          [-0.6338,  0.4565, -0.3298],\n",
            "          [-0.6610,  0.4760, -0.3440]]],\n",
            "\n",
            "\n",
            "        [[[-0.0166,  0.0946,  0.0501],\n",
            "          [-0.0562,  0.3193,  0.1693],\n",
            "          [-0.0343,  0.1952,  0.1035]],\n",
            "\n",
            "         [[-0.2475,  0.4946, -0.0188],\n",
            "          [-0.3081,  0.6157, -0.0233],\n",
            "          [-0.2541,  0.5078, -0.0193]],\n",
            "\n",
            "         [[ 0.1257, -0.2710, -0.0651],\n",
            "          [-0.1278,  0.2757,  0.0662],\n",
            "          [ 0.0590, -0.1273, -0.0306]],\n",
            "\n",
            "         [[-0.4521,  0.3256, -0.2353],\n",
            "          [-0.6338,  0.4565, -0.3298],\n",
            "          [-0.6610,  0.4760, -0.3440]]],\n",
            "\n",
            "\n",
            "        [[[-0.0166,  0.0946,  0.0501],\n",
            "          [-0.0562,  0.3193,  0.1693],\n",
            "          [-0.0343,  0.1952,  0.1035]],\n",
            "\n",
            "         [[-0.2475,  0.4946, -0.0188],\n",
            "          [-0.3081,  0.6157, -0.0233],\n",
            "          [-0.2541,  0.5078, -0.0193]],\n",
            "\n",
            "         [[ 0.1257, -0.2710, -0.0651],\n",
            "          [-0.1278,  0.2757,  0.0662],\n",
            "          [ 0.0590, -0.1273, -0.0306]],\n",
            "\n",
            "         [[-0.4521,  0.3256, -0.2353],\n",
            "          [-0.6338,  0.4565, -0.3298],\n",
            "          [-0.6610,  0.4760, -0.3440]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "scores: torch.Size([3, 4, 3, 3])\n",
            "masks: torch.Size([3, 1, 1, 3])\n",
            "masked scores: tensor([[[[-1.6638e-02,  9.4593e-02, -1.0000e+09],\n",
            "          [-5.6166e-02,  3.1933e-01, -1.0000e+09],\n",
            "          [-3.4332e-02,  1.9519e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-2.4754e-01,  4.9461e-01, -1.0000e+09],\n",
            "          [-3.0812e-01,  6.1567e-01, -1.0000e+09],\n",
            "          [-2.5413e-01,  5.0777e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.2568e-01, -2.7103e-01, -1.0000e+09],\n",
            "          [-1.2782e-01,  2.7567e-01, -1.0000e+09],\n",
            "          [ 5.9047e-02, -1.2734e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-4.5211e-01,  3.2560e-01, -1.0000e+09],\n",
            "          [-6.3380e-01,  4.5646e-01, -1.0000e+09],\n",
            "          [-6.6097e-01,  4.7602e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.6638e-02,  9.4593e-02, -1.0000e+09],\n",
            "          [-5.6166e-02,  3.1933e-01, -1.0000e+09],\n",
            "          [-3.4332e-02,  1.9519e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-2.4754e-01,  4.9461e-01, -1.0000e+09],\n",
            "          [-3.0812e-01,  6.1567e-01, -1.0000e+09],\n",
            "          [-2.5413e-01,  5.0777e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.2568e-01, -2.7103e-01, -1.0000e+09],\n",
            "          [-1.2782e-01,  2.7567e-01, -1.0000e+09],\n",
            "          [ 5.9047e-02, -1.2734e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-4.5211e-01,  3.2560e-01, -1.0000e+09],\n",
            "          [-6.3380e-01,  4.5646e-01, -1.0000e+09],\n",
            "          [-6.6097e-01,  4.7602e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.6638e-02,  9.4593e-02,  5.0139e-02],\n",
            "          [-5.6166e-02,  3.1933e-01,  1.6926e-01],\n",
            "          [-3.4332e-02,  1.9519e-01,  1.0346e-01]],\n",
            "\n",
            "         [[-2.4754e-01,  4.9461e-01, -1.8754e-02],\n",
            "          [-3.0812e-01,  6.1567e-01, -2.3344e-02],\n",
            "          [-2.5413e-01,  5.0777e-01, -1.9253e-02]],\n",
            "\n",
            "         [[ 1.2568e-01, -2.7103e-01, -6.5065e-02],\n",
            "          [-1.2782e-01,  2.7567e-01,  6.6177e-02],\n",
            "          [ 5.9047e-02, -1.2734e-01, -3.0570e-02]],\n",
            "\n",
            "         [[-4.5211e-01,  3.2560e-01, -2.3529e-01],\n",
            "          [-6.3380e-01,  4.5646e-01, -3.2984e-01],\n",
            "          [-6.6097e-01,  4.7602e-01, -3.4398e-01]]]], device='cuda:0',\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "scaled_scores: tensor([[[[-1.6638e-02,  9.4593e-02, -1.0000e+09],\n",
            "          [-5.6166e-02,  3.1933e-01, -1.0000e+09],\n",
            "          [-3.4332e-02,  1.9519e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-2.4754e-01,  4.9461e-01, -1.0000e+09],\n",
            "          [-3.0812e-01,  6.1567e-01, -1.0000e+09],\n",
            "          [-2.5413e-01,  5.0777e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.2568e-01, -2.7103e-01, -1.0000e+09],\n",
            "          [-1.2782e-01,  2.7567e-01, -1.0000e+09],\n",
            "          [ 5.9047e-02, -1.2734e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-4.5211e-01,  3.2560e-01, -1.0000e+09],\n",
            "          [-6.3380e-01,  4.5646e-01, -1.0000e+09],\n",
            "          [-6.6097e-01,  4.7602e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.6638e-02,  9.4593e-02, -1.0000e+09],\n",
            "          [-5.6166e-02,  3.1933e-01, -1.0000e+09],\n",
            "          [-3.4332e-02,  1.9519e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-2.4754e-01,  4.9461e-01, -1.0000e+09],\n",
            "          [-3.0812e-01,  6.1567e-01, -1.0000e+09],\n",
            "          [-2.5413e-01,  5.0777e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.2568e-01, -2.7103e-01, -1.0000e+09],\n",
            "          [-1.2782e-01,  2.7567e-01, -1.0000e+09],\n",
            "          [ 5.9047e-02, -1.2734e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-4.5211e-01,  3.2560e-01, -1.0000e+09],\n",
            "          [-6.3380e-01,  4.5646e-01, -1.0000e+09],\n",
            "          [-6.6097e-01,  4.7602e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.6638e-02,  9.4593e-02,  5.0139e-02],\n",
            "          [-5.6166e-02,  3.1933e-01,  1.6926e-01],\n",
            "          [-3.4332e-02,  1.9519e-01,  1.0346e-01]],\n",
            "\n",
            "         [[-2.4754e-01,  4.9461e-01, -1.8754e-02],\n",
            "          [-3.0812e-01,  6.1567e-01, -2.3344e-02],\n",
            "          [-2.5413e-01,  5.0777e-01, -1.9253e-02]],\n",
            "\n",
            "         [[ 1.2568e-01, -2.7103e-01, -6.5065e-02],\n",
            "          [-1.2782e-01,  2.7567e-01,  6.6177e-02],\n",
            "          [ 5.9047e-02, -1.2734e-01, -3.0570e-02]],\n",
            "\n",
            "         [[-4.5211e-01,  3.2560e-01, -2.3529e-01],\n",
            "          [-6.3380e-01,  4.5646e-01, -3.2984e-01],\n",
            "          [-6.6097e-01,  4.7602e-01, -3.4398e-01]]]], device='cuda:0',\n",
            "       grad_fn=<DivBackward0>)\n",
            "softmax_scores: tensor([[[[0.4722, 0.5278, 0.0000],\n",
            "          [0.4072, 0.5928, 0.0000],\n",
            "          [0.4429, 0.5571, 0.0000]],\n",
            "\n",
            "         [[0.3225, 0.6775, 0.0000],\n",
            "          [0.2842, 0.7158, 0.0000],\n",
            "          [0.3182, 0.6818, 0.0000]],\n",
            "\n",
            "         [[0.5979, 0.4021, 0.0000],\n",
            "          [0.4005, 0.5995, 0.0000],\n",
            "          [0.5465, 0.4535, 0.0000]],\n",
            "\n",
            "         [[0.3148, 0.6852, 0.0000],\n",
            "          [0.2516, 0.7484, 0.0000],\n",
            "          [0.2429, 0.7571, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.4722, 0.5278, 0.0000],\n",
            "          [0.4072, 0.5928, 0.0000],\n",
            "          [0.4429, 0.5571, 0.0000]],\n",
            "\n",
            "         [[0.3225, 0.6775, 0.0000],\n",
            "          [0.2842, 0.7158, 0.0000],\n",
            "          [0.3182, 0.6818, 0.0000]],\n",
            "\n",
            "         [[0.5979, 0.4021, 0.0000],\n",
            "          [0.4005, 0.5995, 0.0000],\n",
            "          [0.5465, 0.4535, 0.0000]],\n",
            "\n",
            "         [[0.3148, 0.6852, 0.0000],\n",
            "          [0.2516, 0.7484, 0.0000],\n",
            "          [0.2429, 0.7571, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.3138, 0.3507, 0.3355],\n",
            "          [0.2696, 0.3925, 0.3378],\n",
            "          [0.2936, 0.3694, 0.3370]],\n",
            "\n",
            "         [[0.2295, 0.4820, 0.2885],\n",
            "          [0.2063, 0.5195, 0.2742],\n",
            "          [0.2269, 0.4861, 0.2870]],\n",
            "\n",
            "         [[0.4002, 0.2691, 0.3307],\n",
            "          [0.2695, 0.4034, 0.3271],\n",
            "          [0.3644, 0.3024, 0.3332]],\n",
            "\n",
            "         [[0.2263, 0.4926, 0.2811],\n",
            "          [0.1876, 0.5581, 0.2542],\n",
            "          [0.1821, 0.5678, 0.2501]]]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "softmax_socres shape: torch.Size([3, 4, 3, 3])\n",
            "values: tensor([[[[ 0.2765],\n",
            "          [-0.0787],\n",
            "          [ 0.3918]],\n",
            "\n",
            "         [[ 0.7885],\n",
            "          [-0.5177],\n",
            "          [ 0.3691]],\n",
            "\n",
            "         [[-0.9013],\n",
            "          [ 0.4129],\n",
            "          [-0.5904]],\n",
            "\n",
            "         [[-0.5238],\n",
            "          [-1.3185],\n",
            "          [-1.2697]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2765],\n",
            "          [-0.0787],\n",
            "          [ 0.3918]],\n",
            "\n",
            "         [[ 0.7885],\n",
            "          [-0.5177],\n",
            "          [ 0.3691]],\n",
            "\n",
            "         [[-0.9013],\n",
            "          [ 0.4129],\n",
            "          [-0.5904]],\n",
            "\n",
            "         [[-0.5238],\n",
            "          [-1.3185],\n",
            "          [-1.2697]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2765],\n",
            "          [-0.0787],\n",
            "          [ 0.3918]],\n",
            "\n",
            "         [[ 0.7885],\n",
            "          [-0.5177],\n",
            "          [ 0.3691]],\n",
            "\n",
            "         [[-0.9013],\n",
            "          [ 0.4129],\n",
            "          [-0.5904]],\n",
            "\n",
            "         [[-0.5238],\n",
            "          [-1.3185],\n",
            "          [-1.2697]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "v_out shape torch.Size([3, 4, 3, 1])\n",
            "v_out: tensor([[[[ 0.0890],\n",
            "          [ 0.0659],\n",
            "          [ 0.0786]],\n",
            "\n",
            "         [[-0.0964],\n",
            "          [-0.1465],\n",
            "          [-0.1020]],\n",
            "\n",
            "         [[-0.3729],\n",
            "          [-0.1134],\n",
            "          [-0.3053]],\n",
            "\n",
            "         [[-1.0683],\n",
            "          [-1.1185],\n",
            "          [-1.1255]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0890],\n",
            "          [ 0.0659],\n",
            "          [ 0.0786]],\n",
            "\n",
            "         [[-0.0964],\n",
            "          [-0.1465],\n",
            "          [-0.1020]],\n",
            "\n",
            "         [[-0.3729],\n",
            "          [-0.1134],\n",
            "          [-0.3053]],\n",
            "\n",
            "         [[-1.0683],\n",
            "          [-1.1185],\n",
            "          [-1.1255]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1906],\n",
            "          [ 0.1760],\n",
            "          [ 0.1841]],\n",
            "\n",
            "         [[ 0.0379],\n",
            "          [-0.0051],\n",
            "          [ 0.0332]],\n",
            "\n",
            "         [[-0.4448],\n",
            "          [-0.2694],\n",
            "          [-0.4003]],\n",
            "\n",
            "         [[-1.1249],\n",
            "          [-1.1570],\n",
            "          [-1.1615]]]], device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
            "v_out shape before: torch.Size([3, 4, 3, 1])\n",
            "v_out shape: torch.Size([3, 3, 4])\n",
            "v_out reshaped: tensor([[[ 0.0890, -0.0964, -0.3729, -1.0683],\n",
            "         [ 0.0659, -0.1465, -0.1134, -1.1185],\n",
            "         [ 0.0786, -0.1020, -0.3053, -1.1255]],\n",
            "\n",
            "        [[ 0.0890, -0.0964, -0.3729, -1.0683],\n",
            "         [ 0.0659, -0.1465, -0.1134, -1.1185],\n",
            "         [ 0.0786, -0.1020, -0.3053, -1.1255]],\n",
            "\n",
            "        [[ 0.1906,  0.0379, -0.4448, -1.1249],\n",
            "         [ 0.1760, -0.0051, -0.2694, -1.1570],\n",
            "         [ 0.1841,  0.0332, -0.4003, -1.1615]]], device='cuda:0',\n",
            "       grad_fn=<ReshapeAliasBackward0>)\n",
            "output shape torch.Size([3, 3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_headed_attention_encoder_decoder():\n",
        "    num_heads = 4\n",
        "    d_attn = 4\n",
        "    d_x = 4\n",
        "    d_z = 4\n",
        "    d_out = 4\n",
        "    d_mid = 4\n",
        "    length_x = 3\n",
        "    length_z = 3\n",
        "    batch_size = 4\n",
        "    padding_mask = torch.tensor([[1, 1, 0]], dtype=torch.int32).to(device)\n",
        "\n",
        "    multi_headed_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], 0.0, True).to(device)\n",
        "    x = torch.rand(batch_size, length_x, d_x).to(device)\n",
        "    z = torch.rand(batch_size, length_z, d_z).to(device)\n",
        "    output = multi_headed_attention(z, x, padding_mask)\n",
        "    print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, length_x, d_out)\n",
        "test_multi_headed_attention_encoder_decoder()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbYma85ffEHr",
        "outputId": "c733e2c4-f56b-409f-e029-2959f7bbcba3"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "queries torch.Size([4, 3, 4])\n",
            "torch.Size([4, 4, 3, 1])\n",
            "torch.Size([4, 4, 3, 1])\n",
            "torch.Size([4, 4, 3, 1])\n",
            "mask tensor([[[[1, 1, 0]]]], device='cuda:0', dtype=torch.int32)\n",
            "mask torch.Size([1, 1, 1, 3])\n",
            "queries: tensor([[[[-0.2298],\n",
            "          [-0.3926],\n",
            "          [-0.4845]],\n",
            "\n",
            "         [[-0.2591],\n",
            "          [-0.4324],\n",
            "          [ 0.0713]],\n",
            "\n",
            "         [[-0.1487],\n",
            "          [-0.0835],\n",
            "          [-0.1806]],\n",
            "\n",
            "         [[ 0.7135],\n",
            "          [ 0.8557],\n",
            "          [ 0.4950]]],\n",
            "\n",
            "\n",
            "        [[[-0.3888],\n",
            "          [-0.3381],\n",
            "          [-0.4572]],\n",
            "\n",
            "         [[-0.2430],\n",
            "          [-0.1997],\n",
            "          [-0.4964]],\n",
            "\n",
            "         [[-0.1004],\n",
            "          [-0.1711],\n",
            "          [-0.4900]],\n",
            "\n",
            "         [[ 1.0472],\n",
            "          [ 1.0148],\n",
            "          [ 0.7370]]],\n",
            "\n",
            "\n",
            "        [[[-0.2425],\n",
            "          [-0.4885],\n",
            "          [-0.2626]],\n",
            "\n",
            "         [[ 0.1059],\n",
            "          [-0.6467],\n",
            "          [-0.0156]],\n",
            "\n",
            "         [[-0.0546],\n",
            "          [-0.2998],\n",
            "          [-0.1688]],\n",
            "\n",
            "         [[ 0.4608],\n",
            "          [ 1.0953],\n",
            "          [ 0.8941]]],\n",
            "\n",
            "\n",
            "        [[[-0.6503],\n",
            "          [-0.5625],\n",
            "          [-0.4481]],\n",
            "\n",
            "         [[-0.2099],\n",
            "          [-0.0177],\n",
            "          [-0.3038]],\n",
            "\n",
            "         [[-0.3177],\n",
            "          [ 0.2686],\n",
            "          [-0.1285]],\n",
            "\n",
            "         [[ 0.6978],\n",
            "          [ 0.6362],\n",
            "          [ 1.1104]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "keys: tensor([[[[ 0.4506],\n",
            "          [ 0.4500],\n",
            "          [ 0.2348]],\n",
            "\n",
            "         [[ 0.0562],\n",
            "          [ 0.1279],\n",
            "          [-0.0156]],\n",
            "\n",
            "         [[ 0.4304],\n",
            "          [ 0.4279],\n",
            "          [ 0.3336]],\n",
            "\n",
            "         [[ 0.1536],\n",
            "          [ 0.1609],\n",
            "          [ 0.2983]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4718],\n",
            "          [ 0.5589],\n",
            "          [ 0.0988]],\n",
            "\n",
            "         [[ 0.0798],\n",
            "          [ 0.2457],\n",
            "          [-0.0504]],\n",
            "\n",
            "         [[ 0.5783],\n",
            "          [ 0.4936],\n",
            "          [ 0.1449]],\n",
            "\n",
            "         [[ 0.0684],\n",
            "          [ 0.3631],\n",
            "          [ 0.3616]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5788],\n",
            "          [ 0.2732],\n",
            "          [ 0.1826]],\n",
            "\n",
            "         [[ 0.3581],\n",
            "          [-0.3379],\n",
            "          [-0.1399]],\n",
            "\n",
            "         [[ 0.4959],\n",
            "          [ 0.4120],\n",
            "          [ 0.5550]],\n",
            "\n",
            "         [[ 0.3287],\n",
            "          [ 0.3160],\n",
            "          [ 0.3188]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4993],\n",
            "          [ 0.3656],\n",
            "          [ 0.7465]],\n",
            "\n",
            "         [[-0.1842],\n",
            "          [-0.0474],\n",
            "          [ 0.0466]],\n",
            "\n",
            "         [[ 0.6092],\n",
            "          [ 0.4571],\n",
            "          [ 0.8577]],\n",
            "\n",
            "         [[ 0.5535],\n",
            "          [ 0.0515],\n",
            "          [ 0.3549]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "values: tensor([[[[-0.1217],\n",
            "          [-0.1759],\n",
            "          [ 0.0253]],\n",
            "\n",
            "         [[ 0.3473],\n",
            "          [ 0.3339],\n",
            "          [ 0.4987]],\n",
            "\n",
            "         [[-0.0670],\n",
            "          [-0.1457],\n",
            "          [-0.2021]],\n",
            "\n",
            "         [[-0.2718],\n",
            "          [-0.3290],\n",
            "          [-0.3617]]],\n",
            "\n",
            "\n",
            "        [[[-0.0712],\n",
            "          [-0.1591],\n",
            "          [-0.0101]],\n",
            "\n",
            "         [[ 0.2086],\n",
            "          [ 0.5606],\n",
            "          [ 0.5872]],\n",
            "\n",
            "         [[-0.1090],\n",
            "          [-0.3236],\n",
            "          [-0.2122]],\n",
            "\n",
            "         [[-0.3836],\n",
            "          [-0.5281],\n",
            "          [-0.2712]]],\n",
            "\n",
            "\n",
            "        [[[-0.2719],\n",
            "          [ 0.3345],\n",
            "          [ 0.3393]],\n",
            "\n",
            "         [[ 0.4892],\n",
            "          [ 0.6161],\n",
            "          [ 0.4939]],\n",
            "\n",
            "         [[-0.4076],\n",
            "          [ 0.1179],\n",
            "          [-0.2496]],\n",
            "\n",
            "         [[-0.5816],\n",
            "          [-0.1797],\n",
            "          [-0.5580]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3974],\n",
            "          [-0.0328],\n",
            "          [ 0.2095]],\n",
            "\n",
            "         [[ 0.8808],\n",
            "          [ 0.2231],\n",
            "          [ 0.5881]],\n",
            "\n",
            "         [[-0.0895],\n",
            "          [ 0.0104],\n",
            "          [-0.1497]],\n",
            "\n",
            "         [[-0.4809],\n",
            "          [-0.2251],\n",
            "          [-0.6220]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "keys_transposed: tensor([[[[ 0.4506,  0.4500,  0.2348]],\n",
            "\n",
            "         [[ 0.0562,  0.1279, -0.0156]],\n",
            "\n",
            "         [[ 0.4304,  0.4279,  0.3336]],\n",
            "\n",
            "         [[ 0.1536,  0.1609,  0.2983]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4718,  0.5589,  0.0988]],\n",
            "\n",
            "         [[ 0.0798,  0.2457, -0.0504]],\n",
            "\n",
            "         [[ 0.5783,  0.4936,  0.1449]],\n",
            "\n",
            "         [[ 0.0684,  0.3631,  0.3616]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5788,  0.2732,  0.1826]],\n",
            "\n",
            "         [[ 0.3581, -0.3379, -0.1399]],\n",
            "\n",
            "         [[ 0.4959,  0.4120,  0.5550]],\n",
            "\n",
            "         [[ 0.3287,  0.3160,  0.3188]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4993,  0.3656,  0.7465]],\n",
            "\n",
            "         [[-0.1842, -0.0474,  0.0466]],\n",
            "\n",
            "         [[ 0.6092,  0.4571,  0.8577]],\n",
            "\n",
            "         [[ 0.5535,  0.0515,  0.3549]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "scores: tensor([[[[-0.1036, -0.1034, -0.0540],\n",
            "          [-0.1769, -0.1766, -0.0922],\n",
            "          [-0.2183, -0.2180, -0.1138]],\n",
            "\n",
            "         [[-0.0146, -0.0331,  0.0040],\n",
            "          [-0.0243, -0.0553,  0.0068],\n",
            "          [ 0.0040,  0.0091, -0.0011]],\n",
            "\n",
            "         [[-0.0640, -0.0636, -0.0496],\n",
            "          [-0.0359, -0.0357, -0.0279],\n",
            "          [-0.0777, -0.0773, -0.0603]],\n",
            "\n",
            "         [[ 0.1096,  0.1148,  0.2128],\n",
            "          [ 0.1314,  0.1377,  0.2553],\n",
            "          [ 0.0760,  0.0797,  0.1477]]],\n",
            "\n",
            "\n",
            "        [[[-0.1834, -0.2173, -0.0384],\n",
            "          [-0.1595, -0.1890, -0.0334],\n",
            "          [-0.2157, -0.2555, -0.0452]],\n",
            "\n",
            "         [[-0.0194, -0.0597,  0.0123],\n",
            "          [-0.0159, -0.0490,  0.0101],\n",
            "          [-0.0396, -0.1220,  0.0250]],\n",
            "\n",
            "         [[-0.0580, -0.0495, -0.0145],\n",
            "          [-0.0990, -0.0845, -0.0248],\n",
            "          [-0.2833, -0.2419, -0.0710]],\n",
            "\n",
            "         [[ 0.0716,  0.3802,  0.3786],\n",
            "          [ 0.0694,  0.3685,  0.3669],\n",
            "          [ 0.0504,  0.2676,  0.2665]]],\n",
            "\n",
            "\n",
            "        [[[-0.1404, -0.0663, -0.0443],\n",
            "          [-0.2827, -0.1334, -0.0892],\n",
            "          [-0.1520, -0.0717, -0.0480]],\n",
            "\n",
            "         [[ 0.0379, -0.0358, -0.0148],\n",
            "          [-0.2316,  0.2185,  0.0905],\n",
            "          [-0.0056,  0.0053,  0.0022]],\n",
            "\n",
            "         [[-0.0271, -0.0225, -0.0303],\n",
            "          [-0.1487, -0.1235, -0.1664],\n",
            "          [-0.0837, -0.0695, -0.0937]],\n",
            "\n",
            "         [[ 0.1514,  0.1456,  0.1469],\n",
            "          [ 0.3600,  0.3461,  0.3492],\n",
            "          [ 0.2939,  0.2825,  0.2851]]],\n",
            "\n",
            "\n",
            "        [[[-0.3247, -0.2377, -0.4855],\n",
            "          [-0.2808, -0.2056, -0.4199],\n",
            "          [-0.2237, -0.1638, -0.3346]],\n",
            "\n",
            "         [[ 0.0387,  0.0099, -0.0098],\n",
            "          [ 0.0033,  0.0008, -0.0008],\n",
            "          [ 0.0560,  0.0144, -0.0142]],\n",
            "\n",
            "         [[-0.1935, -0.1452, -0.2725],\n",
            "          [ 0.1636,  0.1228,  0.2304],\n",
            "          [-0.0783, -0.0587, -0.1103]],\n",
            "\n",
            "         [[ 0.3862,  0.0360,  0.2476],\n",
            "          [ 0.3521,  0.0328,  0.2258],\n",
            "          [ 0.6146,  0.0572,  0.3941]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "scores: torch.Size([4, 4, 3, 3])\n",
            "masks: torch.Size([1, 1, 1, 3])\n",
            "masked scores: tensor([[[[-1.0357e-01, -1.0343e-01, -1.0000e+09],\n",
            "          [-1.7689e-01, -1.7665e-01, -1.0000e+09],\n",
            "          [-2.1833e-01, -2.1803e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.4569e-02, -3.3124e-02, -1.0000e+09],\n",
            "          [-2.4317e-02, -5.5288e-02, -1.0000e+09],\n",
            "          [ 4.0089e-03,  9.1148e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-6.3996e-02, -6.3619e-02, -1.0000e+09],\n",
            "          [-3.5932e-02, -3.5720e-02, -1.0000e+09],\n",
            "          [-7.7748e-02, -7.7290e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.0959e-01,  1.1481e-01, -1.0000e+09],\n",
            "          [ 1.3144e-01,  1.3769e-01, -1.0000e+09],\n",
            "          [ 7.6035e-02,  7.9654e-02, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.8343e-01, -2.1732e-01, -1.0000e+09],\n",
            "          [-1.5952e-01, -1.8899e-01, -1.0000e+09],\n",
            "          [-2.1570e-01, -2.5554e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.9390e-02, -5.9694e-02, -1.0000e+09],\n",
            "          [-1.5932e-02, -4.9048e-02, -1.0000e+09],\n",
            "          [-3.9616e-02, -1.2196e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-5.8036e-02, -4.9542e-02, -1.0000e+09],\n",
            "          [-9.8954e-02, -8.4472e-02, -1.0000e+09],\n",
            "          [-2.8333e-01, -2.4186e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 7.1643e-02,  3.8021e-01, -1.0000e+09],\n",
            "          [ 6.9432e-02,  3.6847e-01, -1.0000e+09],\n",
            "          [ 5.0423e-02,  2.6759e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.4036e-01, -6.6250e-02, -1.0000e+09],\n",
            "          [-2.8272e-01, -1.3345e-01, -1.0000e+09],\n",
            "          [-1.5198e-01, -7.1736e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.7940e-02, -3.5802e-02, -1.0000e+09],\n",
            "          [-2.3159e-01,  2.1854e-01, -1.0000e+09],\n",
            "          [-5.6032e-03,  5.2875e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-2.7053e-02, -2.2478e-02, -1.0000e+09],\n",
            "          [-1.4867e-01, -1.2353e-01, -1.0000e+09],\n",
            "          [-8.3685e-02, -6.9533e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.5144e-01,  1.4560e-01, -1.0000e+09],\n",
            "          [ 3.5997e-01,  3.4607e-01, -1.0000e+09],\n",
            "          [ 2.9386e-01,  2.8252e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-3.2467e-01, -2.3773e-01, -1.0000e+09],\n",
            "          [-2.8084e-01, -2.0564e-01, -1.0000e+09],\n",
            "          [-2.2375e-01, -1.6383e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.8666e-02,  9.9399e-03, -1.0000e+09],\n",
            "          [ 3.2569e-03,  8.3724e-04, -1.0000e+09],\n",
            "          [ 5.5962e-02,  1.4386e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-1.9355e-01, -1.4520e-01, -1.0000e+09],\n",
            "          [ 1.6362e-01,  1.2275e-01, -1.0000e+09],\n",
            "          [-7.8310e-02, -5.8750e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.8619e-01,  3.5966e-02, -1.0000e+09],\n",
            "          [ 3.5214e-01,  3.2795e-02, -1.0000e+09],\n",
            "          [ 6.1457e-01,  5.7236e-02, -1.0000e+09]]]], device='cuda:0',\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "scaled_scores: tensor([[[[-1.0357e-01, -1.0343e-01, -1.0000e+09],\n",
            "          [-1.7689e-01, -1.7665e-01, -1.0000e+09],\n",
            "          [-2.1833e-01, -2.1803e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.4569e-02, -3.3124e-02, -1.0000e+09],\n",
            "          [-2.4317e-02, -5.5288e-02, -1.0000e+09],\n",
            "          [ 4.0089e-03,  9.1148e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-6.3996e-02, -6.3619e-02, -1.0000e+09],\n",
            "          [-3.5932e-02, -3.5720e-02, -1.0000e+09],\n",
            "          [-7.7748e-02, -7.7290e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.0959e-01,  1.1481e-01, -1.0000e+09],\n",
            "          [ 1.3144e-01,  1.3769e-01, -1.0000e+09],\n",
            "          [ 7.6035e-02,  7.9654e-02, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.8343e-01, -2.1732e-01, -1.0000e+09],\n",
            "          [-1.5952e-01, -1.8899e-01, -1.0000e+09],\n",
            "          [-2.1570e-01, -2.5554e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.9390e-02, -5.9694e-02, -1.0000e+09],\n",
            "          [-1.5932e-02, -4.9048e-02, -1.0000e+09],\n",
            "          [-3.9616e-02, -1.2196e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-5.8036e-02, -4.9542e-02, -1.0000e+09],\n",
            "          [-9.8954e-02, -8.4472e-02, -1.0000e+09],\n",
            "          [-2.8333e-01, -2.4186e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 7.1643e-02,  3.8021e-01, -1.0000e+09],\n",
            "          [ 6.9432e-02,  3.6847e-01, -1.0000e+09],\n",
            "          [ 5.0423e-02,  2.6759e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.4036e-01, -6.6250e-02, -1.0000e+09],\n",
            "          [-2.8272e-01, -1.3345e-01, -1.0000e+09],\n",
            "          [-1.5198e-01, -7.1736e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.7940e-02, -3.5802e-02, -1.0000e+09],\n",
            "          [-2.3159e-01,  2.1854e-01, -1.0000e+09],\n",
            "          [-5.6032e-03,  5.2875e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-2.7053e-02, -2.2478e-02, -1.0000e+09],\n",
            "          [-1.4867e-01, -1.2353e-01, -1.0000e+09],\n",
            "          [-8.3685e-02, -6.9533e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.5144e-01,  1.4560e-01, -1.0000e+09],\n",
            "          [ 3.5997e-01,  3.4607e-01, -1.0000e+09],\n",
            "          [ 2.9386e-01,  2.8252e-01, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-3.2467e-01, -2.3773e-01, -1.0000e+09],\n",
            "          [-2.8084e-01, -2.0564e-01, -1.0000e+09],\n",
            "          [-2.2375e-01, -1.6383e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.8666e-02,  9.9399e-03, -1.0000e+09],\n",
            "          [ 3.2569e-03,  8.3724e-04, -1.0000e+09],\n",
            "          [ 5.5962e-02,  1.4386e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-1.9355e-01, -1.4520e-01, -1.0000e+09],\n",
            "          [ 1.6362e-01,  1.2275e-01, -1.0000e+09],\n",
            "          [-7.8310e-02, -5.8750e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.8619e-01,  3.5966e-02, -1.0000e+09],\n",
            "          [ 3.5214e-01,  3.2795e-02, -1.0000e+09],\n",
            "          [ 6.1457e-01,  5.7236e-02, -1.0000e+09]]]], device='cuda:0',\n",
            "       grad_fn=<DivBackward0>)\n",
            "softmax_scores: tensor([[[[0.5000, 0.5000, 0.0000],\n",
            "          [0.4999, 0.5001, 0.0000],\n",
            "          [0.4999, 0.5001, 0.0000]],\n",
            "\n",
            "         [[0.5046, 0.4954, 0.0000],\n",
            "          [0.5077, 0.4923, 0.0000],\n",
            "          [0.4987, 0.5013, 0.0000]],\n",
            "\n",
            "         [[0.4999, 0.5001, 0.0000],\n",
            "          [0.4999, 0.5001, 0.0000],\n",
            "          [0.4999, 0.5001, 0.0000]],\n",
            "\n",
            "         [[0.4987, 0.5013, 0.0000],\n",
            "          [0.4984, 0.5016, 0.0000],\n",
            "          [0.4991, 0.5009, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.5085, 0.4915, 0.0000],\n",
            "          [0.5074, 0.4926, 0.0000],\n",
            "          [0.5100, 0.4900, 0.0000]],\n",
            "\n",
            "         [[0.5101, 0.4899, 0.0000],\n",
            "          [0.5083, 0.4917, 0.0000],\n",
            "          [0.5206, 0.4794, 0.0000]],\n",
            "\n",
            "         [[0.4979, 0.5021, 0.0000],\n",
            "          [0.4964, 0.5036, 0.0000],\n",
            "          [0.4896, 0.5104, 0.0000]],\n",
            "\n",
            "         [[0.4235, 0.5765, 0.0000],\n",
            "          [0.4258, 0.5742, 0.0000],\n",
            "          [0.4459, 0.5541, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.4815, 0.5185, 0.0000],\n",
            "          [0.4628, 0.5372, 0.0000],\n",
            "          [0.4799, 0.5201, 0.0000]],\n",
            "\n",
            "         [[0.5184, 0.4816, 0.0000],\n",
            "          [0.3893, 0.6107, 0.0000],\n",
            "          [0.4973, 0.5027, 0.0000]],\n",
            "\n",
            "         [[0.4989, 0.5011, 0.0000],\n",
            "          [0.4937, 0.5063, 0.0000],\n",
            "          [0.4965, 0.5035, 0.0000]],\n",
            "\n",
            "         [[0.5015, 0.4985, 0.0000],\n",
            "          [0.5035, 0.4965, 0.0000],\n",
            "          [0.5028, 0.4972, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.4783, 0.5217, 0.0000],\n",
            "          [0.4812, 0.5188, 0.0000],\n",
            "          [0.4850, 0.5150, 0.0000]],\n",
            "\n",
            "         [[0.5072, 0.4928, 0.0000],\n",
            "          [0.5006, 0.4994, 0.0000],\n",
            "          [0.5104, 0.4896, 0.0000]],\n",
            "\n",
            "         [[0.4879, 0.5121, 0.0000],\n",
            "          [0.5102, 0.4898, 0.0000],\n",
            "          [0.4951, 0.5049, 0.0000]],\n",
            "\n",
            "         [[0.5867, 0.4133, 0.0000],\n",
            "          [0.5792, 0.4208, 0.0000],\n",
            "          [0.6358, 0.3642, 0.0000]]]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "softmax_socres shape: torch.Size([4, 4, 3, 3])\n",
            "values: tensor([[[[-0.1217],\n",
            "          [-0.1759],\n",
            "          [ 0.0253]],\n",
            "\n",
            "         [[ 0.3473],\n",
            "          [ 0.3339],\n",
            "          [ 0.4987]],\n",
            "\n",
            "         [[-0.0670],\n",
            "          [-0.1457],\n",
            "          [-0.2021]],\n",
            "\n",
            "         [[-0.2718],\n",
            "          [-0.3290],\n",
            "          [-0.3617]]],\n",
            "\n",
            "\n",
            "        [[[-0.0712],\n",
            "          [-0.1591],\n",
            "          [-0.0101]],\n",
            "\n",
            "         [[ 0.2086],\n",
            "          [ 0.5606],\n",
            "          [ 0.5872]],\n",
            "\n",
            "         [[-0.1090],\n",
            "          [-0.3236],\n",
            "          [-0.2122]],\n",
            "\n",
            "         [[-0.3836],\n",
            "          [-0.5281],\n",
            "          [-0.2712]]],\n",
            "\n",
            "\n",
            "        [[[-0.2719],\n",
            "          [ 0.3345],\n",
            "          [ 0.3393]],\n",
            "\n",
            "         [[ 0.4892],\n",
            "          [ 0.6161],\n",
            "          [ 0.4939]],\n",
            "\n",
            "         [[-0.4076],\n",
            "          [ 0.1179],\n",
            "          [-0.2496]],\n",
            "\n",
            "         [[-0.5816],\n",
            "          [-0.1797],\n",
            "          [-0.5580]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3974],\n",
            "          [-0.0328],\n",
            "          [ 0.2095]],\n",
            "\n",
            "         [[ 0.8808],\n",
            "          [ 0.2231],\n",
            "          [ 0.5881]],\n",
            "\n",
            "         [[-0.0895],\n",
            "          [ 0.0104],\n",
            "          [-0.1497]],\n",
            "\n",
            "         [[-0.4809],\n",
            "          [-0.2251],\n",
            "          [-0.6220]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "v_out shape torch.Size([4, 4, 3, 1])\n",
            "v_out: tensor([[[[-0.1488],\n",
            "          [-0.1488],\n",
            "          [-0.1488]],\n",
            "\n",
            "         [[ 0.3407],\n",
            "          [ 0.3407],\n",
            "          [ 0.3406]],\n",
            "\n",
            "         [[-0.1064],\n",
            "          [-0.1064],\n",
            "          [-0.1064]],\n",
            "\n",
            "         [[-0.3005],\n",
            "          [-0.3005],\n",
            "          [-0.3004]]],\n",
            "\n",
            "\n",
            "        [[[-0.1144],\n",
            "          [-0.1145],\n",
            "          [-0.1143]],\n",
            "\n",
            "         [[ 0.3811],\n",
            "          [ 0.3817],\n",
            "          [ 0.3774]],\n",
            "\n",
            "         [[-0.2168],\n",
            "          [-0.2171],\n",
            "          [-0.2185]],\n",
            "\n",
            "         [[-0.4670],\n",
            "          [-0.4666],\n",
            "          [-0.4637]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0425],\n",
            "          [ 0.0539],\n",
            "          [ 0.0434]],\n",
            "\n",
            "         [[ 0.5503],\n",
            "          [ 0.5667],\n",
            "          [ 0.5530]],\n",
            "\n",
            "         [[-0.1443],\n",
            "          [-0.1416],\n",
            "          [-0.1430]],\n",
            "\n",
            "         [[-0.3812],\n",
            "          [-0.3820],\n",
            "          [-0.3818]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1730],\n",
            "          [ 0.1742],\n",
            "          [ 0.1759]],\n",
            "\n",
            "         [[ 0.5567],\n",
            "          [ 0.5523],\n",
            "          [ 0.5588]],\n",
            "\n",
            "         [[-0.0383],\n",
            "          [-0.0406],\n",
            "          [-0.0391]],\n",
            "\n",
            "         [[-0.3751],\n",
            "          [-0.3732],\n",
            "          [-0.3877]]]], device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
            "v_out shape before: torch.Size([4, 4, 3, 1])\n",
            "v_out shape: torch.Size([4, 3, 4])\n",
            "v_out reshaped: tensor([[[-0.1488,  0.3407, -0.1064, -0.3005],\n",
            "         [-0.1488,  0.3407, -0.1064, -0.3005],\n",
            "         [-0.1488,  0.3406, -0.1064, -0.3004]],\n",
            "\n",
            "        [[-0.1144,  0.3811, -0.2168, -0.4670],\n",
            "         [-0.1145,  0.3817, -0.2171, -0.4666],\n",
            "         [-0.1143,  0.3774, -0.2185, -0.4637]],\n",
            "\n",
            "        [[ 0.0425,  0.5503, -0.1443, -0.3812],\n",
            "         [ 0.0539,  0.5667, -0.1416, -0.3820],\n",
            "         [ 0.0434,  0.5530, -0.1430, -0.3818]],\n",
            "\n",
            "        [[ 0.1730,  0.5567, -0.0383, -0.3751],\n",
            "         [ 0.1742,  0.5523, -0.0406, -0.3732],\n",
            "         [ 0.1759,  0.5588, -0.0391, -0.3877]]], device='cuda:0',\n",
            "       grad_fn=<ReshapeAliasBackward0>)\n",
            "output shape torch.Size([4, 3, 4])\n",
            "output: tensor([[[ 0.1426, -0.1900,  0.0498, -0.4583],\n",
            "         [ 0.1426, -0.1900,  0.0498, -0.4583],\n",
            "         [ 0.1426, -0.1900,  0.0498, -0.4583]],\n",
            "\n",
            "        [[ 0.1520, -0.1843, -0.0111, -0.5433],\n",
            "         [ 0.1517, -0.1848, -0.0111, -0.5434],\n",
            "         [ 0.1516, -0.1851, -0.0081, -0.5438]],\n",
            "\n",
            "        [[ 0.1527, -0.1819, -0.0719, -0.5735],\n",
            "         [ 0.1528, -0.1817, -0.0800, -0.5776],\n",
            "         [ 0.1528, -0.1816, -0.0735, -0.5734]],\n",
            "\n",
            "        [[ 0.2126, -0.0883, -0.1099, -0.5799],\n",
            "         [ 0.2128, -0.0882, -0.1072, -0.5813],\n",
            "         [ 0.2158, -0.0836, -0.1158, -0.5827]]], device='cuda:0',\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_headed_attention_decoder_self():\n",
        "    num_heads = 8\n",
        "    d_attn = 8\n",
        "    d_x = 8\n",
        "    d_out = 8\n",
        "    d_mid = 8\n",
        "    length_x = 3\n",
        "    batch_size = 4\n",
        "    padding_mask = torch.tensor([[1, 1, 0], [1, 1, 0], [1, 0, 0], [1, 1, 1]], dtype=torch.int32).to(device)\n",
        "\n",
        "    multi_headed_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_x, d_out, d_mid, MaskStrategy['UNMASKED'], 0.0, True).to(device)\n",
        "    multi_headed_attention.enable_subsequent_mask()\n",
        "    x = torch.rand(batch_size, length_x, d_x).to(device)\n",
        "    output = multi_headed_attention(x, x, padding_mask)\n",
        "    print(\"output:\", output)\n",
        "    assert output.shape == (batch_size, length_x, d_out)\n",
        "test_multi_headed_attention_decoder_self()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vn9s5ak_Yr1B",
        "outputId": "8ede2a0d-d5e4-43af-bb52-00b509b97883"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "queries torch.Size([4, 3, 8])\n",
            "torch.Size([4, 8, 3, 1])\n",
            "torch.Size([4, 8, 3, 1])\n",
            "torch.Size([4, 8, 3, 1])\n",
            "padding mask: torch.Size([4, 1, 3])\n",
            "mask tril tensor([[ True, False, False],\n",
            "        [ True,  True, False],\n",
            "        [ True,  True,  True]], device='cuda:0')\n",
            "merged mask: tensor([[[1, 0, 0],\n",
            "         [1, 1, 0],\n",
            "         [1, 1, 0]],\n",
            "\n",
            "        [[1, 0, 0],\n",
            "         [1, 1, 0],\n",
            "         [1, 1, 0]],\n",
            "\n",
            "        [[1, 0, 0],\n",
            "         [1, 0, 0],\n",
            "         [1, 0, 0]],\n",
            "\n",
            "        [[1, 0, 0],\n",
            "         [1, 1, 0],\n",
            "         [1, 1, 1]]], device='cuda:0', dtype=torch.int32)\n",
            "mask tensor([[[[1, 0, 0],\n",
            "          [1, 1, 0],\n",
            "          [1, 1, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 0, 0],\n",
            "          [1, 1, 0],\n",
            "          [1, 1, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 0, 0],\n",
            "          [1, 0, 0],\n",
            "          [1, 0, 0]]],\n",
            "\n",
            "\n",
            "        [[[1, 0, 0],\n",
            "          [1, 1, 0],\n",
            "          [1, 1, 1]]]], device='cuda:0', dtype=torch.int32)\n",
            "mask torch.Size([4, 1, 3, 3])\n",
            "queries: tensor([[[[ 2.8792e-01],\n",
            "          [ 3.0256e-01],\n",
            "          [-3.9677e-02]],\n",
            "\n",
            "         [[ 8.0188e-01],\n",
            "          [ 5.2465e-01],\n",
            "          [ 6.4416e-01]],\n",
            "\n",
            "         [[ 3.1568e-01],\n",
            "          [ 4.8959e-01],\n",
            "          [ 9.5684e-01]],\n",
            "\n",
            "         [[-2.8292e-01],\n",
            "          [ 1.2687e-01],\n",
            "          [-3.1234e-01]],\n",
            "\n",
            "         [[ 7.6956e-02],\n",
            "          [ 4.8025e-01],\n",
            "          [ 3.8537e-01]],\n",
            "\n",
            "         [[-3.6390e-01],\n",
            "          [-3.7672e-01],\n",
            "          [-8.2057e-01]],\n",
            "\n",
            "         [[ 3.0520e-01],\n",
            "          [ 2.9588e-01],\n",
            "          [ 7.2760e-02]],\n",
            "\n",
            "         [[ 4.4399e-01],\n",
            "          [ 5.4580e-01],\n",
            "          [ 3.1898e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.4692e-01],\n",
            "          [ 1.2710e-01],\n",
            "          [-1.2809e-01]],\n",
            "\n",
            "         [[ 4.1641e-01],\n",
            "          [ 6.8935e-01],\n",
            "          [ 2.7090e-01]],\n",
            "\n",
            "         [[ 4.6507e-01],\n",
            "          [ 7.3577e-01],\n",
            "          [ 5.3853e-01]],\n",
            "\n",
            "         [[-9.3184e-02],\n",
            "          [-2.1053e-01],\n",
            "          [ 3.3745e-01]],\n",
            "\n",
            "         [[ 4.3646e-01],\n",
            "          [ 3.8362e-01],\n",
            "          [ 7.9303e-01]],\n",
            "\n",
            "         [[-3.7769e-01],\n",
            "          [-5.6144e-01],\n",
            "          [-5.3031e-01]],\n",
            "\n",
            "         [[ 1.4465e-01],\n",
            "          [ 2.3583e-01],\n",
            "          [ 1.7842e-01]],\n",
            "\n",
            "         [[-1.7607e-01],\n",
            "          [ 1.8183e-01],\n",
            "          [ 8.4074e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.8981e-02],\n",
            "          [ 1.4711e-01],\n",
            "          [-1.3095e-01]],\n",
            "\n",
            "         [[ 6.1351e-01],\n",
            "          [ 6.8854e-01],\n",
            "          [ 4.0509e-01]],\n",
            "\n",
            "         [[ 3.6233e-01],\n",
            "          [ 8.8673e-02],\n",
            "          [ 5.2742e-01]],\n",
            "\n",
            "         [[-1.9211e-04],\n",
            "          [ 8.8756e-02],\n",
            "          [ 5.9930e-02]],\n",
            "\n",
            "         [[ 3.7186e-01],\n",
            "          [ 3.6667e-01],\n",
            "          [ 6.9956e-01]],\n",
            "\n",
            "         [[ 3.6163e-02],\n",
            "          [-2.1034e-01],\n",
            "          [-2.9985e-01]],\n",
            "\n",
            "         [[ 3.8461e-01],\n",
            "          [ 3.7383e-01],\n",
            "          [ 1.0265e-01]],\n",
            "\n",
            "         [[-1.0375e-02],\n",
            "          [ 2.1604e-01],\n",
            "          [-1.7259e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7378e-02],\n",
            "          [-9.5055e-02],\n",
            "          [ 1.8910e-01]],\n",
            "\n",
            "         [[ 8.1435e-01],\n",
            "          [ 5.6710e-01],\n",
            "          [ 6.4457e-01]],\n",
            "\n",
            "         [[ 4.4089e-01],\n",
            "          [ 7.2155e-01],\n",
            "          [ 2.5555e-01]],\n",
            "\n",
            "         [[-1.3263e-01],\n",
            "          [ 1.8870e-01],\n",
            "          [ 6.1234e-02]],\n",
            "\n",
            "         [[ 3.8301e-01],\n",
            "          [ 7.1023e-01],\n",
            "          [ 4.8682e-01]],\n",
            "\n",
            "         [[-8.2488e-02],\n",
            "          [-4.7256e-01],\n",
            "          [-2.7025e-01]],\n",
            "\n",
            "         [[ 3.1632e-01],\n",
            "          [ 3.4908e-01],\n",
            "          [ 2.4880e-01]],\n",
            "\n",
            "         [[-1.8838e-02],\n",
            "          [-2.0652e-02],\n",
            "          [ 6.3491e-02]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "keys: tensor([[[[ 0.1226],\n",
            "          [ 0.1854],\n",
            "          [-0.1382]],\n",
            "\n",
            "         [[-0.2115],\n",
            "          [-0.5868],\n",
            "          [-0.2015]],\n",
            "\n",
            "         [[ 0.7872],\n",
            "          [ 0.8159],\n",
            "          [ 0.8837]],\n",
            "\n",
            "         [[ 0.3559],\n",
            "          [ 0.1374],\n",
            "          [-0.0599]],\n",
            "\n",
            "         [[ 0.2820],\n",
            "          [ 0.5458],\n",
            "          [ 0.2374]],\n",
            "\n",
            "         [[-0.1841],\n",
            "          [-0.0684],\n",
            "          [-0.2472]],\n",
            "\n",
            "         [[-0.5582],\n",
            "          [-0.5298],\n",
            "          [-0.6967]],\n",
            "\n",
            "         [[-0.1017],\n",
            "          [-0.1159],\n",
            "          [-0.4072]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0802],\n",
            "          [ 0.0215],\n",
            "          [ 0.3208]],\n",
            "\n",
            "         [[-0.0065],\n",
            "          [-0.2861],\n",
            "          [-0.5867]],\n",
            "\n",
            "         [[ 0.4539],\n",
            "          [ 0.7375],\n",
            "          [ 0.7037]],\n",
            "\n",
            "         [[ 0.1229],\n",
            "          [ 0.0885],\n",
            "          [-0.0995]],\n",
            "\n",
            "         [[ 0.4746],\n",
            "          [ 0.2544],\n",
            "          [ 0.7756]],\n",
            "\n",
            "         [[ 0.2878],\n",
            "          [-0.0281],\n",
            "          [ 0.1955]],\n",
            "\n",
            "         [[ 0.1232],\n",
            "          [-0.5163],\n",
            "          [-0.1703]],\n",
            "\n",
            "         [[-0.2377],\n",
            "          [-0.3342],\n",
            "          [-0.2900]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1075],\n",
            "          [ 0.4581],\n",
            "          [ 0.1609]],\n",
            "\n",
            "         [[ 0.0512],\n",
            "          [-0.4968],\n",
            "          [-0.3522]],\n",
            "\n",
            "         [[ 0.6164],\n",
            "          [ 0.7586],\n",
            "          [ 0.4726]],\n",
            "\n",
            "         [[ 0.4581],\n",
            "          [ 0.3742],\n",
            "          [ 0.1426]],\n",
            "\n",
            "         [[ 0.4243],\n",
            "          [ 0.5207],\n",
            "          [ 0.3910]],\n",
            "\n",
            "         [[ 0.2767],\n",
            "          [-0.0271],\n",
            "          [ 0.1010]],\n",
            "\n",
            "         [[ 0.1638],\n",
            "          [-0.3639],\n",
            "          [-0.1030]],\n",
            "\n",
            "         [[-0.3561],\n",
            "          [-0.1732],\n",
            "          [-0.2984]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1009],\n",
            "          [ 0.2650],\n",
            "          [ 0.2809]],\n",
            "\n",
            "         [[-0.1294],\n",
            "          [-0.4353],\n",
            "          [-0.5913]],\n",
            "\n",
            "         [[ 0.6668],\n",
            "          [ 0.8515],\n",
            "          [ 0.4827]],\n",
            "\n",
            "         [[ 0.5006],\n",
            "          [ 0.0265],\n",
            "          [ 0.3031]],\n",
            "\n",
            "         [[ 0.2061],\n",
            "          [ 0.5461],\n",
            "          [ 0.4582]],\n",
            "\n",
            "         [[-0.0362],\n",
            "          [ 0.2156],\n",
            "          [ 0.0061]],\n",
            "\n",
            "         [[-0.2129],\n",
            "          [-0.2573],\n",
            "          [-0.3080]],\n",
            "\n",
            "         [[-0.4043],\n",
            "          [-0.5952],\n",
            "          [-0.0241]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "values: tensor([[[[-0.4421],\n",
            "          [-0.4828],\n",
            "          [-0.3752]],\n",
            "\n",
            "         [[ 0.6036],\n",
            "          [ 0.6242],\n",
            "          [ 0.8511]],\n",
            "\n",
            "         [[ 0.0412],\n",
            "          [ 0.0077],\n",
            "          [ 0.2528]],\n",
            "\n",
            "         [[ 0.7564],\n",
            "          [ 0.9165],\n",
            "          [ 1.0039]],\n",
            "\n",
            "         [[-0.1312],\n",
            "          [-0.2673],\n",
            "          [-0.2868]],\n",
            "\n",
            "         [[ 0.3612],\n",
            "          [ 0.2347],\n",
            "          [ 0.0306]],\n",
            "\n",
            "         [[-0.8842],\n",
            "          [-0.4901],\n",
            "          [-0.9032]],\n",
            "\n",
            "         [[-0.2133],\n",
            "          [-0.2283],\n",
            "          [-0.3304]]],\n",
            "\n",
            "\n",
            "        [[[-0.3210],\n",
            "          [-0.2037],\n",
            "          [-0.3132]],\n",
            "\n",
            "         [[ 0.0236],\n",
            "          [ 0.7791],\n",
            "          [ 0.3552]],\n",
            "\n",
            "         [[-0.2618],\n",
            "          [ 0.0232],\n",
            "          [-0.1934]],\n",
            "\n",
            "         [[ 0.6312],\n",
            "          [ 0.9531],\n",
            "          [ 0.8175]],\n",
            "\n",
            "         [[ 0.4339],\n",
            "          [-0.1309],\n",
            "          [ 0.1612]],\n",
            "\n",
            "         [[-0.1243],\n",
            "          [ 0.1641],\n",
            "          [-0.1817]],\n",
            "\n",
            "         [[-0.5422],\n",
            "          [-0.6978],\n",
            "          [-0.3557]],\n",
            "\n",
            "         [[-0.4952],\n",
            "          [-0.3138],\n",
            "          [-0.3588]]],\n",
            "\n",
            "\n",
            "        [[[-0.3404],\n",
            "          [-0.2503],\n",
            "          [-0.1635]],\n",
            "\n",
            "         [[-0.1434],\n",
            "          [ 0.5357],\n",
            "          [ 0.5229]],\n",
            "\n",
            "         [[-0.5624],\n",
            "          [-0.2688],\n",
            "          [-0.2555]],\n",
            "\n",
            "         [[ 0.8203],\n",
            "          [ 0.6791],\n",
            "          [ 0.6522]],\n",
            "\n",
            "         [[ 0.3699],\n",
            "          [ 0.1479],\n",
            "          [ 0.2772]],\n",
            "\n",
            "         [[ 0.3809],\n",
            "          [ 0.3575],\n",
            "          [ 0.0997]],\n",
            "\n",
            "         [[-0.5416],\n",
            "          [-0.6572],\n",
            "          [-0.3753]],\n",
            "\n",
            "         [[-0.3194],\n",
            "          [-0.0777],\n",
            "          [-0.1731]]],\n",
            "\n",
            "\n",
            "        [[[-0.1762],\n",
            "          [-0.0455],\n",
            "          [-0.2318]],\n",
            "\n",
            "         [[ 0.4854],\n",
            "          [ 0.5005],\n",
            "          [ 0.7248]],\n",
            "\n",
            "         [[-0.3959],\n",
            "          [-0.3573],\n",
            "          [-0.1602]],\n",
            "\n",
            "         [[ 0.8613],\n",
            "          [ 1.0814],\n",
            "          [ 0.7300]],\n",
            "\n",
            "         [[ 0.2179],\n",
            "          [ 0.1167],\n",
            "          [ 0.1899]],\n",
            "\n",
            "         [[ 0.5898],\n",
            "          [ 0.1046],\n",
            "          [ 0.2981]],\n",
            "\n",
            "         [[-0.6992],\n",
            "          [-0.5217],\n",
            "          [-0.4574]],\n",
            "\n",
            "         [[-0.0751],\n",
            "          [-0.3024],\n",
            "          [-0.1810]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "keys_transposed: tensor([[[[ 0.1226,  0.1854, -0.1382]],\n",
            "\n",
            "         [[-0.2115, -0.5868, -0.2015]],\n",
            "\n",
            "         [[ 0.7872,  0.8159,  0.8837]],\n",
            "\n",
            "         [[ 0.3559,  0.1374, -0.0599]],\n",
            "\n",
            "         [[ 0.2820,  0.5458,  0.2374]],\n",
            "\n",
            "         [[-0.1841, -0.0684, -0.2472]],\n",
            "\n",
            "         [[-0.5582, -0.5298, -0.6967]],\n",
            "\n",
            "         [[-0.1017, -0.1159, -0.4072]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0802,  0.0215,  0.3208]],\n",
            "\n",
            "         [[-0.0065, -0.2861, -0.5867]],\n",
            "\n",
            "         [[ 0.4539,  0.7375,  0.7037]],\n",
            "\n",
            "         [[ 0.1229,  0.0885, -0.0995]],\n",
            "\n",
            "         [[ 0.4746,  0.2544,  0.7756]],\n",
            "\n",
            "         [[ 0.2878, -0.0281,  0.1955]],\n",
            "\n",
            "         [[ 0.1232, -0.5163, -0.1703]],\n",
            "\n",
            "         [[-0.2377, -0.3342, -0.2900]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1075,  0.4581,  0.1609]],\n",
            "\n",
            "         [[ 0.0512, -0.4968, -0.3522]],\n",
            "\n",
            "         [[ 0.6164,  0.7586,  0.4726]],\n",
            "\n",
            "         [[ 0.4581,  0.3742,  0.1426]],\n",
            "\n",
            "         [[ 0.4243,  0.5207,  0.3910]],\n",
            "\n",
            "         [[ 0.2767, -0.0271,  0.1010]],\n",
            "\n",
            "         [[ 0.1638, -0.3639, -0.1030]],\n",
            "\n",
            "         [[-0.3561, -0.1732, -0.2984]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1009,  0.2650,  0.2809]],\n",
            "\n",
            "         [[-0.1294, -0.4353, -0.5913]],\n",
            "\n",
            "         [[ 0.6668,  0.8515,  0.4827]],\n",
            "\n",
            "         [[ 0.5006,  0.0265,  0.3031]],\n",
            "\n",
            "         [[ 0.2061,  0.5461,  0.4582]],\n",
            "\n",
            "         [[-0.0362,  0.2156,  0.0061]],\n",
            "\n",
            "         [[-0.2129, -0.2573, -0.3080]],\n",
            "\n",
            "         [[-0.4043, -0.5952, -0.0241]]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward0>)\n",
            "scores: tensor([[[[ 3.5311e-02,  5.3369e-02, -3.9789e-02],\n",
            "          [ 3.7107e-02,  5.6083e-02, -4.1812e-02],\n",
            "          [-4.8661e-03, -7.3547e-03,  5.4832e-03]],\n",
            "\n",
            "         [[-1.6956e-01, -4.7052e-01, -1.6156e-01],\n",
            "          [-1.1094e-01, -3.0785e-01, -1.0570e-01],\n",
            "          [-1.3621e-01, -3.7798e-01, -1.2978e-01]],\n",
            "\n",
            "         [[ 2.4849e-01,  2.5756e-01,  2.7898e-01],\n",
            "          [ 3.8538e-01,  3.9945e-01,  4.3266e-01],\n",
            "          [ 7.5319e-01,  7.8068e-01,  8.4560e-01]],\n",
            "\n",
            "         [[-1.0070e-01, -3.8872e-02,  1.6959e-02],\n",
            "          [ 4.5158e-02,  1.7432e-02, -7.6052e-03],\n",
            "          [-1.1117e-01, -4.2915e-02,  1.8723e-02]],\n",
            "\n",
            "         [[ 2.1702e-02,  4.2006e-02,  1.8273e-02],\n",
            "          [ 1.3544e-01,  2.6214e-01,  1.1403e-01],\n",
            "          [ 1.0868e-01,  2.1035e-01,  9.1503e-02]],\n",
            "\n",
            "         [[ 6.6980e-02,  2.4908e-02,  8.9973e-02],\n",
            "          [ 6.9339e-02,  2.5785e-02,  9.3143e-02],\n",
            "          [ 1.5103e-01,  5.6166e-02,  2.0288e-01]],\n",
            "\n",
            "         [[-1.7036e-01, -1.6168e-01, -2.1262e-01],\n",
            "          [-1.6516e-01, -1.5675e-01, -2.0613e-01],\n",
            "          [-4.0614e-02, -3.8546e-02, -5.0690e-02]],\n",
            "\n",
            "         [[-4.5149e-02, -5.1453e-02, -1.8079e-01],\n",
            "          [-5.5502e-02, -6.3251e-02, -2.2224e-01],\n",
            "          [-3.2437e-02, -3.6965e-02, -1.2988e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.1781e-02, -3.1585e-03, -4.7134e-02],\n",
            "          [ 1.0191e-02,  2.7324e-03,  4.0775e-02],\n",
            "          [-1.0271e-02, -2.7537e-03, -4.1093e-02]],\n",
            "\n",
            "         [[-2.7198e-03, -1.1912e-01, -2.4431e-01],\n",
            "          [-4.5027e-03, -1.9720e-01, -4.0445e-01],\n",
            "          [-1.7694e-03, -7.7495e-02, -1.5894e-01]],\n",
            "\n",
            "         [[ 2.1110e-01,  3.4300e-01,  3.2726e-01],\n",
            "          [ 3.3397e-01,  5.4264e-01,  5.1774e-01],\n",
            "          [ 2.4444e-01,  3.9717e-01,  3.7894e-01]],\n",
            "\n",
            "         [[-1.1456e-02, -8.2510e-03,  9.2689e-03],\n",
            "          [-2.5883e-02, -1.8641e-02,  2.0941e-02],\n",
            "          [ 4.1486e-02,  2.9879e-02, -3.3566e-02]],\n",
            "\n",
            "         [[ 2.0714e-01,  1.1106e-01,  3.3852e-01],\n",
            "          [ 1.8207e-01,  9.7613e-02,  2.9754e-01],\n",
            "          [ 3.7637e-01,  2.0178e-01,  6.1508e-01]],\n",
            "\n",
            "         [[-1.0870e-01,  1.0596e-02, -7.3845e-02],\n",
            "          [-1.6158e-01,  1.5751e-02, -1.0977e-01],\n",
            "          [-1.5262e-01,  1.4878e-02, -1.0368e-01]],\n",
            "\n",
            "         [[ 1.7817e-02, -7.4687e-02, -2.4628e-02],\n",
            "          [ 2.9047e-02, -1.2176e-01, -4.0151e-02],\n",
            "          [ 2.1976e-02, -9.2123e-02, -3.0377e-02]],\n",
            "\n",
            "         [[ 4.1845e-02,  5.8840e-02,  5.1065e-02],\n",
            "          [-4.3214e-02, -6.0765e-02, -5.2736e-02],\n",
            "          [-1.9982e-02, -2.8097e-02, -2.4384e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.1916e-03,  1.7856e-02,  6.2714e-03],\n",
            "          [ 1.5819e-02,  6.7389e-02,  2.3668e-02],\n",
            "          [-1.4081e-02, -5.9988e-02, -2.1068e-02]],\n",
            "\n",
            "         [[ 3.1428e-02, -3.0481e-01, -2.1606e-01],\n",
            "          [ 3.5271e-02, -3.4209e-01, -2.4248e-01],\n",
            "          [ 2.0751e-02, -2.0126e-01, -1.4266e-01]],\n",
            "\n",
            "         [[ 2.2333e-01,  2.7486e-01,  1.7123e-01],\n",
            "          [ 5.4657e-02,  6.7267e-02,  4.1905e-02],\n",
            "          [ 3.2510e-01,  4.0010e-01,  2.4925e-01]],\n",
            "\n",
            "         [[-8.8006e-05, -7.1878e-05, -2.7388e-05],\n",
            "          [ 4.0660e-02,  3.3209e-02,  1.2654e-02],\n",
            "          [ 2.7455e-02,  2.2423e-02,  8.5439e-03]],\n",
            "\n",
            "         [[ 1.5779e-01,  1.9361e-01,  1.4540e-01],\n",
            "          [ 1.5559e-01,  1.9091e-01,  1.4337e-01],\n",
            "          [ 2.9684e-01,  3.6423e-01,  2.7354e-01]],\n",
            "\n",
            "         [[ 1.0008e-02, -9.8108e-04,  3.6531e-03],\n",
            "          [-5.8209e-02,  5.7064e-03, -2.1249e-02],\n",
            "          [-8.2979e-02,  8.1347e-03, -3.0291e-02]],\n",
            "\n",
            "         [[ 6.3016e-02, -1.3996e-01, -3.9600e-02],\n",
            "          [ 6.1248e-02, -1.3604e-01, -3.8490e-02],\n",
            "          [ 1.6818e-02, -3.7354e-02, -1.0569e-02]],\n",
            "\n",
            "         [[ 3.6942e-03,  1.7972e-03,  3.0959e-03],\n",
            "          [-7.6925e-02, -3.7424e-02, -6.4466e-02],\n",
            "          [ 6.1453e-02,  2.9897e-02,  5.1500e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7722e-03,  9.9036e-03,  1.0499e-02],\n",
            "          [-9.5929e-03, -2.5185e-02, -2.6700e-02],\n",
            "          [ 1.9084e-02,  5.0102e-02,  5.3115e-02]],\n",
            "\n",
            "         [[-1.0541e-01, -3.5452e-01, -4.8154e-01],\n",
            "          [-7.3408e-02, -2.4688e-01, -3.3534e-01],\n",
            "          [-8.3435e-02, -2.8061e-01, -3.8114e-01]],\n",
            "\n",
            "         [[ 2.9399e-01,  3.7543e-01,  2.1282e-01],\n",
            "          [ 4.8114e-01,  6.1441e-01,  3.4830e-01],\n",
            "          [ 1.7040e-01,  2.1760e-01,  1.2335e-01]],\n",
            "\n",
            "         [[-6.6400e-02, -3.5135e-03, -4.0203e-02],\n",
            "          [ 9.4474e-02,  4.9990e-03,  5.7200e-02],\n",
            "          [ 3.0656e-02,  1.6222e-03,  1.8561e-02]],\n",
            "\n",
            "         [[ 7.8938e-02,  2.0916e-01,  1.7550e-01],\n",
            "          [ 1.4638e-01,  3.8785e-01,  3.2544e-01],\n",
            "          [ 1.0033e-01,  2.6585e-01,  2.2306e-01]],\n",
            "\n",
            "         [[ 2.9879e-03, -1.7785e-02, -5.0117e-04],\n",
            "          [ 1.7117e-02, -1.0189e-01, -2.8712e-03],\n",
            "          [ 9.7892e-03, -5.8269e-02, -1.6420e-03]],\n",
            "\n",
            "         [[-6.7342e-02, -8.1387e-02, -9.7423e-02],\n",
            "          [-7.4318e-02, -8.9817e-02, -1.0751e-01],\n",
            "          [-5.2969e-02, -6.4016e-02, -7.6629e-02]],\n",
            "\n",
            "         [[ 7.6162e-03,  1.1213e-02,  4.5375e-04],\n",
            "          [ 8.3498e-03,  1.2293e-02,  4.9746e-04],\n",
            "          [-2.5670e-02, -3.7792e-02, -1.5293e-03]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "scores: torch.Size([4, 8, 3, 3])\n",
            "masks: torch.Size([4, 1, 3, 3])\n",
            "masked scores: tensor([[[[ 3.5311e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.7107e-02,  5.6083e-02, -1.0000e+09],\n",
            "          [-4.8661e-03, -7.3547e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-1.6956e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.1094e-01, -3.0785e-01, -1.0000e+09],\n",
            "          [-1.3621e-01, -3.7798e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.4849e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.8538e-01,  3.9945e-01, -1.0000e+09],\n",
            "          [ 7.5319e-01,  7.8068e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.0070e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.5158e-02,  1.7432e-02, -1.0000e+09],\n",
            "          [-1.1117e-01, -4.2915e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.1702e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.3544e-01,  2.6214e-01, -1.0000e+09],\n",
            "          [ 1.0868e-01,  2.1035e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 6.6980e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 6.9339e-02,  2.5785e-02, -1.0000e+09],\n",
            "          [ 1.5103e-01,  5.6166e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-1.7036e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.6516e-01, -1.5675e-01, -1.0000e+09],\n",
            "          [-4.0614e-02, -3.8546e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-4.5149e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-5.5502e-02, -6.3251e-02, -1.0000e+09],\n",
            "          [-3.2437e-02, -3.6965e-02, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.1781e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.0191e-02,  2.7324e-03, -1.0000e+09],\n",
            "          [-1.0271e-02, -2.7537e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-2.7198e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [-4.5027e-03, -1.9720e-01, -1.0000e+09],\n",
            "          [-1.7694e-03, -7.7495e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.1110e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.3397e-01,  5.4264e-01, -1.0000e+09],\n",
            "          [ 2.4444e-01,  3.9717e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.1456e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-2.5883e-02, -1.8641e-02, -1.0000e+09],\n",
            "          [ 4.1486e-02,  2.9879e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.0714e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.8207e-01,  9.7613e-02, -1.0000e+09],\n",
            "          [ 3.7637e-01,  2.0178e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.0870e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.6158e-01,  1.5751e-02, -1.0000e+09],\n",
            "          [-1.5262e-01,  1.4878e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.7817e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.9047e-02, -1.2176e-01, -1.0000e+09],\n",
            "          [ 2.1976e-02, -9.2123e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 4.1845e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-4.3214e-02, -6.0765e-02, -1.0000e+09],\n",
            "          [-1.9982e-02, -2.8097e-02, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 4.1916e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.5819e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.4081e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.1428e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.5271e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.0751e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.2333e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 5.4657e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.2510e-01, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[-8.8006e-05, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.0660e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.7455e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.5779e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.5559e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.9684e-01, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.0008e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-5.8209e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-8.2979e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 6.3016e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 6.1248e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.6818e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.6942e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [-7.6925e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 6.1453e-02, -1.0000e+09, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7722e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [-9.5929e-03, -2.5185e-02, -1.0000e+09],\n",
            "          [ 1.9084e-02,  5.0102e-02,  5.3115e-02]],\n",
            "\n",
            "         [[-1.0541e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-7.3408e-02, -2.4688e-01, -1.0000e+09],\n",
            "          [-8.3435e-02, -2.8061e-01, -3.8114e-01]],\n",
            "\n",
            "         [[ 2.9399e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.8114e-01,  6.1441e-01, -1.0000e+09],\n",
            "          [ 1.7040e-01,  2.1760e-01,  1.2335e-01]],\n",
            "\n",
            "         [[-6.6400e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 9.4474e-02,  4.9990e-03, -1.0000e+09],\n",
            "          [ 3.0656e-02,  1.6222e-03,  1.8561e-02]],\n",
            "\n",
            "         [[ 7.8938e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.4638e-01,  3.8785e-01, -1.0000e+09],\n",
            "          [ 1.0033e-01,  2.6585e-01,  2.2306e-01]],\n",
            "\n",
            "         [[ 2.9879e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.7117e-02, -1.0189e-01, -1.0000e+09],\n",
            "          [ 9.7892e-03, -5.8269e-02, -1.6420e-03]],\n",
            "\n",
            "         [[-6.7342e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-7.4318e-02, -8.9817e-02, -1.0000e+09],\n",
            "          [-5.2969e-02, -6.4016e-02, -7.6629e-02]],\n",
            "\n",
            "         [[ 7.6162e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 8.3498e-03,  1.2293e-02, -1.0000e+09],\n",
            "          [-2.5670e-02, -3.7792e-02, -1.5293e-03]]]], device='cuda:0',\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "scaled_scores: tensor([[[[ 3.5311e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.7107e-02,  5.6083e-02, -1.0000e+09],\n",
            "          [-4.8661e-03, -7.3547e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-1.6956e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.1094e-01, -3.0785e-01, -1.0000e+09],\n",
            "          [-1.3621e-01, -3.7798e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.4849e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.8538e-01,  3.9945e-01, -1.0000e+09],\n",
            "          [ 7.5319e-01,  7.8068e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.0070e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.5158e-02,  1.7432e-02, -1.0000e+09],\n",
            "          [-1.1117e-01, -4.2915e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.1702e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.3544e-01,  2.6214e-01, -1.0000e+09],\n",
            "          [ 1.0868e-01,  2.1035e-01, -1.0000e+09]],\n",
            "\n",
            "         [[ 6.6980e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 6.9339e-02,  2.5785e-02, -1.0000e+09],\n",
            "          [ 1.5103e-01,  5.6166e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-1.7036e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.6516e-01, -1.5675e-01, -1.0000e+09],\n",
            "          [-4.0614e-02, -3.8546e-02, -1.0000e+09]],\n",
            "\n",
            "         [[-4.5149e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-5.5502e-02, -6.3251e-02, -1.0000e+09],\n",
            "          [-3.2437e-02, -3.6965e-02, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[-1.1781e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.0191e-02,  2.7324e-03, -1.0000e+09],\n",
            "          [-1.0271e-02, -2.7537e-03, -1.0000e+09]],\n",
            "\n",
            "         [[-2.7198e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [-4.5027e-03, -1.9720e-01, -1.0000e+09],\n",
            "          [-1.7694e-03, -7.7495e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.1110e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.3397e-01,  5.4264e-01, -1.0000e+09],\n",
            "          [ 2.4444e-01,  3.9717e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.1456e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-2.5883e-02, -1.8641e-02, -1.0000e+09],\n",
            "          [ 4.1486e-02,  2.9879e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.0714e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.8207e-01,  9.7613e-02, -1.0000e+09],\n",
            "          [ 3.7637e-01,  2.0178e-01, -1.0000e+09]],\n",
            "\n",
            "         [[-1.0870e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.6158e-01,  1.5751e-02, -1.0000e+09],\n",
            "          [-1.5262e-01,  1.4878e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.7817e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.9047e-02, -1.2176e-01, -1.0000e+09],\n",
            "          [ 2.1976e-02, -9.2123e-02, -1.0000e+09]],\n",
            "\n",
            "         [[ 4.1845e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-4.3214e-02, -6.0765e-02, -1.0000e+09],\n",
            "          [-1.9982e-02, -2.8097e-02, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 4.1916e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.5819e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-1.4081e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.1428e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.5271e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.0751e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 2.2333e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 5.4657e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 3.2510e-01, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[-8.8006e-05, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.0660e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.7455e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.5779e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.5559e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 2.9684e-01, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 1.0008e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-5.8209e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-8.2979e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 6.3016e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 6.1248e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.6818e-02, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "         [[ 3.6942e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [-7.6925e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 6.1453e-02, -1.0000e+09, -1.0000e+09]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7722e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [-9.5929e-03, -2.5185e-02, -1.0000e+09],\n",
            "          [ 1.9084e-02,  5.0102e-02,  5.3115e-02]],\n",
            "\n",
            "         [[-1.0541e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [-7.3408e-02, -2.4688e-01, -1.0000e+09],\n",
            "          [-8.3435e-02, -2.8061e-01, -3.8114e-01]],\n",
            "\n",
            "         [[ 2.9399e-01, -1.0000e+09, -1.0000e+09],\n",
            "          [ 4.8114e-01,  6.1441e-01, -1.0000e+09],\n",
            "          [ 1.7040e-01,  2.1760e-01,  1.2335e-01]],\n",
            "\n",
            "         [[-6.6400e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 9.4474e-02,  4.9990e-03, -1.0000e+09],\n",
            "          [ 3.0656e-02,  1.6222e-03,  1.8561e-02]],\n",
            "\n",
            "         [[ 7.8938e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.4638e-01,  3.8785e-01, -1.0000e+09],\n",
            "          [ 1.0033e-01,  2.6585e-01,  2.2306e-01]],\n",
            "\n",
            "         [[ 2.9879e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 1.7117e-02, -1.0189e-01, -1.0000e+09],\n",
            "          [ 9.7892e-03, -5.8269e-02, -1.6420e-03]],\n",
            "\n",
            "         [[-6.7342e-02, -1.0000e+09, -1.0000e+09],\n",
            "          [-7.4318e-02, -8.9817e-02, -1.0000e+09],\n",
            "          [-5.2969e-02, -6.4016e-02, -7.6629e-02]],\n",
            "\n",
            "         [[ 7.6162e-03, -1.0000e+09, -1.0000e+09],\n",
            "          [ 8.3498e-03,  1.2293e-02, -1.0000e+09],\n",
            "          [-2.5670e-02, -3.7792e-02, -1.5293e-03]]]], device='cuda:0',\n",
            "       grad_fn=<DivBackward0>)\n",
            "softmax_scores: tensor([[[[1.0000, 0.0000, 0.0000],\n",
            "          [0.4953, 0.5047, 0.0000],\n",
            "          [0.5006, 0.4994, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5491, 0.4509, 0.0000],\n",
            "          [0.5602, 0.4398, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4965, 0.5035, 0.0000],\n",
            "          [0.4931, 0.5069, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5069, 0.4931, 0.0000],\n",
            "          [0.4829, 0.5171, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4684, 0.5316, 0.0000],\n",
            "          [0.4746, 0.5254, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5109, 0.4891, 0.0000],\n",
            "          [0.5237, 0.4763, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4979, 0.5021, 0.0000],\n",
            "          [0.4995, 0.5005, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5019, 0.4981, 0.0000],\n",
            "          [0.5011, 0.4989, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000],\n",
            "          [0.5019, 0.4981, 0.0000],\n",
            "          [0.4981, 0.5019, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5480, 0.4520, 0.0000],\n",
            "          [0.5189, 0.4811, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4480, 0.5520, 0.0000],\n",
            "          [0.4619, 0.5381, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4982, 0.5018, 0.0000],\n",
            "          [0.5029, 0.4971, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5211, 0.4789, 0.0000],\n",
            "          [0.5435, 0.4565, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4558, 0.5442, 0.0000],\n",
            "          [0.4582, 0.5418, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5376, 0.4624, 0.0000],\n",
            "          [0.5285, 0.4715, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5044, 0.4956, 0.0000],\n",
            "          [0.5020, 0.4980, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000],\n",
            "          [1.0000, 0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000],\n",
            "          [0.5039, 0.4961, 0.0000],\n",
            "          [0.3261, 0.3364, 0.3374]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5433, 0.4567, 0.0000],\n",
            "          [0.3901, 0.3203, 0.2896]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4667, 0.5333, 0.0000],\n",
            "          [0.3331, 0.3492, 0.3178]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5224, 0.4776, 0.0000],\n",
            "          [0.3379, 0.3282, 0.3338]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4399, 0.5601, 0.0000],\n",
            "          [0.3021, 0.3564, 0.3415]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5297, 0.4703, 0.0000],\n",
            "          [0.3421, 0.3196, 0.3382]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.5039, 0.4961, 0.0000],\n",
            "          [0.3372, 0.3335, 0.3293]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4990, 0.5010, 0.0000],\n",
            "          [0.3320, 0.3280, 0.3401]]]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "softmax_socres shape: torch.Size([4, 8, 3, 3])\n",
            "values: tensor([[[[-0.4421],\n",
            "          [-0.4828],\n",
            "          [-0.3752]],\n",
            "\n",
            "         [[ 0.6036],\n",
            "          [ 0.6242],\n",
            "          [ 0.8511]],\n",
            "\n",
            "         [[ 0.0412],\n",
            "          [ 0.0077],\n",
            "          [ 0.2528]],\n",
            "\n",
            "         [[ 0.7564],\n",
            "          [ 0.9165],\n",
            "          [ 1.0039]],\n",
            "\n",
            "         [[-0.1312],\n",
            "          [-0.2673],\n",
            "          [-0.2868]],\n",
            "\n",
            "         [[ 0.3612],\n",
            "          [ 0.2347],\n",
            "          [ 0.0306]],\n",
            "\n",
            "         [[-0.8842],\n",
            "          [-0.4901],\n",
            "          [-0.9032]],\n",
            "\n",
            "         [[-0.2133],\n",
            "          [-0.2283],\n",
            "          [-0.3304]]],\n",
            "\n",
            "\n",
            "        [[[-0.3210],\n",
            "          [-0.2037],\n",
            "          [-0.3132]],\n",
            "\n",
            "         [[ 0.0236],\n",
            "          [ 0.7791],\n",
            "          [ 0.3552]],\n",
            "\n",
            "         [[-0.2618],\n",
            "          [ 0.0232],\n",
            "          [-0.1934]],\n",
            "\n",
            "         [[ 0.6312],\n",
            "          [ 0.9531],\n",
            "          [ 0.8175]],\n",
            "\n",
            "         [[ 0.4339],\n",
            "          [-0.1309],\n",
            "          [ 0.1612]],\n",
            "\n",
            "         [[-0.1243],\n",
            "          [ 0.1641],\n",
            "          [-0.1817]],\n",
            "\n",
            "         [[-0.5422],\n",
            "          [-0.6978],\n",
            "          [-0.3557]],\n",
            "\n",
            "         [[-0.4952],\n",
            "          [-0.3138],\n",
            "          [-0.3588]]],\n",
            "\n",
            "\n",
            "        [[[-0.3404],\n",
            "          [-0.2503],\n",
            "          [-0.1635]],\n",
            "\n",
            "         [[-0.1434],\n",
            "          [ 0.5357],\n",
            "          [ 0.5229]],\n",
            "\n",
            "         [[-0.5624],\n",
            "          [-0.2688],\n",
            "          [-0.2555]],\n",
            "\n",
            "         [[ 0.8203],\n",
            "          [ 0.6791],\n",
            "          [ 0.6522]],\n",
            "\n",
            "         [[ 0.3699],\n",
            "          [ 0.1479],\n",
            "          [ 0.2772]],\n",
            "\n",
            "         [[ 0.3809],\n",
            "          [ 0.3575],\n",
            "          [ 0.0997]],\n",
            "\n",
            "         [[-0.5416],\n",
            "          [-0.6572],\n",
            "          [-0.3753]],\n",
            "\n",
            "         [[-0.3194],\n",
            "          [-0.0777],\n",
            "          [-0.1731]]],\n",
            "\n",
            "\n",
            "        [[[-0.1762],\n",
            "          [-0.0455],\n",
            "          [-0.2318]],\n",
            "\n",
            "         [[ 0.4854],\n",
            "          [ 0.5005],\n",
            "          [ 0.7248]],\n",
            "\n",
            "         [[-0.3959],\n",
            "          [-0.3573],\n",
            "          [-0.1602]],\n",
            "\n",
            "         [[ 0.8613],\n",
            "          [ 1.0814],\n",
            "          [ 0.7300]],\n",
            "\n",
            "         [[ 0.2179],\n",
            "          [ 0.1167],\n",
            "          [ 0.1899]],\n",
            "\n",
            "         [[ 0.5898],\n",
            "          [ 0.1046],\n",
            "          [ 0.2981]],\n",
            "\n",
            "         [[-0.6992],\n",
            "          [-0.5217],\n",
            "          [-0.4574]],\n",
            "\n",
            "         [[-0.0751],\n",
            "          [-0.3024],\n",
            "          [-0.1810]]]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
            "v_out shape torch.Size([4, 8, 3, 1])\n",
            "v_out: tensor([[[[-0.4421],\n",
            "          [-0.4627],\n",
            "          [-0.4624]],\n",
            "\n",
            "         [[ 0.6036],\n",
            "          [ 0.6129],\n",
            "          [ 0.6127]],\n",
            "\n",
            "         [[ 0.0412],\n",
            "          [ 0.0243],\n",
            "          [ 0.0242]],\n",
            "\n",
            "         [[ 0.7564],\n",
            "          [ 0.8353],\n",
            "          [ 0.8392]],\n",
            "\n",
            "         [[-0.1312],\n",
            "          [-0.2036],\n",
            "          [-0.2027]],\n",
            "\n",
            "         [[ 0.3612],\n",
            "          [ 0.2993],\n",
            "          [ 0.3009]],\n",
            "\n",
            "         [[-0.8842],\n",
            "          [-0.6864],\n",
            "          [-0.6870]],\n",
            "\n",
            "         [[-0.2133],\n",
            "          [-0.2208],\n",
            "          [-0.2208]]],\n",
            "\n",
            "\n",
            "        [[[-0.3210],\n",
            "          [-0.2626],\n",
            "          [-0.2621]],\n",
            "\n",
            "         [[ 0.0236],\n",
            "          [ 0.3651],\n",
            "          [ 0.3870]],\n",
            "\n",
            "         [[-0.2618],\n",
            "          [-0.1045],\n",
            "          [-0.1084]],\n",
            "\n",
            "         [[ 0.6312],\n",
            "          [ 0.7927],\n",
            "          [ 0.7912]],\n",
            "\n",
            "         [[ 0.4339],\n",
            "          [ 0.1634],\n",
            "          [ 0.1761]],\n",
            "\n",
            "         [[-0.1243],\n",
            "          [ 0.0326],\n",
            "          [ 0.0319]],\n",
            "\n",
            "         [[-0.5422],\n",
            "          [-0.6141],\n",
            "          [-0.6156]],\n",
            "\n",
            "         [[-0.4952],\n",
            "          [-0.4053],\n",
            "          [-0.4049]]],\n",
            "\n",
            "\n",
            "        [[[-0.3404],\n",
            "          [-0.3404],\n",
            "          [-0.3404]],\n",
            "\n",
            "         [[-0.1434],\n",
            "          [-0.1434],\n",
            "          [-0.1434]],\n",
            "\n",
            "         [[-0.5624],\n",
            "          [-0.5624],\n",
            "          [-0.5624]],\n",
            "\n",
            "         [[ 0.8203],\n",
            "          [ 0.8203],\n",
            "          [ 0.8203]],\n",
            "\n",
            "         [[ 0.3699],\n",
            "          [ 0.3699],\n",
            "          [ 0.3699]],\n",
            "\n",
            "         [[ 0.3809],\n",
            "          [ 0.3809],\n",
            "          [ 0.3809]],\n",
            "\n",
            "         [[-0.5416],\n",
            "          [-0.5416],\n",
            "          [-0.5416]],\n",
            "\n",
            "         [[-0.3194],\n",
            "          [-0.3194],\n",
            "          [-0.3194]]],\n",
            "\n",
            "\n",
            "        [[[-0.1762],\n",
            "          [-0.1114],\n",
            "          [-0.1510]],\n",
            "\n",
            "         [[ 0.4854],\n",
            "          [ 0.4923],\n",
            "          [ 0.5596]],\n",
            "\n",
            "         [[-0.3959],\n",
            "          [-0.3753],\n",
            "          [-0.3075]],\n",
            "\n",
            "         [[ 0.8613],\n",
            "          [ 0.9664],\n",
            "          [ 0.8897]],\n",
            "\n",
            "         [[ 0.2179],\n",
            "          [ 0.1612],\n",
            "          [ 0.1723]],\n",
            "\n",
            "         [[ 0.5898],\n",
            "          [ 0.3616],\n",
            "          [ 0.3361]],\n",
            "\n",
            "         [[-0.6992],\n",
            "          [-0.6111],\n",
            "          [-0.5604]],\n",
            "\n",
            "         [[-0.0751],\n",
            "          [-0.1890],\n",
            "          [-0.1857]]]], device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
            "v_out shape before: torch.Size([4, 8, 3, 1])\n",
            "v_out shape: torch.Size([4, 3, 8])\n",
            "v_out reshaped: tensor([[[-0.4421,  0.6036,  0.0412,  0.7564, -0.1312,  0.3612, -0.8842,\n",
            "          -0.2133],\n",
            "         [-0.4627,  0.6129,  0.0243,  0.8353, -0.2036,  0.2993, -0.6864,\n",
            "          -0.2208],\n",
            "         [-0.4624,  0.6127,  0.0242,  0.8392, -0.2027,  0.3009, -0.6870,\n",
            "          -0.2208]],\n",
            "\n",
            "        [[-0.3210,  0.0236, -0.2618,  0.6312,  0.4339, -0.1243, -0.5422,\n",
            "          -0.4952],\n",
            "         [-0.2626,  0.3651, -0.1045,  0.7927,  0.1634,  0.0326, -0.6141,\n",
            "          -0.4053],\n",
            "         [-0.2621,  0.3870, -0.1084,  0.7912,  0.1761,  0.0319, -0.6156,\n",
            "          -0.4049]],\n",
            "\n",
            "        [[-0.3404, -0.1434, -0.5624,  0.8203,  0.3699,  0.3809, -0.5416,\n",
            "          -0.3194],\n",
            "         [-0.3404, -0.1434, -0.5624,  0.8203,  0.3699,  0.3809, -0.5416,\n",
            "          -0.3194],\n",
            "         [-0.3404, -0.1434, -0.5624,  0.8203,  0.3699,  0.3809, -0.5416,\n",
            "          -0.3194]],\n",
            "\n",
            "        [[-0.1762,  0.4854, -0.3959,  0.8613,  0.2179,  0.5898, -0.6992,\n",
            "          -0.0751],\n",
            "         [-0.1114,  0.4923, -0.3753,  0.9664,  0.1612,  0.3616, -0.6111,\n",
            "          -0.1890],\n",
            "         [-0.1510,  0.5596, -0.3075,  0.8897,  0.1723,  0.3361, -0.5604,\n",
            "          -0.1857]]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
            "output shape torch.Size([4, 3, 8])\n",
            "output: tensor([[[-0.0486,  0.1622,  0.1717, -0.1525,  0.9936,  0.1495,  0.1620,\n",
            "          -0.2677],\n",
            "         [ 0.0280,  0.0798,  0.1360, -0.1548,  0.9620,  0.1101,  0.1115,\n",
            "          -0.2429],\n",
            "         [ 0.0288,  0.0791,  0.1343, -0.1543,  0.9636,  0.1094,  0.1115,\n",
            "          -0.2438]],\n",
            "\n",
            "        [[-0.2540,  0.2457,  0.0244,  0.0699,  0.5358, -0.0759,  0.1166,\n",
            "           0.1651],\n",
            "         [-0.0986,  0.1277,  0.0235, -0.0235,  0.7655, -0.0018,  0.1589,\n",
            "           0.0350],\n",
            "         [-0.1027,  0.1339,  0.0267, -0.0218,  0.7709, -0.0025,  0.1684,\n",
            "           0.0455]],\n",
            "\n",
            "        [[-0.1814,  0.1470, -0.0231,  0.0748,  0.7125, -0.1557,  0.1305,\n",
            "          -0.0450],\n",
            "         [-0.1814,  0.1470, -0.0231,  0.0748,  0.7125, -0.1557,  0.1305,\n",
            "          -0.0450],\n",
            "         [-0.1814,  0.1470, -0.0231,  0.0748,  0.7125, -0.1557,  0.1305,\n",
            "          -0.0450]],\n",
            "\n",
            "        [[-0.0696,  0.1057,  0.0739,  0.0048,  1.0051, -0.0301,  0.3081,\n",
            "          -0.1054],\n",
            "         [-0.0041,  0.0340,  0.0182,  0.0061,  0.9603, -0.0809,  0.2803,\n",
            "          -0.0106],\n",
            "         [-0.0248,  0.0588,  0.0418, -0.0035,  0.9326, -0.0448,  0.2755,\n",
            "           0.0097]]], device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, feature_length):\n",
        "        super().__init__()\n",
        "        self.scale = nn.Parameter(torch.ones(feature_length))\n",
        "        self.offset = nn.Parameter(torch.zeros(feature_length))\n",
        "\n",
        "    def forward(self, activations):\n",
        "        mean = torch.mean(activations, -1, keepdim=True)\n",
        "        #print(\"mean:\", mean)\n",
        "        #print(\"activations - mean\", activations - mean)\n",
        "        variance = torch.var(activations, -1, keepdim=True, unbiased=False)\n",
        "        normalized_activations = (activations - mean) / torch.sqrt(variance + 1e-6)\n",
        "        return (normalized_activations * self.scale) + self.offset"
      ],
      "metadata": {
        "id": "-Si4-i6PTlFu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_layer_norm():\n",
        "    feature_length = 4\n",
        "    length_x = 3\n",
        "    batch_size = 5\n",
        "    layer_norm = LayerNorm(feature_length)\n",
        "\n",
        "    activations = torch.rand(batch_size, length_x, feature_length)\n",
        "\n",
        "    print(\"activations:\", activations)\n",
        "    print(\"layer_normed:\", layer_norm(activations))\n",
        "    assert layer_norm(activations).shape == activations.shape\n",
        "\n",
        "test_layer_norm()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyzgWHqrWXL4",
        "outputId": "5aa7af91-d7ab-43c8-f6bb-890c13cd1cee"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "activations: tensor([[[0.9585, 0.8650, 0.0683, 0.0543],\n",
            "         [0.5648, 0.4272, 0.2048, 0.5931],\n",
            "         [0.2845, 0.4615, 0.7539, 0.5990]],\n",
            "\n",
            "        [[0.1542, 0.8416, 0.1384, 0.8494],\n",
            "         [0.0991, 0.7173, 0.6886, 0.2655],\n",
            "         [0.8548, 0.9399, 0.1831, 0.6274]],\n",
            "\n",
            "        [[0.1678, 0.5979, 0.4609, 0.9057],\n",
            "         [0.7310, 0.5347, 0.3886, 0.9706],\n",
            "         [0.3451, 0.1739, 0.4810, 0.4873]],\n",
            "\n",
            "        [[0.2569, 0.6881, 0.7672, 0.9335],\n",
            "         [0.6350, 0.1567, 0.7177, 0.5660],\n",
            "         [0.8676, 0.5293, 0.5051, 0.2700]],\n",
            "\n",
            "        [[0.7937, 0.1208, 0.8246, 0.1129],\n",
            "         [0.9325, 0.3642, 0.3760, 0.2328],\n",
            "         [0.2098, 0.6366, 0.3749, 0.1389]]])\n",
            "layer_normed: tensor([[[ 1.1065,  0.8873, -0.9804, -1.0134],\n",
            "         [ 0.7645, -0.1323, -1.5806,  0.9485],\n",
            "         [-1.3885, -0.3652,  1.3244,  0.4293]],\n",
            "\n",
            "        [[-0.9773,  0.9887, -1.0224,  1.0110],\n",
            "         [-1.2861,  1.0284,  0.9210, -0.6633],\n",
            "         [ 0.6936,  0.9833, -1.5954, -0.0815]],\n",
            "\n",
            "        [[-1.3766,  0.2443, -0.2718,  1.4041],\n",
            "         [ 0.3424, -0.5563, -1.2254,  1.4393],\n",
            "         [-0.2093, -1.5510,  0.8555,  0.9047]],\n",
            "\n",
            "        [[-1.6195,  0.1067,  0.4235,  1.0892],\n",
            "         [ 0.5382, -1.6776,  0.9210,  0.2185],\n",
            "         [ 1.5237, -0.0642, -0.1778, -1.2817]],\n",
            "\n",
            "        [[ 0.9548, -0.9881,  1.0442, -1.0109],\n",
            "         [ 1.6939, -0.4165, -0.3728, -0.9046],\n",
            "         [-0.6807,  1.5491,  0.1822, -1.0506]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, hiddenLayerWidth, d_e, p_dropout):\n",
        "        super().__init__()\n",
        "        self.mlp1 = nn.Parameter(torch.rand(d_e, hiddenLayerWidth))\n",
        "        self.mlp2 = nn.Parameter(torch.rand(hiddenLayerWidth, d_e))\n",
        "        self.mlp1_bias = nn.Parameter(torch.zeros(hiddenLayerWidth))\n",
        "        self.mlp2_bias = nn.Parameter(torch.zeros(d_e))\n",
        "        self.dropout = nn.Dropout(p_dropout)\n",
        "\n",
        "    def forward(self, activations):\n",
        "        activations = torch.matmul(activations, self.mlp1) + self.mlp1_bias\n",
        "        activations = activations.relu()\n",
        "        activations = torch.matmul(activations, self.mlp2) + self.mlp2_bias\n",
        "        activations = self.dropout(activations)\n",
        "        return activations\n"
      ],
      "metadata": {
        "id": "X7rAEAkFoNI5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_feed_forward():\n",
        "    hiddenLayerWidth = 3\n",
        "    d_e = 4\n",
        "    feed_forward = FeedForward(hiddenLayerWidth, d_e, 0.1)\n",
        "    activations = torch.rand(10, 5, d_e)\n",
        "\n",
        "    print(\"activations:\", activations)\n",
        "    output = feed_forward(activations)\n",
        "    print(\"feed forward:\", output)\n",
        "    assert output.shape == activations.shape\n",
        "\n",
        "test_feed_forward()"
      ],
      "metadata": {
        "id": "mLQj6GjmUFN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e5dfa41-1ae8-4853-fbe2-d5f66f6563f9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "activations: tensor([[[0.7162, 0.8391, 0.7896, 0.7189],\n",
            "         [0.4911, 0.8424, 0.8687, 0.0647],\n",
            "         [0.5847, 0.6227, 0.4764, 0.1564],\n",
            "         [0.2567, 0.7449, 0.0281, 0.6179],\n",
            "         [0.3166, 0.7580, 0.0795, 0.9892]],\n",
            "\n",
            "        [[0.3658, 0.3713, 0.9095, 0.2346],\n",
            "         [0.4220, 0.1864, 0.4479, 0.7047],\n",
            "         [0.4035, 0.5706, 0.9836, 0.7179],\n",
            "         [0.6039, 0.7702, 0.2363, 0.8699],\n",
            "         [0.3465, 0.5148, 0.4237, 0.1254]],\n",
            "\n",
            "        [[0.5822, 0.1521, 0.6643, 0.8174],\n",
            "         [0.0329, 0.2912, 0.9359, 0.8245],\n",
            "         [0.2528, 0.2244, 0.9205, 0.2973],\n",
            "         [0.2273, 0.3442, 0.5416, 0.2200],\n",
            "         [0.6495, 0.2182, 0.4325, 0.9734]],\n",
            "\n",
            "        [[0.3090, 0.1305, 0.4369, 0.2272],\n",
            "         [0.3768, 0.0799, 0.2608, 0.3879],\n",
            "         [0.5184, 0.3851, 0.2945, 0.7441],\n",
            "         [0.4496, 0.0372, 0.6992, 0.9133],\n",
            "         [0.5403, 0.3095, 0.7622, 0.7765]],\n",
            "\n",
            "        [[0.7340, 0.5315, 0.3123, 0.1216],\n",
            "         [0.5905, 0.7827, 0.5246, 0.2818],\n",
            "         [0.3251, 0.9486, 0.7465, 0.8361],\n",
            "         [0.2152, 0.9253, 0.1532, 0.9265],\n",
            "         [0.2236, 0.7171, 0.8762, 0.0173]],\n",
            "\n",
            "        [[0.8524, 0.3550, 0.3433, 0.9243],\n",
            "         [0.8896, 0.5362, 0.7230, 0.5878],\n",
            "         [0.9095, 0.1898, 0.3825, 0.5456],\n",
            "         [0.3005, 0.6267, 0.9540, 0.6107],\n",
            "         [0.5523, 0.0066, 0.2676, 0.8121]],\n",
            "\n",
            "        [[0.9950, 0.5809, 0.3101, 0.2751],\n",
            "         [0.7424, 0.8780, 0.5843, 0.2509],\n",
            "         [0.9813, 0.0220, 0.9977, 0.5921],\n",
            "         [0.8650, 0.0410, 0.4478, 0.3561],\n",
            "         [0.9486, 0.0837, 0.5711, 0.3485]],\n",
            "\n",
            "        [[0.4403, 0.4605, 0.8871, 0.5350],\n",
            "         [0.1355, 0.8206, 0.3544, 0.7126],\n",
            "         [0.6834, 0.8336, 0.2763, 0.3298],\n",
            "         [0.4444, 0.8771, 0.9834, 0.1246],\n",
            "         [0.5706, 0.4069, 0.6391, 0.0773]],\n",
            "\n",
            "        [[0.1389, 0.9018, 0.4024, 0.0963],\n",
            "         [0.7001, 0.1936, 0.8411, 0.3009],\n",
            "         [0.3786, 0.3271, 0.6998, 0.0040],\n",
            "         [0.6517, 0.6713, 0.5993, 0.2971],\n",
            "         [0.9258, 0.1954, 0.3357, 0.1849]],\n",
            "\n",
            "        [[0.7486, 0.9722, 0.1511, 0.8022],\n",
            "         [0.8970, 0.7180, 0.6911, 0.3254],\n",
            "         [0.2980, 0.8007, 0.2312, 0.7990],\n",
            "         [0.9744, 0.0737, 0.7119, 0.0933],\n",
            "         [0.4882, 0.1985, 0.4847, 0.8580]]])\n",
            "feed forward: tensor([[[3.4194, 2.5743, 2.5988, 1.7923],\n",
            "         [2.8555, 2.1619, 2.1881, 1.4603],\n",
            "         [2.3472, 1.7681, 1.7770, 1.2803],\n",
            "         [1.5843, 1.1892, 1.1950, 0.8654],\n",
            "         [1.9157, 1.4348, 0.0000, 1.0266]],\n",
            "\n",
            "        [[2.2165, 0.0000, 1.7073, 1.0750],\n",
            "         [1.7578, 1.3173, 1.3321, 0.9077],\n",
            "         [2.7950, 2.1107, 2.1484, 1.3534],\n",
            "         [2.5625, 1.9202, 1.9279, 1.4099],\n",
            "         [1.7360, 1.3109, 1.3227, 0.9140]],\n",
            "\n",
            "        [[2.2954, 0.0000, 1.7409, 1.1818],\n",
            "         [1.8200, 0.0000, 0.0000, 0.7562],\n",
            "         [1.8908, 1.4327, 0.0000, 0.8712],\n",
            "         [1.5100, 1.1423, 1.1613, 0.7398],\n",
            "         [2.2818, 1.7049, 0.0000, 1.2262]],\n",
            "\n",
            "        [[1.2886, 0.9703, 0.9822, 0.6590],\n",
            "         [1.2092, 0.9044, 0.9093, 0.6572],\n",
            "         [1.9898, 1.4892, 0.0000, 1.0804],\n",
            "         [2.0137, 1.5101, 1.5347, 0.9910],\n",
            "         [2.4999, 1.8793, 1.9044, 1.2652]],\n",
            "\n",
            "        [[2.2965, 1.7240, 1.7222, 1.3190],\n",
            "         [2.6480, 1.9955, 2.0083, 1.4266],\n",
            "         [2.8631, 2.1620, 2.1964, 1.4130],\n",
            "         [1.9910, 1.4968, 1.5126, 0.0000],\n",
            "         [2.2369, 1.7006, 1.7324, 1.0728]],\n",
            "\n",
            "        [[2.6664, 1.9910, 1.9946, 1.4954],\n",
            "         [3.2425, 2.4353, 2.4498, 1.7547],\n",
            "         [2.4667, 1.8425, 1.8412, 1.4126],\n",
            "         [2.6016, 1.9682, 2.0066, 1.2391],\n",
            "         [1.6125, 1.1999, 1.2047, 0.8877]],\n",
            "\n",
            "        [[2.8668, 2.1474, 2.1409, 1.6728],\n",
            "         [3.0777, 2.3184, 2.3300, 1.6790],\n",
            "         [0.0000, 2.3528, 2.3721, 1.6638],\n",
            "         [2.2176, 1.6574, 1.6567, 1.2668],\n",
            "         [2.5522, 1.9102, 1.9122, 1.4401]],\n",
            "\n",
            "        [[2.5452, 1.9212, 1.9518, 1.2559],\n",
            "         [1.8795, 1.4187, 1.4411, 0.9288],\n",
            "         [2.5977, 1.9519, 1.9540, 1.4656],\n",
            "         [2.9724, 2.2524, 0.0000, 1.4874],\n",
            "         [2.2330, 1.6838, 1.6969, 1.1886]],\n",
            "\n",
            "        [[1.7811, 1.3523, 1.3705, 0.8990],\n",
            "         [2.5415, 1.9127, 0.0000, 1.3308],\n",
            "         [1.8489, 1.3992, 1.4181, 0.9338],\n",
            "         [0.0000, 2.0488, 2.0623, 1.4631],\n",
            "         [2.2988, 0.0000, 1.7116, 1.3538]],\n",
            "\n",
            "        [[2.9177, 0.0000, 2.1877, 1.6510],\n",
            "         [3.3182, 2.4960, 2.5075, 1.8163],\n",
            "         [2.0310, 1.5267, 1.5413, 1.0644],\n",
            "         [2.6449, 1.9843, 1.9882, 1.4808],\n",
            "         [1.9925, 1.4920, 1.5081, 1.0325]]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.multi_head_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], p_dropout, verbose)\n",
        "        self.layer_norm1 = LayerNorm(d_z)\n",
        "        self.feed_forward = FeedForward(d_mlp, d_z, p_dropout)\n",
        "        self.layer_norm2 = LayerNorm(d_z)\n",
        "\n",
        "    def forward(self, z, padding_mask):\n",
        "        z = self.layer_norm1(z)\n",
        "        z = z + self.multi_head_attention(z, z, padding_mask)\n",
        "        z = self.layer_norm2(z)\n",
        "        z = z + self.feed_forward(z)\n",
        "        return z"
      ],
      "metadata": {
        "id": "ikM15oD-qghT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        for i in range(num_layers):\n",
        "            encoder_layer = EncoderLayer(num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose)\n",
        "            self.layers.append(encoder_layer)\n",
        "        self.layers = nn.ModuleList(self.layers)\n",
        "        self.final_norm = LayerNorm(d_z)\n",
        "\n",
        "    def forward(self, z, padding_mask):\n",
        "        for layer in self.layers:\n",
        "            z = layer(z, padding_mask)\n",
        "        return self.final_norm(z)"
      ],
      "metadata": {
        "id": "zKGc6Xwr46Vh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.multi_head_self_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['MASKED'], p_dropout, verbose)\n",
        "        self.layer_norm1 = LayerNorm(d_x)\n",
        "        self.multi_head_global_attention = MultiHeadedAttention(num_heads, d_attn, d_x, d_z, d_out, d_mid, MaskStrategy['UNMASKED'], p_dropout, verbose)\n",
        "        self.layer_norm2 = LayerNorm(d_x)\n",
        "        self.feed_forward = FeedForward(d_mlp, d_x, p_dropout)\n",
        "        self.layer_norm3 = LayerNorm(d_x)\n",
        "\n",
        "    def forward(self, z, x, src_mask, tgt_mask):\n",
        "        x = self.layer_norm1(x)\n",
        "        x = x + self.multi_head_self_attention(x, x, tgt_mask)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = x + self.multi_head_global_attention(z, x, src_mask)\n",
        "        x = self.layer_norm3(x)\n",
        "        x = x + self.feed_forward(x)\n",
        "        return x\n",
        "\n",
        "    def disable_subsequent_mask(self):\n",
        "        self.multi_head_self_attention.disable_subsequent_mask()"
      ],
      "metadata": {
        "id": "GSqElGm56xaK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        for i in range(num_layers):\n",
        "            decoder_layer = DecoderLayer(num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose)\n",
        "            self.layers.append(decoder_layer)\n",
        "        self.layers = nn.ModuleList(self.layers)\n",
        "        self.final_norm = LayerNorm(d_x)\n",
        "\n",
        "    def forward(self, z, x, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(z, x, src_mask, tgt_mask)\n",
        "        return self.final_norm(x)\n",
        "\n",
        "    def disable_subsequent_mask(self):\n",
        "        for layer in self.layers:\n",
        "            layer.multi_head_self_attention.disable_subsequent_mask()"
      ],
      "metadata": {
        "id": "LGX007WN8Bw6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoderTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, d_e, vocab_size, max_sequence_length, p_dropout, verbose):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.src_embedding = Embedding(vocab_size, d_e)\n",
        "        self.tgt_embedding = Embedding(vocab_size, d_e)\n",
        "        self.unembedding = Unembedding(vocab_size, d_e)\n",
        "        self.embedding_dropout = nn.Dropout(p_dropout)\n",
        "        self.positionalEmbedding = PositionalEmbedding(d_e, max_sequence_length)\n",
        "        self.encoder = Encoder(num_encoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose)\n",
        "        self.decoder = Decoder(num_decoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, p_dropout, verbose)\n",
        "\n",
        "    def forward(self, z, x, src_mask, tgt_mask):\n",
        "        z = self.src_embedding(z) + self.positionalEmbedding(z)\n",
        "        z = self.embedding_dropout(z)\n",
        "        z = self.encoder(z, src_mask)\n",
        "        x = self.tgt_embedding(x) + self.positionalEmbedding(x)\n",
        "        x = self.decoder(z, x, src_mask, tgt_mask)\n",
        "        #print(\"x after decoder:\", x.shape)\n",
        "        x = self.unembedding(x)\n",
        "        #print(\"x after unembedding:\", x.shape)\n",
        "        return x\n",
        "\n",
        "    def disable_subsequent_mask(self):\n",
        "        self.decoder.disable_subsequent_mask()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ixXJDrPF8RU9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enRawName = \"drive/MyDrive/colab data/multi30kEnTrain.txt\"\n",
        "deRawName = \"drive/MyDrive/colab data/multi30kDeTrain.txt\"\n",
        "en30kVal = \"drive/MyDrive/colab data/multi30kEnVal.txt\"\n",
        "de30kVal = \"drive/MyDrive/colab data/multi30kDeVal.txt\"\n",
        "englishCleanName = \"data/english_tokens.pkl\"\n",
        "germanCleanName = \"data/german_tokens.pkl\"\n",
        "englishSortedName = \"data/englishSorted.pkl\"\n",
        "germanSortedName = \"data/germanSorted.pkl\"\n",
        "\n",
        "truncEn = \"drive/MyDrive/colab data/truncEn.pkl\"\n",
        "truncDe = \"drive/MyDrive/colab data/truncDe.pkl\"\n",
        "\n",
        "enTokenizerName = \"drive/MyDrive/colab data/enTokenizer.pkl\"\n",
        "deTokenizerName = \"drive/MyDrive/colab data/deTokenizer.pkl\"\n",
        "pairsName = \"drive/MyDrive/colab data/pairs.pkl\"\n",
        "folder = \"drive/MyDrive/colab data/\"\n",
        "\n",
        "enTrainingFileName = folder + \"enTraining\"\n",
        "deTrainingFileName = folder + \"deTraining\"\n",
        "enTestFileName = folder + \"enTest\"\n",
        "deTestFileName = folder + \"deTest\"\n",
        "enValFileName = folder + \"enValidation\"\n",
        "deValFileName = folder + \"deValidation\"\n",
        "\n",
        "enCombinedFileName = folder + \"enCombined\"\n",
        "deCombinedFileName = folder + \"deCombined\""
      ],
      "metadata": {
        "id": "NqAcQrmPg4y0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc6Y11QfhEAv",
        "outputId": "5d49f0e2-dcbb-436a-e499-bfd247885a8a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class SentenceDataset(Dataset):\n",
        "\n",
        "#     TOKENIZER_SUFFIX = \"_tokenizer\"\n",
        "#     BOS_TOKEN = \"[SOS]\"\n",
        "#     EOS_TOKEN = \"[EOS]\"\n",
        "#     PAD_TOKEN = \"[PAD]\"\n",
        "#     UNK_TOKEN = \"[UNK]\"\n",
        "\n",
        "#     def __init__(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, sequence_start_index, sequence_end_index):\n",
        "#         src_sequences = self.to_sequences(self.load_doc(src_filename), sequence_start_index, sequence_end_index)\n",
        "#         tgt_sequences = self.to_sequences(self.load_doc(tgt_filename), sequence_start_index, sequence_end_index)\n",
        "#         src_sequences = [self.add_special_tokens(sequence) for sequence in src_sequences]\n",
        "#         tgt_sequences = [self.add_special_tokens(sequence) for sequence in tgt_sequences]\n",
        "#         self.src_tokenizer, self.tgt_tokenizer = self.setup_tokenizers(src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, src_filename + SentenceDataset.TOKENIZER_SUFFIX, tgt_filename + SentenceDataset.TOKENIZER_SUFFIX)\n",
        "#         # src_tokenized = self.src_tokenizer.encode_batch(src_sequences)\n",
        "#         # tgt_tokenized = self.tgt_tokenizer.encode_batch(tgt_sequences)\n",
        "#         # src_tensors = [torch.IntTensor(sequence.ids) for sequence in src_tokenized]\n",
        "#         # tgt_tensor = [torch.IntTensor(sequence.ids) for sequence in tgt_tokenized]\n",
        "#         self.pairs = self.pair_sequences(src_sequences, tgt_sequences)\n",
        "#         #print(\"pairs\", self.pairs)\n",
        "\n",
        "#     # load doc into memory\n",
        "#     def load_doc(self, filename):\n",
        "#         # open the file as read only\n",
        "#         file = open(filename, mode='rt')\n",
        "#         # read all text\n",
        "#         text = file.read()\n",
        "#         # close the file\n",
        "#         file.close()\n",
        "#         return text\n",
        "\n",
        "#     def add_special_tokens(self, sequence):\n",
        "#         sequence = self.BOS_TOKEN + \" \" + sequence + \" \" + self.EOS_TOKEN\n",
        "#         return sequence\n",
        "\n",
        "#     def pair_sequences(self, src_sequences, tgt_sequences):\n",
        "#         paired_sequences = list(zip(src_sequences, tgt_sequences))\n",
        "#         sorted_pairs = sorted(paired_sequences, key=lambda x: len(x[0]))\n",
        "#         return sorted_pairs\n",
        "\n",
        "#     # split a loaded document into sequences\n",
        "#     def to_sequences(self, doc, sequence_start_index, sequence_end_index):\n",
        "#         sequences = doc.strip().split('\\n')\n",
        "#         return sequences[sequence_start_index:sequence_end_index]\n",
        "\n",
        "#     def setup_tokenizers(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, src_tokenizer_name, tgt_tokenizer_name):\n",
        "#         print(\"creating tokenizer for \" + src_filename)\n",
        "#         src_tokenizer = Tokenizer(BPE(unk_token=SentenceDataset.UNK_TOKEN))\n",
        "#         src_tokenizer.pre_tokenizer = Whitespace()\n",
        "#         # src_tokenizer.post_processor = TemplateProcessing(\n",
        "#         #     single=\"[BOS] $A [EOS]\",\n",
        "#         #     special_tokens=[(\"[BOS]\", 0), (\"[EOS]\", 1)],\n",
        "#         # )\n",
        "#         trainer = BpeTrainer(vocab_size = src_vocab_size, special_tokens=[SentenceDataset.BOS_TOKEN, SentenceDataset.EOS_TOKEN, SentenceDataset.PAD_TOKEN, SentenceDataset.UNK_TOKEN])\n",
        "#         src_tokenizer.train([src_filename], trainer=trainer)\n",
        "#         pickle.dump(src_tokenizer, open(src_tokenizer_name, \"wb\"))\n",
        "\n",
        "#         print(\"creating tokenizer for \" + tgt_filename)\n",
        "#         tgt_tokenizer = Tokenizer(BPE(unk_token=SentenceDataset.UNK_TOKEN))\n",
        "#         tgt_tokenizer.pre_tokenizer = Whitespace()\n",
        "#         trainer = BpeTrainer(vocab_size = tgt_vocab_size, special_tokens=[SentenceDataset.BOS_TOKEN, SentenceDataset.EOS_TOKEN, SentenceDataset.PAD_TOKEN, SentenceDataset.UNK_TOKEN])\n",
        "#         tgt_tokenizer.train([tgt_filename], trainer=trainer)\n",
        "#         pickle.dump(tgt_tokenizer, open(tgt_tokenizer_name, \"wb\"))\n",
        "#         return src_tokenizer, tgt_tokenizer\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.pairs)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         src_seq, tgt_seq = self.pairs[index]\n",
        "#         return src_seq, tgt_seq\n"
      ],
      "metadata": {
        "id": "PkCWRs4-khoT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SequencePairDataset(Dataset):\n",
        "    BOS_TOKEN = \"[SOS]\"\n",
        "    EOS_TOKEN = \"[EOS]\"\n",
        "    PAD_TOKEN = \"[PAD]\"\n",
        "    UNK_TOKEN = \"[UNK]\"\n",
        "    PAD_ID = 2\n",
        "\n",
        "    def __init__(self, src_text, tgt_text, start_index, end_index):\n",
        "        src_sequences = self.to_sequences(src_text, start_index, end_index)\n",
        "        #tgt_sequences = self.to_sequences(tgt_text, start_index, end_index)\n",
        "        tgt_sequences = self.to_sequences(src_text, start_index, end_index)\n",
        "        #src_sequences = [self.add_special_tokens(sequence) for sequence in src_sequences]\n",
        "        #tgt_sequences = [self.add_special_tokens(sequence) for sequence in tgt_sequences]\n",
        "        self.pairs = self.pair_sequences(src_sequences, tgt_sequences)\n",
        "\n",
        "    def pair_sequences(self, src_sequences, tgt_sequences):\n",
        "        paired_sequences = list(zip(src_sequences, tgt_sequences))\n",
        "        sorted_pairs = sorted(paired_sequences, key=lambda x: len(x[0]))\n",
        "        return sorted_pairs\n",
        "\n",
        "    # split a loaded document into sequences\n",
        "    def to_sequences(self, doc, sequence_start_index, sequence_end_index):\n",
        "        sequences = doc.strip().split('\\n')\n",
        "        return sequences[sequence_start_index : sequence_end_index]\n",
        "\n",
        "    def add_special_tokens(self, sequence):\n",
        "        sequence = self.BOS_TOKEN + \" \" + sequence + \" \" + self.EOS_TOKEN\n",
        "        return sequence\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        src_seq, tgt_seq = self.pairs[index]\n",
        "        return src_seq, tgt_seq"
      ],
      "metadata": {
        "id": "vVeybRtnKtJX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainAndValidationSequenceDatasets():\n",
        "    def __init__(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, train_start_index, train_end_index, val_start_index, val_end_index):\n",
        "        src_text = self.load_doc(src_filename)\n",
        "        tgt_text = self.load_doc(tgt_filename)\n",
        "        self.train_dataset = SequencePairDataset(src_text, tgt_text, train_start_index, train_end_index)\n",
        "        self.val_dataset = SequencePairDataset(src_text, tgt_text, val_start_index, val_end_index)\n",
        "\n",
        "        # load doc into memory\n",
        "    def load_doc(self, filename):\n",
        "        # open the file as read only\n",
        "        file = open(filename, mode='rt')\n",
        "        # read all text\n",
        "        text = file.read()\n",
        "        # close the file\n",
        "        file.close()\n",
        "        return text"
      ],
      "metadata": {
        "id": "IJciEqbpJajo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "class PadCollate:\n",
        "    TOKENIZER_SUFFIX = \"_tokenizer\"\n",
        "\n",
        "    def __init__(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size):\n",
        "        self.src_tokenizer, self.tgt_tokenizer = self.setup_tokenizers(src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, src_filename + self.TOKENIZER_SUFFIX, tgt_filename + self.TOKENIZER_SUFFIX)\n",
        "\n",
        "    def setup_tokenizers(self, src_filename, tgt_filename, src_vocab_size, tgt_vocab_size, src_tokenizer_name, tgt_tokenizer_name):\n",
        "        print(\"creating tokenizer for \" + src_filename)\n",
        "        src_tokenizer = Tokenizer(BPE(unk_token=SequencePairDataset.UNK_TOKEN))\n",
        "        src_tokenizer.pre_tokenizer = Whitespace()\n",
        "        # src_tokenizer.post_processor = TemplateProcessing(\n",
        "        #     single=\"[BOS] $A [EOS]\",\n",
        "        #     special_tokens=[(\"[BOS]\", 0), (\"[EOS]\", 1)],\n",
        "        # )\n",
        "        trainer = BpeTrainer(vocab_size = src_vocab_size, special_tokens=[SequencePairDataset.BOS_TOKEN, SequencePairDataset.EOS_TOKEN, SequencePairDataset.PAD_TOKEN, SequencePairDataset.UNK_TOKEN])\n",
        "        src_tokenizer.train([src_filename], trainer=trainer)\n",
        "        pickle.dump(src_tokenizer, open(src_tokenizer_name, \"wb\"))\n",
        "\n",
        "        print(\"creating tokenizer for \" + tgt_filename)\n",
        "        tgt_tokenizer = Tokenizer(BPE(unk_token=SequencePairDataset.UNK_TOKEN))\n",
        "        tgt_tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = BpeTrainer(vocab_size = tgt_vocab_size, special_tokens=[SequencePairDataset.BOS_TOKEN, SequencePairDataset.EOS_TOKEN, SequencePairDataset.PAD_TOKEN, SequencePairDataset.UNK_TOKEN])\n",
        "        tgt_tokenizer.train([tgt_filename], trainer=trainer)\n",
        "        tgt_tokenizer = copy.deepcopy(src_tokenizer)\n",
        "        tgt_tokenizer.post_processor = TemplateProcessing(\n",
        "            single=\"[BOS] $A [EOS]\",\n",
        "            special_tokens=[(\"[BOS]\", 0), (\"[EOS]\", 1)],\n",
        "        )\n",
        "        pickle.dump(tgt_tokenizer, open(tgt_tokenizer_name, \"wb\"))\n",
        "        return src_tokenizer, tgt_tokenizer\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # max_len_src = max([len(pair[0].split()) for pair in batch])\n",
        "        # max_len_tgt = max([len(pair[1].split()) for pair in batch])\n",
        "\n",
        "        #tgt_sequence_lengths\n",
        "\n",
        "        self.src_tokenizer.no_padding()\n",
        "        self.tgt_tokenizer.no_padding()\n",
        "\n",
        "        self.src_tokenizer.no_truncation()\n",
        "        self.tgt_tokenizer.no_truncation()\n",
        "\n",
        "        src_tokenized = self.src_tokenizer.encode_batch([pair[0] for pair in batch])\n",
        "        tgt_tokenized = self.tgt_tokenizer.encode_batch([pair[1] for pair in batch])\n",
        "\n",
        "        max_len_src = max([len(sequence) for sequence in src_tokenized])\n",
        "        max_len_tgt = max([len(sequence) for sequence in tgt_tokenized])\n",
        "\n",
        "        # print(\"max len src:\", max_len_src)\n",
        "        # print(\"max len tgt:\", max_len_tgt)\n",
        "\n",
        "        self.src_tokenizer.enable_padding(pad_id = SequencePairDataset.PAD_ID, pad_token = SequencePairDataset.PAD_TOKEN)\n",
        "        self.src_tokenizer.enable_truncation(max_length=max_len_src)\n",
        "        self.tgt_tokenizer.enable_padding(pad_id = SequencePairDataset.PAD_ID, pad_token = SequencePairDataset.PAD_TOKEN)\n",
        "        self.tgt_tokenizer.enable_truncation(max_length=max_len_tgt)\n",
        "\n",
        "        # print(\"src batch:\", [pair[0] for pair in batch])\n",
        "        # print(\"tgt batch:\", [pair[1] for pair in batch])\n",
        "\n",
        "        src_tokenized = self.src_tokenizer.encode_batch([pair[0] for pair in batch])\n",
        "        tgt_tokenized = self.tgt_tokenizer.encode_batch([pair[1] for pair in batch])\n",
        "        # src_tokenized = [sequence.ids for sequence in src_tokenized]\n",
        "        # tgt_tokenized = [sequence.ids for sequence in tgt_tokenized]\n",
        "        # src_tensors = torch.IntTensor(src_tokenized)\n",
        "        # tgt_tensor = torch.IntTensor(tgt_tokenized)\n",
        "\n",
        "        return src_tokenized, tgt_tokenized"
      ],
      "metadata": {
        "id": "P1wzP-DDLZ1X"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_encoder_layers = 4\n",
        "num_decoder_layers = 4\n",
        "num_heads = 8\n",
        "d_attn = 32\n",
        "d_x = 256\n",
        "d_z = 256\n",
        "d_out = 256\n",
        "d_mid = 256\n",
        "d_mlp = 512\n",
        "d_e = 256\n",
        "vocab_size = 10000\n",
        "max_sequence_length = 100\n",
        "p_dropout = 0.1"
      ],
      "metadata": {
        "id": "vFPxq_hkffx3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_validation_sequence_datasets = TrainAndValidationSequenceDatasets(enRawName, enRawName, vocab_size, vocab_size, 0, 20000, 20000, 25000)\n",
        "train_dataset = train_and_validation_sequence_datasets.train_dataset\n",
        "val_dataset = train_and_validation_sequence_datasets.val_dataset"
      ],
      "metadata": {
        "id": "uDnN_IgsNIJD"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.__getitem__(0))"
      ],
      "metadata": {
        "id": "L3WwzUFdRbk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ca1adf-e729-4f77-91f6-d1ee714a7485"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('A man on the sea.', 'A man on the sea.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pad_collate = PadCollate(enRawName, enRawName, vocab_size, vocab_size)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128, collate_fn = pad_collate)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=128, collate_fn = pad_collate)"
      ],
      "metadata": {
        "id": "zLayKBKBLAYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed17cf89-be4c-4b9f-d07b-1f8381383044"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating tokenizer for drive/MyDrive/colab data/multi30kEnTrain.txt\n",
            "creating tokenizer for drive/MyDrive/colab data/multi30kEnTrain.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for src, tgt in train_dataloader:\n",
        "    print(src[0].ids, tgt[0].ids)\n",
        "    # print(\"decoded\", sequenceDataset.src_tokenizer.decode_batch([sequence.ids for sequence in src]))\n",
        "    # print(\"tgt\", tgt)\n",
        "\n",
        "    # print(\"mask:\", src[0].attention_mask)\n",
        "    break"
      ],
      "metadata": {
        "id": "7mnqolodQCYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa8657b-8b97-4306-e07b-34baa596c41f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30, 93, 89, 94, 1602, 15, 2] [0, 30, 93, 89, 94, 1602, 15, 1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(x, tokenizer):\n",
        "    x = torch.softmax(x, -1)\n",
        "    #print(\"x softmax:\", x)\n",
        "    x = torch.argmax(x, dim=-1)\n",
        "    x = x.tolist()\n",
        "    print(\"argmax x:\", x)\n",
        "    return tokenizer.decode(x)"
      ],
      "metadata": {
        "id": "ERDbLlQVzJUx"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_decode(tokenizer):\n",
        "    x = torch.tensor([[0, 5], [10, 20]], dtype=torch.float32)\n",
        "    words = decode(x, tokenizer)\n",
        "    print(words)\n",
        "\n",
        "test_decode(pad_collate.tgt_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSetg38X8TAo",
        "outputId": "eee6f25f-a71f-4020-c6f7-a01a7d3719ae"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax x: [1, 1]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(25)\n",
        "encoder_decoder_transformer = EncoderDecoderTransformer(num_encoder_layers, num_decoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, d_e, vocab_size, max_sequence_length, 0.1, False).to(device)\n",
        "opt = optim.Adam(encoder_decoder_transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "loss_function = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "epochs = 1000\n",
        "state_dict_filename = folder + \"encoder_decoder_transformer_state_dict_\" + datetime.today().strftime('%Y-%m-%d %H')\n",
        "# Large models need this to actually train\n",
        "for p in encoder_decoder_transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "#labelSmoothing = LabelSmoothing(2000, PADDING_IDX, 0.1)\n",
        "training_step = 0\n",
        "validation_step = 0\n",
        "best_val_loss = 100\n",
        "num_fails = 0\n",
        "for i in range(epochs):\n",
        "    epoch_time_start = time.time()\n",
        "    dataloader_iter = iter(train_dataloader)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    for src_batch, tgt_batch in dataloader_iter:\n",
        "        # print(\"x:\", sequence_x)\n",
        "        # print(\"z:\", sequence_z)\n",
        "        # sequence_x, sequence_z = sequenceDataset.__getitem__(i)\n",
        "        src_tokens = torch.IntTensor([sequence.ids for sequence in src_batch]).to(device)\n",
        "        encoder_input = src_tokens\n",
        "        train_tgt_tokens = torch.IntTensor([sequence.ids for sequence in tgt_batch]).to(device)\n",
        "        decoder_input = train_tgt_tokens[:, :-1]\n",
        "        decoder_desired_output_train = train_tgt_tokens[:, 1:]\n",
        "        src_masks = torch.IntTensor([sequence.attention_mask for sequence in src_batch]).to(device)\n",
        "        tgt_masks = torch.IntTensor([sequence.attention_mask for sequence in tgt_batch])[:, :-1].to(device)\n",
        "        # print(\"src masks\", src_masks)\n",
        "        # print(\"tgt masks\", tgt_masks)\n",
        "        train_output = encoder_decoder_transformer(encoder_input, decoder_input, src_masks, tgt_masks)\n",
        "        #print(output.shape)\n",
        "        # print(\"output\", output.shape)\n",
        "        output_transpose = train_output.transpose(-1, -2) # output needs to be N, C, other dimension for torch cross entropy\n",
        "        loss = loss_function(output_transpose, decoder_desired_output_train.long())\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        train_losses.append(loss.item())\n",
        "        if (training_step % 20 == 0):\n",
        "            print(\"Completed training step\", training_step)\n",
        "        training_step += 1\n",
        "\n",
        "    for src_batch, tgt_batch in val_dataloader:\n",
        "        src_tokens = torch.IntTensor([sequence.ids for sequence in src_batch]).to(device)\n",
        "        encoder_input = src_tokens\n",
        "        val_tgt_tokens = torch.IntTensor([sequence.ids for sequence in tgt_batch]).to(device)\n",
        "        decoder_input = val_tgt_tokens[:, :-1]\n",
        "        decoder_desired_output_val = val_tgt_tokens[:, 1:]\n",
        "        src_masks = torch.IntTensor([sequence.attention_mask for sequence in src_batch]).to(device)\n",
        "        tgt_masks = torch.IntTensor([sequence.attention_mask for sequence in tgt_batch])[:, :-1].to(device)\n",
        "        val_output = encoder_decoder_transformer(encoder_input, decoder_input, src_masks, tgt_masks)\n",
        "        output_transpose = val_output.transpose(-1, -2) # output needs to be N, C, other dimension for torch cross entropy\n",
        "        loss = loss_function(output_transpose, decoder_desired_output_val.long())\n",
        "        val_losses.append(loss.item())\n",
        "        if (validation_step % 20 == 0):\n",
        "            print(\"Completed validation step\", validation_step)\n",
        "        validation_step += 1\n",
        "\n",
        "    print(\"epoch\", i, \"took\", time.time() - epoch_time_start)\n",
        "    print(\"avg training loss:\", sum(train_losses) / len(train_losses))\n",
        "    avg_val_loss = sum(val_losses)/ len(val_losses)\n",
        "    print(\"avg validation loss:\", avg_val_loss)\n",
        "    expected_train_output = pad_collate.tgt_tokenizer.decode(decoder_desired_output_train[0].tolist())\n",
        "    print(\"expected train output\", expected_train_output)\n",
        "    decoded_output = decode(train_output[0], pad_collate.tgt_tokenizer)\n",
        "    print(\"decoded train output:\", decoded_output)\n",
        "    expected_val_output = pad_collate.tgt_tokenizer.decode(decoder_desired_output_val[0].tolist())\n",
        "    print(\"expected validation output\", expected_val_output)\n",
        "    decoded_output = decode(val_output[0], pad_collate.tgt_tokenizer)\n",
        "    print(\"decoded validation output:\", decoded_output)\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(encoder_decoder_transformer.state_dict(), state_dict_filename)\n",
        "        print(\"Saved model state dict to\", state_dict_filename)\n",
        "        num_fails = 0\n",
        "    else:\n",
        "        print(\"Average validation loss did not decrease from \", best_val_loss)\n",
        "        num_fails += 1\n",
        "        print(\"Failed to decrease the average validation loss\", num_fails, \"times.\")\n",
        "        if num_fails >= 2:\n",
        "            print(\"Stopping training\")\n",
        "            break\n",
        "    print()\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCHMIluxq6Su",
        "outputId": "cc75e8ff-a37a-41d8-9ad9-82d3f3f05120"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed training step 0\n",
            "Completed training step 20\n",
            "Completed training step 40\n",
            "Completed training step 60\n",
            "Completed training step 80\n",
            "Completed training step 100\n",
            "Completed training step 120\n",
            "Completed training step 140\n",
            "Completed validation step 0\n",
            "Completed validation step 20\n",
            "epoch 0 took 13.454680442810059\n",
            "avg training loss: 6.694152634614592\n",
            "avg validation loss: 5.364588463306427\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 2, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A a a a a a a a a a a a a a a a a a a a a a a\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [30, 2, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2]\n",
            "decoded validation output: A a a a a a a a a a a a a a a a a a a a a\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 160\n",
            "Completed training step 180\n",
            "Completed training step 200\n",
            "Completed training step 220\n",
            "Completed training step 240\n",
            "Completed training step 260\n",
            "Completed training step 280\n",
            "Completed training step 300\n",
            "Completed validation step 40\n",
            "Completed validation step 60\n",
            "epoch 1 took 13.477014064788818\n",
            "avg training loss: 4.828630851332549\n",
            "avg validation loss: 4.7981919884681705\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 15, 57, 57, 15, 57, 57, 1, 57, 57, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man a a a a a a a a a a a a a a a a a a a a a . a a . a a a a .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [30, 93, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 1, 57, 1, 2, 2, 2, 2]\n",
            "decoded validation output: A man a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 320\n",
            "Completed training step 340\n",
            "Completed training step 360\n",
            "Completed training step 380\n",
            "Completed training step 400\n",
            "Completed training step 420\n",
            "Completed training step 440\n",
            "Completed training step 460\n",
            "Completed validation step 80\n",
            "Completed validation step 100\n",
            "epoch 2 took 13.525323629379272\n",
            "avg training loss: 4.433024784561935\n",
            "avg validation loss: 4.639421117305756\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 83, 57, 57, 96, 96, 96, 57, 160, 96, 96, 57, 160, 96, 57, 96, 96, 96, 57, 96, 57, 57, 1, 15, 15, 57, 15, 57, 15, 57, 15, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man in a a and and and a shirt and and a shirt and a and and and a and a a . . a . a . a . .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [30, 93, 83, 57, 96, 96, 57, 83, 57, 160, 96, 96, 57, 57, 57, 96, 57, 57, 96, 96, 57, 15, 83, 96, 83, 83, 57, 57, 57, 1, 57, 1, 2, 2, 2, 2]\n",
            "decoded validation output: A man in a and and a in a shirt and and a a a and a a and and a . in and in in a a a a\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 480\n",
            "Completed training step 500\n",
            "Completed training step 520\n",
            "Completed training step 540\n",
            "Completed training step 560\n",
            "Completed training step 580\n",
            "Completed training step 600\n",
            "Completed training step 620\n",
            "Completed validation step 120\n",
            "Completed validation step 140\n",
            "epoch 3 took 13.430439710617065\n",
            "avg training loss: 4.199920198719973\n",
            "avg validation loss: 4.4570597290992735\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 83, 57, 57, 96, 96, 96, 57, 160, 160, 96, 57, 160, 96, 57, 160, 96, 57, 57, 96, 89, 57, 1, 15, 89, 57, 160, 96, 96, 57, 1, 160, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man in a a and and and a shirt shirt and a shirt and a shirt and a a and on a . on a shirt and and a shirt\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [30, 93, 83, 57, 83, 57, 57, 83, 57, 93, 160, 96, 57, 83, 57, 93, 96, 96, 96, 96, 57, 1, 104, 57, 15, 96, 57, 2, 57, 1, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: A man in a in a a in a man shirt and a in a man and and and and a of a . and a a .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 640\n",
            "Completed training step 660\n",
            "Completed training step 680\n",
            "Completed training step 700\n",
            "Completed training step 720\n",
            "Completed training step 740\n",
            "Completed training step 760\n",
            "Completed training step 780\n",
            "Completed validation step 160\n",
            "Completed validation step 180\n",
            "epoch 4 took 13.35774278640747\n",
            "avg training loss: 4.035745441533957\n",
            "avg validation loss: 4.216614049673081\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 101, 57, 160, 160, 96, 96, 57, 160, 160, 96, 57, 160, 96, 57, 160, 96, 96, 57, 96, 89, 57, 1, 89, 96, 89, 1, 96, 15, 57, 216, 96, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man is a shirt shirt and and a shirt shirt and a shirt and a shirt and and a and on a on and on and . a street and\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [30, 93, 83, 57, 96, 57, 57, 104, 57, 160, 160, 96, 57, 57, 57, 154, 104, 57, 104, 96, 57, 291, 104, 57, 104, 104, 57, 57, 57, 216, 57, 1, 2, 2, 2, 2]\n",
            "decoded validation output: A man in a and a a of a shirt shirt and a a a red of a of and a large of a of of a a a street a\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 800\n",
            "Completed training step 820\n",
            "Completed training step 840\n",
            "Completed training step 860\n",
            "Completed training step 880\n",
            "Completed training step 900\n",
            "Completed training step 920\n",
            "Completed training step 940\n",
            "Completed validation step 200\n",
            "Completed validation step 220\n",
            "epoch 5 took 13.261849641799927\n",
            "avg training loss: 3.869814734550039\n",
            "avg validation loss: 4.140329414606095\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 156, 57, 160, 160, 96, 96, 57, 160, 160, 96, 57, 160, 96, 57, 160, 96, 96, 57, 96, 89, 57, 191, 89, 57, 89, 154, 96, 96, 57, 191, 96, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man wearing a shirt shirt and and a shirt shirt and a shirt and a shirt and and a and on a blue on a on red and and a blue and\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [30, 93, 83, 57, 96, 83, 57, 104, 57, 93, 160, 96, 57, 13, 57, 93, 104, 57, 93, 104, 57, 93, 104, 57, 104, 104, 57, 104, 57, 93, 104, 1, 2, 2, 2, 2]\n",
            "decoded validation output: A man in a and in a of a man shirt and a , a man of a man of a man of a of of a of a man of\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 960\n",
            "Completed training step 980\n",
            "Completed training step 1000\n",
            "Completed training step 1020\n",
            "Completed training step 1040\n",
            "Completed training step 1060\n",
            "Completed training step 1080\n",
            "Completed validation step 240\n",
            "Completed validation step 260\n",
            "epoch 6 took 13.423939228057861\n",
            "avg training loss: 3.7383895117765777\n",
            "avg validation loss: 4.07142145037651\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 57, 160, 160, 96, 96, 57, 168, 160, 96, 57, 160, 96, 57, 160, 96, 96, 57, 96, 89, 57, 216, 89, 15, 89, 1, 15, 15, 57, 216, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with a shirt shirt and and a white shirt and a shirt and a shirt and and a and on a street on . on . . a street .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [30, 93, 83, 57, 96, 83, 57, 104, 57, 160, 160, 13, 57, 57, 57, 93, 83, 57, 93, 83, 57, 291, 104, 57, 104, 104, 57, 104, 57, 216, 104, 1, 2, 2, 2, 2]\n",
            "decoded validation output: A man in a and in a of a shirt shirt , a a a man in a man in a large of a of of a of a street of\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 1100\n",
            "Completed training step 1120\n",
            "Completed training step 1140\n",
            "Completed training step 1160\n",
            "Completed training step 1180\n",
            "Completed training step 1200\n",
            "Completed training step 1220\n",
            "Completed training step 1240\n",
            "Completed validation step 280\n",
            "Completed validation step 300\n",
            "epoch 7 took 13.358554363250732\n",
            "avg training loss: 3.6305558863718796\n",
            "avg validation loss: 4.02959361076355\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 57, 160, 160, 96, 197, 57, 168, 160, 13, 57, 160, 13, 57, 160, 96, 197, 57, 96, 89, 57, 216, 89, 57, 57, 154, 89, 89, 57, 168, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with a shirt shirt and sitting a white shirt , a shirt , a shirt and sitting a and on a street on a a red on on a white .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [30, 93, 83, 57, 96, 83, 57, 104, 57, 160, 160, 96, 57, 13, 57, 93, 13, 57, 93, 96, 57, 291, 96, 57, 104, 83, 57, 104, 57, 93, 104, 1, 2, 2, 2, 2]\n",
            "decoded validation output: A man in a and in a of a shirt shirt and a , a man , a man and a large and a of in a of a man of\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 1260\n",
            "Completed training step 1280\n",
            "Completed training step 1300\n",
            "Completed training step 1320\n",
            "Completed training step 1340\n",
            "Completed training step 1360\n",
            "Completed training step 1380\n",
            "Completed training step 1400\n",
            "Completed validation step 320\n",
            "Completed validation step 340\n",
            "epoch 8 took 13.41554856300354\n",
            "avg training loss: 3.529032930447038\n",
            "avg validation loss: 3.941862004995346\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 57, 96, 160, 96, 96, 57, 191, 160, 13, 57, 160, 96, 57, 160, 96, 96, 57, 96, 96, 57, 216, 96, 96, 57, 216, 96, 15, 57, 168, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with a and shirt and and a blue shirt , a shirt and a shirt and and a and and a street and and a street and . a white .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [30, 93, 83, 57, 96, 83, 57, 104, 57, 160, 160, 13, 57, 96, 57, 291, 96, 57, 93, 83, 57, 93, 96, 57, 15, 83, 57, 104, 57, 291, 104, 1, 2, 2, 2, 2]\n",
            "decoded validation output: A man in a and in a of a shirt shirt , a and a large and a man in a man and a . in a of a large of\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 1420\n",
            "Completed training step 1440\n",
            "Completed training step 1460\n",
            "Completed training step 1480\n",
            "Completed training step 1500\n",
            "Completed training step 1520\n",
            "Completed training step 1540\n",
            "Completed training step 1560\n",
            "Completed validation step 360\n",
            "Completed validation step 380\n",
            "epoch 9 took 13.345471858978271\n",
            "avg training loss: 3.4337933807616023\n",
            "avg validation loss: 3.9553105354309084\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 57, 180, 160, 96, 96, 57, 191, 160, 96, 57, 160, 96, 57, 160, 89, 89, 57, 89, 89, 94, 216, 89, 89, 57, 168, 89, 89, 57, 168, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with a black shirt and and a blue shirt and a shirt and a shirt on on a on on the street on on a white on on a white .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [265, 93, 156, 57, 96, 83, 57, 104, 57, 385, 160, 96, 57, 57, 57, 93, 96, 57, 93, 83, 57, 93, 83, 57, 15, 15, 57, 104, 57, 93, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: An man wearing a and in a of a orange shirt and a a a man and a man in a man in a . . a of a man .\n",
            "Average validation loss did not decrease from  3.941862004995346\n",
            "Failed to decrease the average validation loss 1 times.\n",
            "\n",
            "\n",
            "Completed training step 1580\n",
            "Completed training step 1600\n",
            "Completed training step 1620\n",
            "Completed training step 1640\n",
            "Completed training step 1660\n",
            "Completed training step 1680\n",
            "Completed training step 1700\n",
            "Completed training step 1720\n",
            "Completed validation step 400\n",
            "Completed validation step 420\n",
            "epoch 10 took 13.319223403930664\n",
            "avg training loss: 3.3421803826739076\n",
            "avg validation loss: 3.870465409755707\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 57, 160, 160, 96, 57, 57, 191, 160, 13, 57, 160, 13, 57, 160, 89, 89, 57, 13, 13, 57, 415, 109, 109, 57, 191, 109, 109, 57, 154, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with a shirt shirt and a a blue shirt , a shirt , a shirt on on a , , a background with with a blue with with a red .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [30, 93, 96, 57, 96, 83, 57, 104, 57, 160, 93, 96, 57, 184, 57, 93, 96, 205, 93, 109, 57, 93, 109, 57, 83, 109, 57, 104, 57, 1, 109, 1, 2, 2, 2, 2]\n",
            "decoded validation output: A man and a and in a of a shirt man and a while a man and standing man with a man with a in with a of a with\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 1740\n",
            "Completed training step 1760\n",
            "Completed training step 1780\n",
            "Completed training step 1800\n",
            "Completed training step 1820\n",
            "Completed training step 1840\n",
            "Completed training step 1860\n",
            "Completed training step 1880\n",
            "Completed validation step 440\n",
            "Completed validation step 460\n",
            "epoch 11 took 13.449576139450073\n",
            "avg training loss: 3.235623879037845\n",
            "avg validation loss: 3.7515048384666443\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 57, 13, 160, 13, 13, 57, 191, 13, 13, 57, 160, 13, 57, 160, 89, 89, 57, 13, 13, 57, 387, 13, 13, 57, 191, 13, 13, 57, 154, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with a , shirt , , a blue , , a shirt , a shirt on on a , , a beach , , a blue , , a red .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [265, 93, 156, 383, 101, 83, 57, 104, 57, 160, 160, 96, 57, 96, 57, 93, 96, 205, 93, 96, 57, 93, 96, 57, 96, 83, 57, 104, 57, 93, 96, 1, 2, 2, 2, 2]\n",
            "decoded validation output: An man wearing glasses is in a of a shirt shirt and a and a man and standing man and a man and a and in a of a man and\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 1900\n",
            "Completed training step 1920\n",
            "Completed training step 1940\n",
            "Completed training step 1960\n",
            "Completed training step 1980\n",
            "Completed training step 2000\n",
            "Completed training step 2020\n",
            "Completed training step 2040\n",
            "Completed validation step 480\n",
            "Completed validation step 500\n",
            "epoch 12 took 13.608535051345825\n",
            "avg training loss: 3.112407523355666\n",
            "avg validation loss: 3.5037687599658964\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 57, 394, 160, 109, 156, 57, 191, 160, 13, 57, 160, 89, 57, 160, 89, 89, 57, 89, 89, 57, 415, 89, 109, 57, 1, 15, 15, 57, 154, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with a hair shirt with wearing a blue shirt , a shirt on a shirt on on a on on a background on with a . . a red .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [265, 93, 156, 57, 83, 83, 57, 104, 57, 377, 93, 96, 236, 96, 57, 93, 96, 236, 93, 96, 57, 93, 96, 57, 96, 109, 57, 104, 57, 93, 96, 1, 2, 2, 2, 2]\n",
            "decoded validation output: An man wearing a in in a of a top man and holding and a man and holding man and a man and a and with a of a man and\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 2060\n",
            "Completed training step 2080\n",
            "Completed training step 2100\n",
            "Completed training step 2120\n",
            "Completed training step 2140\n",
            "Completed training step 2160\n",
            "Completed training step 2180\n",
            "Completed validation step 520\n",
            "Completed validation step 540\n",
            "epoch 13 took 13.399123191833496\n",
            "avg training loss: 2.9463183819108707\n",
            "avg validation loss: 3.276120162010193\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 570, 14, 14, 13, 156, 57, 154, 160, 13, 154, 160, 13, 57, 160, 13, 13, 57, 13, 13, 94, 415, 13, 13, 57, 154, 13, 13, 154, 154, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with long - - , wearing a red shirt , red shirt , a shirt , , a , , the background , , a red , , red red .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [265, 93, 156, 383, 396, 83, 230, 104, 84, 299, 331, 184, 236, 184, 57, 93, 96, 236, 93, 96, 57, 93, 96, 57, 96, 401, 57, 104, 57, 93, 96, 1, 2, 2, 2, 2]\n",
            "decoded validation output: An man wearing glasses stands in front of an old building while holding while a man and holding man and a man and a and behind a of a man and\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 2200\n",
            "Completed training step 2220\n",
            "Completed training step 2240\n",
            "Completed training step 2260\n",
            "Completed training step 2280\n",
            "Completed training step 2300\n",
            "Completed training step 2320\n",
            "Completed training step 2340\n",
            "Completed validation step 560\n",
            "Completed validation step 580\n",
            "epoch 14 took 13.316300392150879\n",
            "avg training loss: 2.747971744294379\n",
            "avg validation loss: 3.060643208026886\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 570, 394, 394, 13, 156, 57, 154, 160, 96, 57, 160, 13, 57, 160, 109, 109, 94, 96, 96, 94, 344, 109, 156, 109, 154, 15, 15, 172, 168, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with long hair hair , wearing a red shirt and a shirt , a shirt with with the and and the grass with wearing with red . . his white .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [265, 93, 156, 383, 398, 83, 230, 104, 84, 573, 372, 96, 57, 184, 57, 93, 184, 236, 93, 184, 57, 93, 184, 236, 184, 184, 57, 104, 57, 93, 96, 1, 2, 2, 2, 2]\n",
            "decoded validation output: An man wearing glasses sits in front of an outdoor table and a while a man while holding man while a man while holding while while a of a man and\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 2360\n",
            "Completed training step 2380\n",
            "Completed training step 2400\n",
            "Completed training step 2420\n",
            "Completed training step 2440\n",
            "Completed training step 2460\n",
            "Completed training step 2480\n",
            "Completed training step 2500\n",
            "Completed validation step 600\n",
            "Completed validation step 620\n",
            "epoch 15 took 13.324084758758545\n",
            "avg training loss: 2.541462728172351\n",
            "avg validation loss: 2.9430737853050233\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 570, 394, 394, 13, 156, 57, 154, 14, 96, 191, 394, 13, 57, 160, 13, 89, 57, 89, 14, 57, 255, 15, 109, 57, 264, 89, 13, 172, 513, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with long hair hair , wearing a red - and blue hair , a shirt , on a on - a back . with a green on , his bench .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [30, 93, 156, 383, 398, 83, 230, 104, 84, 386, 541, 96, 363, 96, 184, 93, 184, 363, 93, 184, 57, 93, 184, 363, 184, 184, 57, 104, 57, 93, 184, 1, 2, 2, 2, 2]\n",
            "decoded validation output: A man wearing glasses sits in front of an head chair and another and while man while another man while a man while another while while a of a man while\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 2520\n",
            "Completed training step 2540\n",
            "Completed training step 2560\n",
            "Completed training step 2580\n",
            "Completed training step 2600\n",
            "Completed training step 2620\n",
            "Completed training step 2640\n",
            "Completed training step 2660\n",
            "Completed validation step 640\n",
            "Completed validation step 660\n",
            "epoch 16 took 13.306828498840332\n",
            "avg training loss: 2.3632274919254765\n",
            "avg validation loss: 2.6844816505908966\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 938, 613, 394, 13, 156, 57, 154, 899, 96, 191, 540, 109, 190, 160, 89, 89, 94, 160, 13, 94, 528, 15, 156, 94, 168, 160, 15, 94, 168, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with many middle hair , wearing a red tank and blue pants with one shirt on on the shirt , the sand . wearing the white shirt . the white .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 377, 424, 96, 983, 184, 57, 93, 184, 57, 93, 156, 57, 93, 184, 363, 184, 184, 57, 104, 57, 93, 96, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an top wall and drinking while a man while a man wearing a man while another while while a of a man and\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 2680\n",
            "Completed training step 2700\n",
            "Completed training step 2720\n",
            "Completed training step 2740\n",
            "Completed training step 2760\n",
            "Completed training step 2780\n",
            "Completed training step 2800\n",
            "Completed training step 2820\n",
            "Completed validation step 680\n",
            "Completed validation step 700\n",
            "epoch 17 took 13.42537808418274\n",
            "avg training loss: 2.197761108920832\n",
            "avg validation loss: 2.4134099185466766\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 1056, 960, 394, 13, 156, 57, 154, 582, 96, 191, 552, 109, 191, 160, 89, 89, 110, 507, 89, 94, 528, 13, 89, 94, 168, 774, 89, 94, 528, 89, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with four uniforms hair , wearing a red coat and blue shorts with blue shirt on on to has on the sand , on the white floor on the sand on\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 1238, 541, 96, 983, 313, 57, 93, 184, 363, 93, 101, 57, 93, 101, 363, 15, 15, 57, 104, 57, 93, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an edge chair and drinking from a man while another man is a man is another . . a of a man .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 2840\n",
            "Completed training step 2860\n",
            "Completed training step 2880\n",
            "Completed training step 2900\n",
            "Completed training step 2920\n",
            "Completed training step 2940\n",
            "Completed training step 2960\n",
            "Completed training step 2980\n",
            "Completed validation step 720\n",
            "Completed validation step 740\n",
            "epoch 18 took 13.271783590316772\n",
            "avg training loss: 2.0617063242918365\n",
            "avg validation loss: 2.329517525434494\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 736, 917, 394, 13, 156, 57, 154, 735, 96, 191, 96, 109, 736, 160, 89, 89, 190, 507, 13, 57, 255, 89, 89, 94, 1136, 13, 89, 57, 1136, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with no sweater hair , wearing a red cap and blue and with no shirt on on one has , a back on on the couch , on a couch .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 1467, 1151, 96, 983, 313, 57, 728, 184, 363, 93, 96, 57, 257, 96, 363, 184, 184, 57, 104, 57, 698, 184, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an empty computer and drinking from a bag while another man and a hat and another while while a of a microphone while\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 3000\n",
            "Completed training step 3020\n",
            "Completed training step 3040\n",
            "Completed training step 3060\n",
            "Completed training step 3080\n",
            "Completed training step 3100\n",
            "Completed training step 3120\n",
            "Completed validation step 760\n",
            "Completed validation step 780\n",
            "epoch 19 took 13.474980354309082\n",
            "avg training loss: 1.9597501640866517\n",
            "avg validation loss: 2.2043480217456817\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 1056, 745, 394, 13, 156, 57, 154, 917, 96, 191, 552, 191, 736, 160, 13, 89, 190, 774, 13, 94, 774, 13, 577, 57, 190, 13, 13, 191, 774, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with four shirts hair , wearing a red sweater and blue shorts blue no shirt , on one floor , the floor , look a one , , blue floor .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 872, 850, 96, 983, 313, 57, 728, 184, 363, 93, 156, 57, 698, 236, 57, 15, 363, 57, 156, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an rain cellphone and drinking from a bag while another man wearing a microphone holding a . another a wearing a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 3140\n",
            "Completed training step 3160\n",
            "Completed training step 3180\n",
            "Completed training step 3200\n",
            "Completed training step 3220\n",
            "Completed training step 3240\n",
            "Completed training step 3260\n",
            "Completed training step 3280\n",
            "Completed validation step 800\n",
            "Completed validation step 820\n",
            "epoch 20 took 13.391775369644165\n",
            "avg training loss: 1.8764079368797837\n",
            "avg validation loss: 2.100140380859375\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 1819, 613, 394, 13, 156, 57, 154, 1200, 96, 191, 552, 109, 736, 160, 13, 89, 190, 1203, 97, 94, 287, 15, 669, 94, 387, 15, 15, 172, 528, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with shallow middle hair , wearing a red skirt and blue shorts with no shirt , on one number at the ground . toward the beach . . his sand .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 751, 1151, 96, 938, 313, 57, 1055, 184, 363, 93, 156, 57, 257, 184, 363, 184, 184, 57, 104, 57, 257, 184, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an umbrella computer and many from a cup while another man wearing a hat while another while while a of a hat while\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 3300\n",
            "Completed training step 3320\n",
            "Completed training step 3340\n",
            "Completed training step 3360\n",
            "Completed training step 3380\n",
            "Completed training step 3400\n",
            "Completed training step 3420\n",
            "Completed training step 3440\n",
            "Completed validation step 840\n",
            "Completed validation step 860\n",
            "epoch 21 took 13.411184072494507\n",
            "avg training loss: 1.8127961667479984\n",
            "avg validation loss: 1.9972919076681137\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 1977, 613, 394, 13, 156, 57, 154, 160, 96, 191, 552, 109, 736, 160, 13, 89, 190, 986, 97, 94, 1469, 15, 94, 57, 168, 774, 15, 736, 1469, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with curly middle hair , wearing a red shirt and blue shorts with no shirt , on one shoes at the lawn . the a white floor . no lawn .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 299, 1151, 96, 938, 313, 57, 1055, 184, 236, 93, 156, 57, 735, 96, 236, 96, 96, 57, 96, 57, 698, 96, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an old computer and many from a cup while holding man wearing a cap and holding and and a and a microphone and\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 3460\n",
            "Completed training step 3480\n",
            "Completed training step 3500\n",
            "Completed training step 3520\n",
            "Completed training step 3540\n",
            "Completed training step 3560\n",
            "Completed training step 3580\n",
            "Completed training step 3600\n",
            "Completed validation step 880\n",
            "Completed validation step 900\n",
            "epoch 22 took 13.449255228042603\n",
            "avg training loss: 1.7554462943107458\n",
            "avg validation loss: 1.9431075900793076\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 1106, 2358, 394, 13, 156, 57, 154, 986, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 15, 264, 94, 387, 15, 13, 94, 387, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with wet socks hair , wearing a red shoes and blue shorts with no shirt , on one knee at the beach . green the beach . , the beach .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 872, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 257, 96, 57, 15, 401, 57, 15, 57, 698, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an rain computer and drinks from a cup while another man wearing a hat and a . behind a . a microphone .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 3620\n",
            "Completed training step 3640\n",
            "Completed training step 3660\n",
            "Completed training step 3680\n",
            "Completed training step 3700\n",
            "Completed training step 3720\n",
            "Completed training step 3740\n",
            "Completed training step 3760\n",
            "Completed validation step 920\n",
            "Completed validation step 940\n",
            "epoch 23 took 13.330772638320923\n",
            "avg training loss: 1.7057211133325176\n",
            "avg validation loss: 1.8461653232574462\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 1036, 745, 394, 13, 156, 57, 154, 1395, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 109, 1109, 57, 528, 13, 109, 94, 288, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with short shirts hair , wearing a red tie and blue shorts with no shirt , on one knee at the beach with both a sand , with the little .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 1703, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 236, 15, 401, 57, 15, 57, 452, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an foreground computer and drinks from a cup while another man wearing a cap and holding . behind a . a camera .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 3780\n",
            "Completed training step 3800\n",
            "Completed training step 3820\n",
            "Completed training step 3840\n",
            "Completed training step 3860\n",
            "Completed training step 3880\n",
            "Completed training step 3900\n",
            "Completed training step 3920\n",
            "Completed validation step 960\n",
            "Completed validation step 980\n",
            "epoch 24 took 13.304368734359741\n",
            "avg training loss: 1.6647000920241046\n",
            "avg validation loss: 1.8237840503454208\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 2296, 2276, 394, 13, 156, 57, 154, 2404, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 57, 387, 96, 94, 94, 1469, 13, 109, 57, 1469, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with camouflage skirts hair , wearing a red bandanna and blue shorts with no shirt , on one knee at a beach and the the lawn , with a lawn .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 96, 1055, 184, 363, 93, 156, 57, 735, 96, 1561, 184, 401, 57, 96, 222, 698, 96, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from and cup while another man wearing a cap and sings while behind a and her microphone and\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 3940\n",
            "Completed training step 3960\n",
            "Completed training step 3980\n",
            "Completed training step 4000\n",
            "Completed training step 4020\n",
            "Completed training step 4040\n",
            "Completed training step 4060\n",
            "Completed training step 4080\n",
            "Completed validation step 1000\n",
            "Completed validation step 1020\n",
            "epoch 25 took 13.328147888183594\n",
            "avg training loss: 1.6321428847161068\n",
            "avg validation loss: 1.921980020403862\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 160, 394, 13, 156, 57, 154, 2076, 96, 191, 552, 109, 736, 160, 94, 89, 190, 2042, 97, 94, 387, 94, 387, 57, 387, 15, 97, 94, 387, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium shirt hair , wearing a red button and blue shorts with no shirt the on one knee at the beach the beach a beach . at the beach .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 96, 1055, 184, 363, 93, 156, 57, 735, 96, 57, 96, 401, 57, 96, 57, 257, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from and cup while another man wearing a cap and a and behind a and a hat .\n",
            "Average validation loss did not decrease from  1.8237840503454208\n",
            "Failed to decrease the average validation loss 1 times.\n",
            "\n",
            "\n",
            "Completed training step 4100\n",
            "Completed training step 4120\n",
            "Completed training step 4140\n",
            "Completed training step 4160\n",
            "Completed training step 4180\n",
            "Completed training step 4200\n",
            "Completed training step 4220\n",
            "Completed validation step 1040\n",
            "Completed validation step 1060\n",
            "epoch 26 took 13.352716445922852\n",
            "avg training loss: 1.6005012366422422\n",
            "avg validation loss: 1.7682382762432098\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 2376, 745, 394, 13, 156, 57, 154, 1951, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 89, 255, 94, 1469, 13, 13, 57, 1469, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with musician shirts hair , wearing a red sleeve and blue shorts with no shirt , on one knee at the beach on back the lawn , , a lawn .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 156, 109, 57, 96, 57, 452, 96, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from a cup while another man wearing a cap and glasses wearing with a and a camera and\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 4240\n",
            "Completed training step 4260\n",
            "Completed training step 4280\n",
            "Completed training step 4300\n",
            "Completed training step 4320\n",
            "Completed training step 4340\n",
            "Completed training step 4360\n",
            "Completed training step 4380\n",
            "Completed validation step 1080\n",
            "Completed validation step 1100\n",
            "epoch 27 took 13.504343271255493\n",
            "avg training loss: 1.5709511231464945\n",
            "avg validation loss: 1.7224042147397995\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 2477, 2404, 394, 13, 156, 57, 154, 2366, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 109, 89, 57, 190, 15, 109, 57, 168, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with German bandanna hair , wearing a red tube and blue shorts with no shirt , on one knee at the beach with on a one . with a white .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 2937, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 93, 96, 383, 156, 96, 57, 96, 57, 688, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an apartment computer and drinks from a cup while another man wearing a man and glasses wearing and a and a sunglasses .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 4400\n",
            "Completed training step 4420\n",
            "Completed training step 4440\n",
            "Completed training step 4460\n",
            "Completed training step 4480\n",
            "Completed training step 4500\n",
            "Completed training step 4520\n",
            "Completed training step 4540\n",
            "Completed validation step 1120\n",
            "Completed validation step 1140\n",
            "epoch 28 took 13.497114181518555\n",
            "avg training loss: 1.5458417895493235\n",
            "avg validation loss: 1.6982556611299515\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 2453, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 109, 94, 94, 154, 255, 109, 57, 528, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium sleeved hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach with the the red back with a sand .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 15, 109, 84, 96, 57, 435, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from a cup while another man wearing a cap and glasses . with an and a picture .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 4560\n",
            "Completed training step 4580\n",
            "Completed training step 4600\n",
            "Completed training step 4620\n",
            "Completed training step 4640\n",
            "Completed training step 4660\n",
            "Completed training step 4680\n",
            "Completed training step 4700\n",
            "Completed validation step 1160\n",
            "Completed validation step 1180\n",
            "epoch 29 took 13.436859369277954\n",
            "avg training loss: 1.5222922517995165\n",
            "avg validation loss: 1.6706357806921006\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 4995, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 109, 190, 57, 190, 631, 109, 57, 154, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach with one a one swing with a red .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 401, 401, 57, 104, 57, 363, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from a cup while another man wearing a cap and glasses behind behind a of a another .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 4720\n",
            "Completed training step 4740\n",
            "Completed training step 4760\n",
            "Completed training step 4780\n",
            "Completed training step 4800\n",
            "Completed training step 4820\n",
            "Completed training step 4840\n",
            "Completed training step 4860\n",
            "Completed validation step 1200\n",
            "Completed validation step 1220\n",
            "epoch 30 took 13.422278881072998\n",
            "avg training loss: 1.5013470672498084\n",
            "avg validation loss: 1.7854721605777741\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 2404, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 94, 94, 387, 13, 13, 94, 528, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium bandanna hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , the the beach , , the sand .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 83, 401, 57, 104, 57, 1200, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from a cup while another man wearing a cap and glasses in behind a of a skirt .\n",
            "Average validation loss did not decrease from  1.6706357806921006\n",
            "Failed to decrease the average validation loss 1 times.\n",
            "\n",
            "\n",
            "Completed training step 4880\n",
            "Completed training step 4900\n",
            "Completed training step 4920\n",
            "Completed training step 4940\n",
            "Completed training step 4960\n",
            "Completed training step 4980\n",
            "Completed training step 5000\n",
            "Completed training step 5020\n",
            "Completed validation step 1240\n",
            "Completed validation step 1260\n",
            "epoch 31 took 13.546867370605469\n",
            "avg training loss: 1.4831601821692886\n",
            "avg validation loss: 1.643091532588005\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 1977, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 15, 57, 528, 15, 109, 57, 528, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium curly hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , . a sand . with a sand .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 96, 401, 57, 96, 57, 566, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from a cup while another man wearing a cap and glasses and behind a and a baby .\n",
            "Saved model state dict to drive/MyDrive/colab data/encoder_decoder_transformer_state_dict_2024-08-05 15\n",
            "\n",
            "\n",
            "Completed training step 5040\n",
            "Completed training step 5060\n",
            "Completed training step 5080\n",
            "Completed training step 5100\n",
            "Completed training step 5120\n",
            "Completed training step 5140\n",
            "Completed training step 5160\n",
            "Completed training step 5180\n",
            "Completed validation step 1280\n",
            "Completed validation step 1300\n",
            "epoch 32 took 13.393759965896606\n",
            "avg training loss: 1.4674009441570113\n",
            "avg validation loss: 1.6432374835014343\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 4995, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 353, 57, 736, 15, 15, 57, 168, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , running a no . . a white .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 3760, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 396, 231, 57, 104, 57, 1, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an icy computer and drinks from a cup while another man wearing a cap and glasses stands up a of a .\n",
            "Average validation loss did not decrease from  1.643091532588005\n",
            "Failed to decrease the average validation loss 1 times.\n",
            "\n",
            "\n",
            "Completed training step 5200\n",
            "Completed training step 5220\n",
            "Completed training step 5240\n",
            "Completed training step 5260\n",
            "Completed training step 5280\n",
            "Completed training step 5300\n",
            "Completed training step 5320\n",
            "Completed validation step 1320\n",
            "Completed validation step 1340\n",
            "epoch 33 took 13.328004360198975\n",
            "avg training loss: 1.452353993798517\n",
            "avg validation loss: 1.659559491276741\n",
            "expected train output A man with medium length hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , sculpting a sand castle with a trough .\n",
            "argmax x: [30, 93, 109, 3648, 3864, 394, 13, 156, 57, 154, 3518, 96, 191, 552, 109, 736, 160, 13, 89, 190, 2042, 97, 94, 387, 13, 2275, 57, 387, 3081, 13, 57, 190, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "decoded train output: A man with medium belts hair , wearing a red beanie and blue shorts with no shirt , on one knee at the beach , fix a beach castle , a one .\n",
            "expected validation output One man wearing glasses sits in front of an Apple computer and drinks from a cup while another man wearing a cap and glasses stands in front of a microphone .\n",
            "argmax x: [766, 93, 156, 383, 398, 83, 230, 104, 84, 4383, 1151, 96, 1477, 313, 57, 1055, 184, 363, 93, 156, 57, 735, 96, 383, 104, 83, 57, 15, 57, 415, 15, 1, 2, 2, 2, 2]\n",
            "decoded validation output: One man wearing glasses sits in front of an easel computer and drinks from a cup while another man wearing a cap and glasses of in a . a background .\n",
            "Average validation loss did not decrease from  1.643091532588005\n",
            "Failed to decrease the average validation loss 2 times.\n",
            "Stopping training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_decoder_transformer = EncoderDecoderTransformer(num_encoder_layers, num_decoder_layers, num_heads, d_attn, d_x, d_z, d_out, d_mid, d_mlp, d_e, vocab_size, max_sequence_length, p_dropout, False).to(device)\n"
      ],
      "metadata": {
        "id": "VLT6fo1IjhG3"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#state_dict_filename = folder + \"encoder_decoder_transformer_state_dict_\" + \"2024-08-02 15\"\n",
        "state_dict = torch.load(state_dict_filename, map_location = device)\n",
        "print(state_dict.keys())\n",
        "encoder_decoder_transformer.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGBx0brYMJE6",
        "outputId": "4aa324fd-afa6-493a-c7e0-b697011f14ab"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['src_embedding.table.weight', 'tgt_embedding.table.weight', 'unembedding.weight.weight', 'unembedding.weight.bias', 'positionalEmbedding.table.weight', 'encoder.layers.0.multi_head_attention.weight_query.weight', 'encoder.layers.0.multi_head_attention.weight_query.bias', 'encoder.layers.0.multi_head_attention.weight_key.weight', 'encoder.layers.0.multi_head_attention.weight_key.bias', 'encoder.layers.0.multi_head_attention.weight_value.weight', 'encoder.layers.0.multi_head_attention.weight_value.bias', 'encoder.layers.0.multi_head_attention.weight_out.weight', 'encoder.layers.0.multi_head_attention.weight_out.bias', 'encoder.layers.0.layer_norm1.scale', 'encoder.layers.0.layer_norm1.offset', 'encoder.layers.0.feed_forward.mlp1', 'encoder.layers.0.feed_forward.mlp2', 'encoder.layers.0.feed_forward.mlp1_bias', 'encoder.layers.0.feed_forward.mlp2_bias', 'encoder.layers.0.layer_norm2.scale', 'encoder.layers.0.layer_norm2.offset', 'encoder.layers.1.multi_head_attention.weight_query.weight', 'encoder.layers.1.multi_head_attention.weight_query.bias', 'encoder.layers.1.multi_head_attention.weight_key.weight', 'encoder.layers.1.multi_head_attention.weight_key.bias', 'encoder.layers.1.multi_head_attention.weight_value.weight', 'encoder.layers.1.multi_head_attention.weight_value.bias', 'encoder.layers.1.multi_head_attention.weight_out.weight', 'encoder.layers.1.multi_head_attention.weight_out.bias', 'encoder.layers.1.layer_norm1.scale', 'encoder.layers.1.layer_norm1.offset', 'encoder.layers.1.feed_forward.mlp1', 'encoder.layers.1.feed_forward.mlp2', 'encoder.layers.1.feed_forward.mlp1_bias', 'encoder.layers.1.feed_forward.mlp2_bias', 'encoder.layers.1.layer_norm2.scale', 'encoder.layers.1.layer_norm2.offset', 'encoder.layers.2.multi_head_attention.weight_query.weight', 'encoder.layers.2.multi_head_attention.weight_query.bias', 'encoder.layers.2.multi_head_attention.weight_key.weight', 'encoder.layers.2.multi_head_attention.weight_key.bias', 'encoder.layers.2.multi_head_attention.weight_value.weight', 'encoder.layers.2.multi_head_attention.weight_value.bias', 'encoder.layers.2.multi_head_attention.weight_out.weight', 'encoder.layers.2.multi_head_attention.weight_out.bias', 'encoder.layers.2.layer_norm1.scale', 'encoder.layers.2.layer_norm1.offset', 'encoder.layers.2.feed_forward.mlp1', 'encoder.layers.2.feed_forward.mlp2', 'encoder.layers.2.feed_forward.mlp1_bias', 'encoder.layers.2.feed_forward.mlp2_bias', 'encoder.layers.2.layer_norm2.scale', 'encoder.layers.2.layer_norm2.offset', 'encoder.layers.3.multi_head_attention.weight_query.weight', 'encoder.layers.3.multi_head_attention.weight_query.bias', 'encoder.layers.3.multi_head_attention.weight_key.weight', 'encoder.layers.3.multi_head_attention.weight_key.bias', 'encoder.layers.3.multi_head_attention.weight_value.weight', 'encoder.layers.3.multi_head_attention.weight_value.bias', 'encoder.layers.3.multi_head_attention.weight_out.weight', 'encoder.layers.3.multi_head_attention.weight_out.bias', 'encoder.layers.3.layer_norm1.scale', 'encoder.layers.3.layer_norm1.offset', 'encoder.layers.3.feed_forward.mlp1', 'encoder.layers.3.feed_forward.mlp2', 'encoder.layers.3.feed_forward.mlp1_bias', 'encoder.layers.3.feed_forward.mlp2_bias', 'encoder.layers.3.layer_norm2.scale', 'encoder.layers.3.layer_norm2.offset', 'encoder.final_norm.scale', 'encoder.final_norm.offset', 'decoder.layers.0.multi_head_self_attention.weight_query.weight', 'decoder.layers.0.multi_head_self_attention.weight_query.bias', 'decoder.layers.0.multi_head_self_attention.weight_key.weight', 'decoder.layers.0.multi_head_self_attention.weight_key.bias', 'decoder.layers.0.multi_head_self_attention.weight_value.weight', 'decoder.layers.0.multi_head_self_attention.weight_value.bias', 'decoder.layers.0.multi_head_self_attention.weight_out.weight', 'decoder.layers.0.multi_head_self_attention.weight_out.bias', 'decoder.layers.0.layer_norm1.scale', 'decoder.layers.0.layer_norm1.offset', 'decoder.layers.0.multi_head_global_attention.weight_query.weight', 'decoder.layers.0.multi_head_global_attention.weight_query.bias', 'decoder.layers.0.multi_head_global_attention.weight_key.weight', 'decoder.layers.0.multi_head_global_attention.weight_key.bias', 'decoder.layers.0.multi_head_global_attention.weight_value.weight', 'decoder.layers.0.multi_head_global_attention.weight_value.bias', 'decoder.layers.0.multi_head_global_attention.weight_out.weight', 'decoder.layers.0.multi_head_global_attention.weight_out.bias', 'decoder.layers.0.layer_norm2.scale', 'decoder.layers.0.layer_norm2.offset', 'decoder.layers.0.feed_forward.mlp1', 'decoder.layers.0.feed_forward.mlp2', 'decoder.layers.0.feed_forward.mlp1_bias', 'decoder.layers.0.feed_forward.mlp2_bias', 'decoder.layers.0.layer_norm3.scale', 'decoder.layers.0.layer_norm3.offset', 'decoder.layers.1.multi_head_self_attention.weight_query.weight', 'decoder.layers.1.multi_head_self_attention.weight_query.bias', 'decoder.layers.1.multi_head_self_attention.weight_key.weight', 'decoder.layers.1.multi_head_self_attention.weight_key.bias', 'decoder.layers.1.multi_head_self_attention.weight_value.weight', 'decoder.layers.1.multi_head_self_attention.weight_value.bias', 'decoder.layers.1.multi_head_self_attention.weight_out.weight', 'decoder.layers.1.multi_head_self_attention.weight_out.bias', 'decoder.layers.1.layer_norm1.scale', 'decoder.layers.1.layer_norm1.offset', 'decoder.layers.1.multi_head_global_attention.weight_query.weight', 'decoder.layers.1.multi_head_global_attention.weight_query.bias', 'decoder.layers.1.multi_head_global_attention.weight_key.weight', 'decoder.layers.1.multi_head_global_attention.weight_key.bias', 'decoder.layers.1.multi_head_global_attention.weight_value.weight', 'decoder.layers.1.multi_head_global_attention.weight_value.bias', 'decoder.layers.1.multi_head_global_attention.weight_out.weight', 'decoder.layers.1.multi_head_global_attention.weight_out.bias', 'decoder.layers.1.layer_norm2.scale', 'decoder.layers.1.layer_norm2.offset', 'decoder.layers.1.feed_forward.mlp1', 'decoder.layers.1.feed_forward.mlp2', 'decoder.layers.1.feed_forward.mlp1_bias', 'decoder.layers.1.feed_forward.mlp2_bias', 'decoder.layers.1.layer_norm3.scale', 'decoder.layers.1.layer_norm3.offset', 'decoder.layers.2.multi_head_self_attention.weight_query.weight', 'decoder.layers.2.multi_head_self_attention.weight_query.bias', 'decoder.layers.2.multi_head_self_attention.weight_key.weight', 'decoder.layers.2.multi_head_self_attention.weight_key.bias', 'decoder.layers.2.multi_head_self_attention.weight_value.weight', 'decoder.layers.2.multi_head_self_attention.weight_value.bias', 'decoder.layers.2.multi_head_self_attention.weight_out.weight', 'decoder.layers.2.multi_head_self_attention.weight_out.bias', 'decoder.layers.2.layer_norm1.scale', 'decoder.layers.2.layer_norm1.offset', 'decoder.layers.2.multi_head_global_attention.weight_query.weight', 'decoder.layers.2.multi_head_global_attention.weight_query.bias', 'decoder.layers.2.multi_head_global_attention.weight_key.weight', 'decoder.layers.2.multi_head_global_attention.weight_key.bias', 'decoder.layers.2.multi_head_global_attention.weight_value.weight', 'decoder.layers.2.multi_head_global_attention.weight_value.bias', 'decoder.layers.2.multi_head_global_attention.weight_out.weight', 'decoder.layers.2.multi_head_global_attention.weight_out.bias', 'decoder.layers.2.layer_norm2.scale', 'decoder.layers.2.layer_norm2.offset', 'decoder.layers.2.feed_forward.mlp1', 'decoder.layers.2.feed_forward.mlp2', 'decoder.layers.2.feed_forward.mlp1_bias', 'decoder.layers.2.feed_forward.mlp2_bias', 'decoder.layers.2.layer_norm3.scale', 'decoder.layers.2.layer_norm3.offset', 'decoder.layers.3.multi_head_self_attention.weight_query.weight', 'decoder.layers.3.multi_head_self_attention.weight_query.bias', 'decoder.layers.3.multi_head_self_attention.weight_key.weight', 'decoder.layers.3.multi_head_self_attention.weight_key.bias', 'decoder.layers.3.multi_head_self_attention.weight_value.weight', 'decoder.layers.3.multi_head_self_attention.weight_value.bias', 'decoder.layers.3.multi_head_self_attention.weight_out.weight', 'decoder.layers.3.multi_head_self_attention.weight_out.bias', 'decoder.layers.3.layer_norm1.scale', 'decoder.layers.3.layer_norm1.offset', 'decoder.layers.3.multi_head_global_attention.weight_query.weight', 'decoder.layers.3.multi_head_global_attention.weight_query.bias', 'decoder.layers.3.multi_head_global_attention.weight_key.weight', 'decoder.layers.3.multi_head_global_attention.weight_key.bias', 'decoder.layers.3.multi_head_global_attention.weight_value.weight', 'decoder.layers.3.multi_head_global_attention.weight_value.bias', 'decoder.layers.3.multi_head_global_attention.weight_out.weight', 'decoder.layers.3.multi_head_global_attention.weight_out.bias', 'decoder.layers.3.layer_norm2.scale', 'decoder.layers.3.layer_norm2.offset', 'decoder.layers.3.feed_forward.mlp1', 'decoder.layers.3.feed_forward.mlp2', 'decoder.layers.3.feed_forward.mlp1_bias', 'decoder.layers.3.feed_forward.mlp2_bias', 'decoder.layers.3.layer_norm3.scale', 'decoder.layers.3.layer_norm3.offset', 'decoder.final_norm.scale', 'decoder.final_norm.offset'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_with_one_sample(model):\n",
        "    dataloader_iter = iter(train_dataloader)\n",
        "    for src_batch, tgt_batch in dataloader_iter:\n",
        "        # print(\"x:\", sequence_x)\n",
        "        # print(\"z:\", sequence_z)\n",
        "        # sequence_x, sequence_z = sequenceDataset.__getitem__(i)\n",
        "        src_batch = [src_batch[0]]\n",
        "        tgt_batch = [tgt_batch[0]]\n",
        "        src_tokens = torch.IntTensor([sequence.ids for sequence in src_batch]).to(device)\n",
        "        encoder_input = src_tokens\n",
        "        print(\"encoder input\", encoder_input)\n",
        "        decoded_input = pad_collate.src_tokenizer.decode(encoder_input[0].tolist())\n",
        "        print(\"decoded input\", decoded_input)\n",
        "        train_tgt_tokens = torch.IntTensor([sequence.ids for sequence in tgt_batch]).to(device)\n",
        "        decoder_input = train_tgt_tokens[:, :-1]\n",
        "        print(\"decoder input\", decoder_input)\n",
        "        decoder_desired_output_train = train_tgt_tokens[:, 1:]\n",
        "        src_masks = torch.IntTensor([sequence.attention_mask for sequence in src_batch]).to(device)\n",
        "        tgt_masks = torch.IntTensor([sequence.attention_mask for sequence in tgt_batch])[:, :-1].to(device)\n",
        "        # print(\"src masks\", src_masks)\n",
        "        # print(\"tgt masks\", tgt_masks)\n",
        "        train_output = encoder_decoder_transformer(encoder_input, decoder_input, src_masks, tgt_masks)\n",
        "        print(train_output.shape)\n",
        "        decoded_output = decode(train_output[0], pad_collate.tgt_tokenizer)\n",
        "        print(\"decoded output\", decoded_output)\n",
        "        break\n",
        "\n",
        "test_model_with_one_sample(encoder_decoder_transformer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bertgYLZzZ7m",
        "outputId": "3d579f81-a422-4eb2-a04d-cbf8c57805a5"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder input tensor([[  30,   93,   89,   94, 1602,   15,    2,    2]], device='cuda:0',\n",
            "       dtype=torch.int32)\n",
            "decoded input A man on the sea .\n",
            "decoder input tensor([[   0,   30,   93,   89,   94, 1602,   15,    1,    2]],\n",
            "       device='cuda:0', dtype=torch.int32)\n",
            "torch.Size([1, 9, 10000])\n",
            "argmax x: [30, 93, 89, 94, 1602, 15, 1, 2, 2]\n",
            "decoded output A man on the sea .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(encoder_decoder_transformer.state_dict(), state_dict_filename)"
      ],
      "metadata": {
        "id": "kuI-KXWBa1Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_from_tokens(model, input, src_tokenizer, tgt_tokenizer):\n",
        "    model.disable_subsequent_mask()\n",
        "    src_tokenizer.no_padding()\n",
        "    tgt_tokenizer.no_padding()\n",
        "\n",
        "    src_tokenizer.no_truncation()\n",
        "    tgt_tokenizer.no_truncation()\n",
        "    src_sequence = input\n",
        "    print(src_sequence)\n",
        "    src_sequence = src_tokenizer.encode(src_sequence)\n",
        "    print(src_sequence)\n",
        "    print(src_tokenizer.decode(src_sequence.ids))\n",
        "    src_sequence = torch.IntTensor(src_sequence.ids).unsqueeze(0).to(device)\n",
        "    print(\"src tokens\", src_sequence)\n",
        "    tgt_sequence = torch.IntTensor([0]).unsqueeze(0).to(device)\n",
        "    src_mask = torch.ones(src_sequence.shape, dtype=torch.int32).to(device)\n",
        "    print(\"decoder input\", tgt_sequence)\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        length_gen = 10\n",
        "        for i in range(length_gen):\n",
        "            tgt_mask = torch.ones(tgt_sequence.shape, dtype=torch.int32).to(device)\n",
        "            prediction = model(src_sequence, tgt_sequence, src_mask, tgt_mask)\n",
        "            #print(\"prediction:\", prediction)\n",
        "            prediction = torch.softmax(prediction, -1)\n",
        "            #print(\"softmax prediction:\", prediction.shape)\n",
        "            prediction = torch.argmax(prediction, dim=-1)\n",
        "            print(\"argmax prediction:\", prediction)\n",
        "            print(\"actual prediction:\", tgt_tokenizer.decode(prediction[0].tolist()))\n",
        "            last_token = prediction[0][-1]\n",
        "            tgt_sequence = torch.cat((tgt_sequence, last_token.unsqueeze(0).unsqueeze(0)), dim=-1)\n",
        "            if last_token == 1:\n",
        "                break\n",
        "    return tgt_sequence\n",
        "\n",
        "tgt_sequence = predict_from_tokens(encoder_decoder_transformer, \"A man on the sea\", pad_collate.src_tokenizer, pad_collate.tgt_tokenizer)\n",
        "print(tgt_sequence)\n",
        "print(pad_collate.tgt_tokenizer.decode(tgt_sequence[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVEp1SmrhVuf",
        "outputId": "4851bd5d-b658-4de8-e453-371160e63730"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A man on the sea\n",
            "Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "A man on the sea\n",
            "src tokens tensor([[  30,   93,   89,   94, 1602]], device='cuda:0', dtype=torch.int32)\n",
            "decoder input tensor([[0]], device='cuda:0', dtype=torch.int32)\n",
            "argmax prediction: tensor([[30]], device='cuda:0')\n",
            "actual prediction: A\n",
            "argmax prediction: tensor([[93, 93]], device='cuda:0')\n",
            "actual prediction: man man\n",
            "argmax prediction: tensor([[93, 93, 89]], device='cuda:0')\n",
            "actual prediction: man man on\n",
            "argmax prediction: tensor([[94, 93, 89, 94]], device='cuda:0')\n",
            "actual prediction: the man on the\n",
            "argmax prediction: tensor([[   2,    1,   89,   94, 1602]], device='cuda:0')\n",
            "actual prediction: on the sea\n",
            "argmax prediction: tensor([[   1,    1,   89,    2, 1602,    1]], device='cuda:0')\n",
            "actual prediction: on sea\n",
            "tensor([[   0,   30,   93,   89,   94, 1602,    1]], device='cuda:0')\n",
            "A man on the sea\n"
          ]
        }
      ]
    }
  ]
}